{"/2022-02-20":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-02-24":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-02-25":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-02-28":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-03-07":{"title":"Untitled Page","content":"# Journal\nI've had some difficulty understanding material in [[Stat 502 - Probability II]]. It has felt like the results were just being thrown around without much to build to. Really I felt I was missing more concrete examples in probability. The book [[Resnick - A Probability Path]] is more like any other math textbook, introducing results then slowly putting them together for more complicated results.  I'm feeling confused about the relative difficulty of the results I'm seeing. At lower levels, I had a good sense of what was too complicated for basic methods and what wasn't. Here, with probability (since there is a lower level simple formulation of this stuff) I feel unclear about when are these more advanced methods applicable and when are simpler methods applicable. For example, with [[Martingale]]s, perhaps a problem could be framed as a martingale or some simpler method might get me the answer. This kind of feeling comes not really understanding the role these techniques play and where simpler techniques fail. I think this sense might better come from doing exercises and rederiving proofs.\n\nI think it's best not to forget my basics and replace them with more complicated and abstract methods. It's best to be familiar with the most elementary techinques and proofs for solving problems even though the abstract way may give more succinct or elegant proofs. At the same time, it's worth understanding the more abstract and complicated side to things as well.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-03-16":{"title":"Untitled Page","content":"#Breadcrumbs\n - Organize [[Closed]] set results. I want to establish them around [[A Set is Closed iff it is its own Closure]] and appropriate characterizations of the [[Closure]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-03-21":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-04-10":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-04-12":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/2022-04-17":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Closed-Convex-Set-is-the-Intersection-of-all-Closed-Halfspaces-containing-it":{"title":"Untitled Page","content":"# Statement\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$ and $S \\subset V$ be [[Closed]] and [[Convex Set|convex]]. Then $$S = \\bigcap\\limits_{} \\{H \\subset V : S \\subset H, H \\text{ is a closed halfspace}\\}$$\n## Proof\n[[TODO]] - see [[Boyd - Convex Optimization]] section 2.5 pg 46","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Convex-Cone-is-a-Convex-Set":{"title":"Untitled Page","content":"# Statement\nLet $V$ be a [[Vector Space]] on $\\mathbb{R}$ and let $S \\subset V$ be a [[Convex Cone]]. Then $S$ is a [[Convex Set]].\n\n## Proof\nSuppose $\\mathbf{u}, \\mathbf{v} \\in S$ and $a \\in [0, 1]$. If $a = 0$ or $a = 1$ we have that $a \\mathbf{u} + (1-a) \\mathbf{v} \\in \\{u, v\\} \\subset S$. Otherwise $a \\in (0,1)$ so $a \u003e 0$ and $1-a \u003e 0$. Because $S$ is a [[Convex Cone]], we have that $a \\mathbf{u} + (1-a) \\mathbf{v} \\in S$. Thus $S$ is a [[Convex Set]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Distribution-Function-is-Determined-on-a-Dense-Set":{"title":"Untitled Page","content":"# Statement\nLet $D \\subset \\mathbb{R}$ be [[Dense]] and let $F_{D}: D \\to \\mathbb{R}$. Then if we define\n\n$$F(x) = \\inf_{y \\in D; y \u003e x} F_{D}(y)$$\n$F$ is a [[Distribution Function]]. \n\nFurthermore, if $G$ is a [[Distribution Function]] so that $G {\\big|}_{D} = F_D$, then $G = F$.\n\n# Proof\nObserve that since $D$ is [[Dense]] and $\\mathbb{R}$ is [[Archimedean]], $\\{F_{D}(y): y \\in D, y \u003e x\\}$ is nonempty $\\forall x \\in \\mathbb{R}$. So we must simply show that $F$ satisfies the definition of a [[Distribution Function]]. That is, we must show that $F$ is [[Non-Decreasing]] and [[Right-Continuous]].\n\nTo see $F$ is non-decreasing, let $x,z \\in \\mathbb{R}$ s.t. $x \\leq z$. Then we see\n\n$$\\begin{align*}\nF(x) \u0026= \\inf \\{F_{D}(y) : y \\in D, y \u003e x \\}\\\\\n\u0026\\leq \\inf\\{F_{D}(y) : y \\in D, y \u003e z \\geq x\\}\\\\\n\u0026= F(z)\n\\end{align*}$$\n\nTo see $F$ is right-continuous, let $x \\in \\mathbb{R}$, $\\epsilon \u003e 0$. We will find $\\delta \u003e 0$ s.t. $F(z) - F(x) \u003c \\epsilon$ $\\forall z \\in (x, x+\\delta)$. Since [[Finite Extremums get arbitrarily close]], we have that there exists $x \u003c y \\in D$ s.t. $F_{D}(y)- F(x) \u003c \\frac{\\epsilon}{2}$.  Set $\\delta = y - x$. Then for any $z \\in (x, x + \\delta)$ we have that $z \u003c y$ so $F_{D}(y) \\geq F(z)$. But also $z \u003e x$ so $F(z) \\geq F(x)$ since $F$ is non-decreasing. Thus \n\n$$F(z) - F(x) \\leq F_{D}(y) - F(x) \u003c \\frac{\\epsilon}{2} \u003c \\epsilon$$\nshowing that $F$ is right-continuous and that it is a distribution function.\n\nNow let $G$ be a [[Distribution Function]] so that $G {\\big|}_{D} = F_D$. This means $F_{D}$ is [[Non-Decreasing]] and $G$ is [[Right-Continuous]]. Let $x \\in \\mathbb{R}$. Since $D$ is [[Dense]], we can construct a decreasing [[Sequence]] $\\{y_n\\}_{n=1}^{\\infty} \\subset D$ s.t. $y_{n} \\in (x + \\frac{1}{n+1}, x + \\frac{1}{n})$ $\\forall n \\in \\mathbb{N}$. Then \n$$\\{y \\in D : y \u003e x\\} \\supset \\{y_n\\}_{n=1}^{\\infty}$$\nso \n\n$$\\begin{align*}\nF(x) \u0026= \\inf \\{F_{D}(y) : y \\in D, y \u003e x \\} \\\\\n\u0026\\leq \\inf_{y_{n}} F_{D}(y_{n})\\\\\n\u0026= \\lim\\limits_{y_{n} \\to x} G(y_{n})\\\\\n\u0026= G(x)\n\\end{align*}$$\nOn the other hand, for every $y \\in D$ s.t. $y \u003e x$, $\\exists n \\in \\mathbb{N}$ s.t. \n$$y_{n} \u003c x + \\frac{1}{n} \u003c y$$\nThus, by monotonicity of $F_D$,\n$$F_{D}(y) \\geq \\lim\\limits_{y_{n} \\to x} F_{D}(y_{n})= G(x)$$\nso\n$$F(x) = \\inf \\{F_{D}(y) : y \\in D, y \u003e x \\} \\geq G(x)$$\n\ngiving us $G(x) = F(x)$. $\\blacksquare$\n\n\n# Remarks\n1. The first half of the proof does not actually require $D$ to be [[Dense]] or for $F_{D}$ to be [[Non-Decreasing]]. $D$ need only be [[Archimedean]]. This proof gives us a technique for constructing [[Distribution Function]]s from any function on an [[Archimedean]] subset of $\\mathbb{R}$.\n2. If $F$ and $G$ are any two [[Distribution Function]]s that agree on a [[Dense]] subset of $\\mathbb{R}$, we must have that $F = G$, namely because they are both equal to the distribution function generated by their restrictions to the dense set.\n\n# Encounters\n1. [[2022-02-24]] - [[Resnick - A Probability Path]] - pg 248","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Minimum-in-a-Total-Ordering-is-Unique":{"title":"Untitled Page","content":"# Statement\nSuppose $(T, \\leq)$ is a [[Total Ordering]]. Let $A \\subset T$. Then if $x \\in A$ is a [[Minimum|minimal element]] of $A$, $x$ is the only [[Minimum]] of $A$.\n\n# Proof\nLet $x, x' \\in A$ be [[Minimum|minimal elements]]. Since $T$ is a total ordering, we can compare $x$ and $x'$. Since $x$ is a [[Minimum]], we know $x \\leq x'$. However $x'$ is also a [[Minimum]], so $x' \\leq x$. By definition of an [[Order Relation]], we have that $x' = x$ and the [[Minimum]] is unique. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Set-is-Affine-iff-it-contains-all-of-its-Convex-Combinations":{"title":"Untitled Page","content":"# Statement\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. $S$ is a [[Affine Set]] [[If and Only If]] $\\forall n \\in \\mathbb{N}$, $x_{1}, \\dots, x_{n} \\in S$, all [[Affine Combination]]s (of $x_{1}, \\cdots, x_{n}$) $\\lambda_{1} x_{1} + \\cdots + \\lambda_{n} x_{n} \\in S$.\n\n## Proof\n$(\\Leftarrow)$: Let $u, v \\in S$. Note that for $\\lambda \\in \\mathbb{R}$, $\\lambda u + (1 - \\lambda) v = w$  is a [[Affine Combination]] of $u,v$ and, thus, $w \\in S$. Thus, by definition, $S$ is a [[Affine Set]]. $\\checkmark$\n\n$(\\Rightarrow)$: We proceed by [[Induction]]. We skip the trivial case where $n=1$.\n\n*Base Case* ($n = 2$): Let $x_{1}, x_{2} \\in S$ and let $\\lambda_{1}, \\lambda_{2} \\in \\mathbb{R}$ so that $\\lambda_{1} + \\lambda_{2} = 1$. Then $\\lambda_{2} = (1- \\lambda_{1})$. Since $S$ is a [[Affine Set]], we have that $$S \\ni \\lambda_{1}x_{1} + (1 - \\lambda_{1})x_{2} = \\lambda_{1}x_{1} + \\lambda_{2}x_{2}$$ establishing the case when $n = 2$.\n\n*Inductive Step*: Let $n \\in \\mathbb{N}$ so that $n \u003e 2$, and assume the result holds true for $k \\in [n-1]$. Let $\\lambda_{1}, \\dots, \\lambda_{n} \\in \\mathbb{R}$ so that $\\sum\\limits_{i=1}^{n} \\lambda_{i} = 1$. We must have $\\exists j \\in [n]$ so that $\\lambda_{j} \\neq 1$, otherwise $\\sum\\limits_{i=1}^{n} \\lambda_{i} = n \\neq 1$.[[Without Loss of Generality]], let $j = 1$ (otherwise, we can just swap them). Then $\\sum\\limits_{i=2}^{n} \\lambda_{i} = 1 - \\lambda_{1}$ and $\\sum\\limits_{i=2}^{n} \\frac{\\lambda_{i}}{1- \\lambda_{1}} = 1$. By [[Induction]] we know that $$\\frac{\\lambda_{2}}{1-\\lambda_{1}}x_{2} + \\cdots + \\frac{\\lambda_{n}}{1-\\lambda_{1}}x_{n} \\in S.$$ Because $S$ is a [[Affine Set]]\n$$\\begin{align*}\nS \u0026\\ni \\lambda_{1} x_{1} + (1 - \\lambda_{1}) \\left(\\frac{\\lambda_{2}}{1-\\lambda_{1}}x_{2} + \\cdots + \\frac{\\lambda_{n}}{1-\\lambda_{1}}x_{n}\\right)\\\\\n\u0026=\\lambda_{1}x_{1} + \\cdots + \\lambda_{n} x_{n}\n\\end{align*}$$\nso $S$ is closed under [[Affine Combination]]s. $\\checkmark \\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Set-is-Closed-iff-it-is-its-own-Closure":{"title":"Untitled Page","content":"# Statement\nLet $(X, \\tau)$ be a [[Topological Space]]. Then $K \\subset X$ is [[Closed]] [[If and Only If]] $\\text{cl} K = K$.\n\n## Proof\n($\\Rightarrow$) Suppose $K$ is [[Closed]]. A [[Closure]] is defined as\n\n$$\\text{cl} K = \\bigcap\\limits \\{R : R \\text{ is closed in } X, R \\supset K\\}$$\n\n$K$ is one such [[Set]] in the [[Set Intersection|intersection]] so $\\text{cl} K \\subset K$. However, all sets in the intersection contain $K$ so $\\text{cl} K \\supset K$. Therefore $\\text{cl} K = K$. $\\checkmark$\n\n($\\Leftarrow$) Suppose $\\text{cl} K = K$. Since $\\text{cl} K$ is [[Closed]], we have that $K$ is [[Closed]]. $\\checkmark$ $\\blacksquare$\n\n# Other Outlinks\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Set-is-Closed-in-a-First-Countable-Space-iff-it-contains-all-its-Sequential-Limits":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[First Countable Space]]. Then $K \\subset X$ is [[Closed]] [[If and Only If]] $$K \\supset \\{x : \\exists (x_n) \\subset K \\text{ s.t. } x_{n} \\to x\\}$$\nThat is, $K$ contains all its [[Sequence Convergence|sequential limits]].\n\n## Proof\n[[TODO]] ","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Set-is-Closed-in-a-Metric-Space-iff-it-contains-all-its-Sequential-Limits":{"title":"Untitled Page","content":"# Statement\nSuppose $(M, d)$ is a [[Metric Space]] and $S \\subset M$. Then $S$ is [[Closed]] [[If and Only If]] $S$ contains all of it's [[Sequence Convergence|sequential limits]].\n\n## Proof\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Set-is-Convex-iff-it-contains-all-of-its-Convex-Combinations":{"title":"Untitled Page","content":"# Statement\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. $S$ is a [[Convex Set]] [[If and Only If]] $\\forall n \\in \\mathbb{N}$, $x_{1}, \\dots, x_{n} \\in S$, all [[Convex Combination]]s (of $x_{1}, \\cdots, x_{n}$) $\\lambda_{1} x_{1} + \\cdots + \\lambda_{n} x_{n} \\in S$.\n\n## Proof\n$(\\Leftarrow)$: Let $u, v \\in S$. Note that for $\\lambda \\in [0,1]$, $\\lambda u + (1 - \\lambda) v = w$  is a [[Convex Combination]] of $u,v$ and, thus, $w \\in S$. Thus, by definition, $S$ is a [[Convex Set]]. $\\checkmark$\n\n$(\\Rightarrow)$: We proceed by [[Induction]]. We skip the trivial case where $n=1$.\n\n*Base Case* ($n = 2$): Let $x_{1}, x_{2} \\in S$ and let $\\lambda_{1}, \\lambda_{2} \\in [0,1]$ so that $\\lambda_{1} + \\lambda_{2} = 1$. Then $\\lambda_{2} = (1- \\lambda_{1})$. Since $S$ is a [[Convex Set]], we have that $$S \\ni \\lambda_{1}x_{1} + (1 - \\lambda_{1})x_{2} = \\lambda_{1}x_{1} + \\lambda_{2}x_{2}$$ establishing the case when $n = 2$.\n\n*Inductive Step*: Let $n \\in \\mathbb{N}$ so that $n \u003e 2$, and assume the result holds true for $k \\in [n-1]$. Let $\\lambda_{1}, \\dots, \\lambda_{n} \\in [0,1]$ so that $\\sum\\limits_{i=1}^{n} \\lambda_{i} = 1$. If $\\lambda_{1} = 1$, then we can drop $\\lambda_{i} = 0$ for $i \\in \\{2, \\dots, n\\}$ and our [[Convex Combination]] is just $x_{1} \\in S$ $\\checkmark$. Otherwise, assuming $\\lambda_{1} \\neq 1$. Then $\\sum\\limits_{i=2}^{n} \\lambda_{i} = 1 - \\lambda_{1}$ and $\\sum\\limits_{i=2}^{n} \\frac{\\lambda_{i}}{1- \\lambda_{1}} = 1$. By [[Induction]] we know that $$\\frac{\\lambda_{2}}{1-\\lambda_{1}}x_{2} + \\cdots + \\frac{\\lambda_{n}}{1-\\lambda_{1}}x_{n} \\in S.$$ Because $S$ is a [[Convex Set]]\n$$\\begin{align*}\nS \u0026\\ni \\lambda_{1} x_{1} + (1 - \\lambda_{1}) \\left(\\frac{\\lambda_{2}}{1-\\lambda_{1}}x_{2} + \\cdots + \\frac{\\lambda_{n}}{1-\\lambda_{1}}x_{n}\\right)\\\\\n\u0026=\\lambda_{1}x_{1} + \\cdots + \\lambda_{n} x_{n}\n\\end{align*}$$\nso $S$ is closed under [[Convex Combination]]s. $\\checkmark \\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Simple-Function-Induces-a-Finite-Sigma-Algebra":{"title":"Untitled Page","content":"# Statement\nLet $(Y, \\mathcal{N})$ be a [[Measure Space]] and let $X$ be a [[Set]]. Suppose $f: X \\to Y$ is a [[Simple Function]]. Then $\\sigma(f)$ is a [[Finite Set]].\n\n## Proof\nSuppose $f$ is a [[Simple Function]]. Then $f(X) \\subset Y$ is a [[Finite Set]]. Suppose $|f(X)| = n \\in \\mathbb{N}$. Let $E \\in \\mathcal{N}$ be a [[Measureable Set]]. Then\n$$\\begin{align*}\nf^{-1}(E) \u0026= f^{-1}(E) \\cap X \\\\\n\u0026= f^{-1}(E) \\cap f^{-1}(f(X))\\\\\n\u0026=f^{-1}(E \\cap f(X))\n\\end{align*}$$\nSince $E \\cap f(X) \\subset f(X)$ and $f(X)$ is finite, $E \\cap f(X)$ has only $2^{n}$ possibilities. Thus, $|\\sigma(f)| \\leq 2^{n}$ and is thus a [[Finite Set]]. $\\blacksquare$\n\n# Other Outlinks\n* [[Sigma Algebra induced by Function]]\n* [[Function Image]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Subset-of-a-Vector-Space-is-a-Subspace-iff-it-is-closed-under-scaling-and-addition":{"title":"Untitled Page","content":"# Statement\n$W$ is a [[Vector Subspace]] of $V$ [[If and Only If]] $c u + v \\in W$ for all $c \\in F$ and $u, v \\in W$.\n## Proof\n$(\\Rightarrow)$: If $W$ is a [[Vector Subspace]] of $V$, then it is a [[Vector Space]] itself and $c u + v \\in W$ by closure of $W$ under $+$ and $*$. $\\checkmark$\n\n$(\\Leftarrow)$: We simply check that $W$ is a [[Vector Space]] in its own right since we already know $W \\subset V$. We let $c, d \\in F$ and $u, v, w \\in W$ be arbitrary.\n1. [[Abelian Group]]:\n\t1. $W \\ni (-1) u + u = \\mathbf{0}$ because [[Scalar product with -1 is additive inverse in vector spaces]]\n\t2. $W \\ni 1 * u + v = u + v = v + u$, so [[Commutativity]] is satisfied by inheritance from $V$. This also establishes that $+$ is well-defined when restricted to $W$\n\t3. $W \\ni 1 * u + (1 * v + w) = u + (v + w) = (u + v) + w$, so [[Associativity]] is satisfied by inheritance from $V$.\n\t4. $W \\ni (-1) * u + \\mathbf{0} = -u$, so $W$ contains [[Additive Inverse]]s\n2. Compatibility with [[Scalar Multiplication]]. Note that $W \\ni c * u + \\mathbf{0} = c * u$, so $*$ is well-defined when restricted to $W$.\n\t1. $1 * u = u$, by inheritance from $V$.\n\t2. $(cd)*u = c * (d * u)$, by inheritance from $V$.\n\t3. $(c + d) * u = c * u + d * u$ by inheritance from $V$.\n\t4. $c * (u + v) = c * u + c * v$ by inheritance from $V$\n\nThus $+$ and $*$ are well-defined when restricted to $W$, and $W$ is a [[Vector Space]]. $\\blacksquare$\n\n## Remarks\n1. This provides an alternate definition for [[Vector Subspace]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-Union-of-all-Neighborhood-Bases-is-a-Topological-Basis":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[Topological Space]] and for each $x \\in X$, $\\mathcal{B}_{x}$ is a [[Neighborhood Basis]] for $x$. Then $$\\mathcal{B} = \\bigcup\\limits_{x \\in X} \\mathcal{B}_{x}$$ is a [[Topological Basis]] for $X$.\n\n## Proof\nSuppose $U \\subset X$ is [[Open]]. Then for each $x \\in U$, there exists some $B_{x} \\in \\mathcal{B}_{x}$ so that $B_{x} \\subset U$ (by definition of [[Neighborhood Basis]]). Thus\n\n$$\\bigcup\\limits_{x \\in U} B_{x} \\subset U$$\n\nOn the other hand, for each $x \\in U$, we have $x \\in B_{x}$ since $B_{x}$ is in a [[Neighborhood Basis]] for $x$. Thus\n\n$$U \\subset \\bigcup\\limits_{x \\in U} B_{x}$$\n\nTherefore $U = \\bigcup\\limits_{x \\in U} B_{x}$, completing the proof. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-point-in-a-First-Countable-Space-is-a-Limit-Point-iff-it-is-a-Sequential-Limit":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[First Countable Space]] and let $S \\subset X$. Then $\\bar{S} = \\{x :  \\exists (x_n) \\subset S \\text{ s.t. } x_{n} \\to x\\}$. That is, $x \\in X$ is a [[Limit Point]] of $S$ [[If and Only If]] it is a [[Sequence Convergence|sequential limit]] of a [[Sequence]] in $S$.\n\n## Proof\nDenote $T = \\{x :  \\exists (x_n) \\subset S \\text{ s.t. } x_{n} \\to x\\}$. \n\nBecause we know [[Sequential Limits are Limit Points of the Sequence]], we know for each $x \\in T$, there exists $\\{x_n\\}_{n=1}^{\\infty} \\subset S$ so that $x$ is a [[Limit Point]] of $\\{x_{n}\\}$. Since [[Limit Points of a subset are Limit Points of the original Set]], $x$ is also a [[Limit Point]] of $S$ and $\\bar{S} \\supset T$. $\\checkmark$\n\nOn the other hand, suppose $x \\in \\bar{S}$. Since $X$ is a [[First Countable Space]], there is some [[Countable]] [[Neighborhood Basis]] $\\mathcal{B}_{x} \\subset \\tau$ of $x$. Enumerate $\\mathcal{B}_{x} = \\{B_n\\}_{n=1}^{\\infty}$. Construct $(x_{n})$ [[Induction|inductively]] in the following way:\n1. Set $A_{1} = B_{1}$. Since $B_{1}$ is [[Open]] and $x \\in B_{1}$, there exists $x_{1} \\in A_{1} \\cap S$.\n2. Set $A_{2} = B_{2} \\cap A_{1}$. Since $B_{2}$ is [[Open]] and [[Topological Space|topological spaces are closed under finite set intersections]], $A_{2}$ is [[Open]]. Since $x \\in A_{1}$ and $x \\in B_{2}$, we see $x \\in A_{2}$. Thus, as $x$ is a [[Limit Point]], there exists $x_{2} \\in A_{2} \\cap S$.\n3. Continue in this fashion...\n\nNow we show $(x_{n})$ [[Sequence Convergence|converges]] to $x$. Suppose $U \\subset X$ [[Open]] so that $x \\in U$. Then there exists $N \\in \\mathbb{N}$ so that $B_{N} \\subset U$ since $\\mathcal{B}_{x}$ is a [[Neighborhood Basis]] of $x$. By construction, $A_{n} \\subset B_{N}$ for all $n \\geq N$. Since $x_{n} \\in A_{n}$ by construction, we have that for all $n \\geq N$, \n$$x_{n} \\in A_{n} \\subset B_{N} \\subset U$$\nand $(x_{n}) \\to x$. Thus $x \\in T$ and $\\bar{S} \\subset T$. $\\checkmark$\n\nTherefore $\\bar{S} = T$. $\\blacksquare$\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/A-set-is-Closed-iff-it-contains-all-its-Limit-Points":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Then $K \\subset X$ is [[Closed]] [[If and Only If]] $\\bar{K} \\subset K$. That is, $K$ is [[Closed]] [[If and Only If]] for all $x \\in X$ [[Limit Point]] of $K$, $x \\in K$.\n## Proof\n$(\\Rightarrow)$ Since $K$ is [[Closed]] and [[A Set is Closed iff it is its own Closure]], $\\text{cl} K = K$. We also know that $\\bar{K} = \\text{cl} K$. So $\\bar{K} = K$ and thus $\\bar{K} \\subset K$. $\\checkmark$\n\n$(\\Leftarrow)$ Suppose $\\bar{K} \\subset K$. Recall [[All points in a Set are Limit Points]], so $K \\subset \\bar{K}$. Thus $\\bar{K} = K$. Since $\\bar{K} = \\text{cl} K$ and $\\text{cl} K$ is [[Closed]], we have that $K = \\text{cl} K$ is [[Closed]]. $\\checkmark$ $\\blacksquare$\n\n$\\blacksquare$\n\n## Remark\n1. This seems like a trivial result, but I anticipate using it occasionally. It's easier to reference this result than repeat the line of reasoning in the proof everytime\n\n# Other Outlinks\n* [[Closure]]\n* [[Limit Point]]\n* [[A Set is Closed iff it is its own Closure]]\n* [[Closure of a Set is all its Limit Points]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Abelian-Group":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is a [[Group]]. Then if $ab = ba$ for all $a, b \\in G$, we say that $G$ is an [[Abelian Group]].\n\n## Properties\n1. [[Left and Right Inverses in an Abelian Group are the Unique Inverse]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Adjacency-Matrix":{"title":"Untitled Page","content":"# Definition 1\nSuppose $G = (V,E)$ is a [[Directed Graph]]. Then the [[Adjacency Matrix]] of $G$, denoted $\\text{Adj}(G) \\in \\{0, 1\\}^{|V| \\times |V|}$ is the [[Matrix]] with entries\n\n$$\\text{Adj}(G)_{vw} = \\begin{cases}1 \u0026 \\text{if }(v, w) \\in E\\\\ 0 \u0026 \\text{otherwise}\\end{cases}$$\n# Definition 2\nSuppose $G = (V, E)$ is an [[Undirected Graph]]. Let $G'$ be the [[Represent an Undirected Graph as a Directed Graph|Directed Graph representation]] of $G$. Then the [[Adjacency Matrix]] of $G$, denoted $\\text{Adj}(G) \\in \\{0, 1\\}^{|V| \\times |V|}$ is $\\text{Adj}(G')$.\n\n## Properties\n1. [[Adjacency Matrix of an Undirected Graph is Symmetric]]\n2. [[Adjacency Matrix of an Undirected Graph has an all 0 main diagonal]]\n\n# Other Outlinks\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Affine-Combination":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $x_{1}, \\dots, x_{n} \\in V$ for some $n \\in \\mathbb{N}$. Suppose $\\lambda_{1}, \\dots, \\lambda_{n} \\in \\mathbb{R}$ and $\\sum\\limits_{i=1}^{n} \\lambda_{i} = 1$. Then $$\\lambda_{1} x_{1} + \\cdots + \\lambda_{n}x_{n}$$ is an [[Affine Combination]] of $x_{1}, \\dots, x_{n}$.\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Affine-Dimension":{"title":"Untitled Page","content":"# Definition 1\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and let $S \\subset V$ be an [[Affine Set]]. Then the [[Affine Dimension]] of $S$ is the [[Dimension]] of the [[Subspace associated with an Affine Set|subspace associated with]] $S$. \n\n# Definition 2\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and let $S \\subset V$. Then the [[Affine Dimension]] of $S$ is the [[Affine Dimension]] of $\\mathbf{aff} S$. \n\n# Other Outlinks\n* [[Affine Hull]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Affine-Hull":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. Let $\\mathcal{T} = \\{T \\subset V : S \\subset T, T \\text{ is affine}\\}$. Then the [[Affine Hull]] of $S$, denoted $\\mathbf{aff} S$, is defined as $$\\mathbf{aff} S = \\bigcap\\limits_{T \\in \\mathcal{T}} T.$$ \n## Remarks\n1. $\\mathbf{aff} S$ is the smallest [[Affine Set]] containing $S$ (since [[Intersection of Affine Sets is Affine]]).\n2. [[The Set of all Convex Combinations is the Convex Hull]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Affine-Set":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. $S$ is an [[Affine Set]] if for all $\\lambda \\in \\mathbb{R}$ and all $u, v \\in S$\n$$\\lambda u + (1 - \\lambda) v \\in S$$\n## Remarks\n1. $S$ is an [[Affine Set]] [[If and Only If]] $S$ contains all [[Line]]s between its points. This is just a restatement of the definition of a [[Affine Set]].\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Affine-Sets-are-Convex":{"title":"Untitled Page","content":"# Statement\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$ be an [[Affine Set]]. Then $S$ is a [[Convex Set]].\n\n## Proof\nLet $u, v \\in S$ and let $\\lambda \\in [0,1]$. Since $\\lambda \\in \\mathbb{R}$ and $S$ is [[Affine Set|affine]], we have that\n$$\\lambda u + (1 - \\lambda) v \\in S$$\nso $S$ is [[Convex Set|convex]].\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/All-Simple-Functions-can-be-Written-in-Standard-Form":{"title":"Untitled Page","content":"# Statement\nLet $\\phi: X \\to \\mathbb{C}^{n}$ be a [[Simple Function]]. Let $F = \\phi(X)$ (which is a [[Finite Set]]). Let $\\{A_{i}\\} = \\{\\phi^{-1}(\\{a_{i}\\}) \\subset X : a_{i} \\in F\\}$. Then $\\{A_{i}\\}$ [[Partition]]s $X$ and we can write\n\n$$\\phi = \\sum\\limits_{i=1}^{|F|} a_{i}1_{A_{i}}$$\n# Proof\nTo establish that $\\{A_{i}\\}$ partitions $X$, we show that\n1. the collection is [[Disjoint Sets|disjoint]].\n2. the [[Set Union]] of the collection is $X$.\n\nThis is all we need to show since $\\{A_{i}\\} \\subset \\mathcal{P}(X)$ by construction. To see (1) observe that for $i, j \\in [|F|]$ s.t. $i \\neq j$ we have that\n\n$$\\begin{align*}\nA_{i} \\cap A_{j} \u0026= \\phi^{-1}(\\{a_{i}\\}) \\cap \\phi^{-1}(\\{a_{j}\\})\\\\\n\u0026=\\phi^{-1}(\\{a_{i}\\} \\cap \\{a_{j}\\})\\\\\n\u0026= \\phi^{-1}(\\emptyset)\\\\\n\u0026= \\emptyset\n\\end{align*}$$\n[[TODO]] \n\n# Other Outlinks\n* [[Complex Numbers]]\n* [[Function Image]]\n* [[Function Preimage]]\n* [[Indicator Function]]\n* [[Power Set]],\n* [[Set Cardinality]]\n\n# Encounters\n1. [[Caliu - Deep Learning Architectures]] - Appendix, unknown page\n2. [[Folland - Real Analysis]] - Ch 2, unknown page","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/All-points-in-a-Set-are-Limit-Points":{"title":"Untitled Page","content":"# Statement\nLet $X, \\tau$ be a [[Topological Space]] and let $S \\subset X$. Then $$\\bar{S} \\supset S$$\n\n## Proof\nSuppose $x \\in S$. Suppose $U \\subset X$ be [[Open]] s.t. $x \\in U$. Then $U \\cap S \\supset \\{x\\}$. Since $U$ was arbitrary, $x$ is a [[Limit Point]] of $S$. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Almost-Everywhere-Pointwise-Convergence":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\mathcal{M}, \\mu)$ is a [[Measure Space]] and $(Y, \\tau)$ is a [[Topological Space]]. Suppose $(f_{n})$ is a [[Sequence]] of [[Function]]s from $X$ to $Y$ and suppose $f: X \\to Y$. If there exists a set $N \\subset X$ so that\n1. $\\mu(N) = 0$\n2. $\\lim\\limits_{n \\to \\infty} f_{n}(x) = f(x)$ for all $x \\in N^{C}$.\n\nthen we say $f_{n}$ [[Almost Everywhere Pointwise Convergence|converges pointwise almost everywhere]] to $f$.\n\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Almost-Sure-Convergence":{"title":"Untitled Page","content":"# Definition\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]] and $(Y, \\tau)$ is a [[Topological Space]]. Suppose $(X_{n} : \\Omega \\to Y)$ is a [[Sequence]] of [[Random Element]]s and $X: \\Omega \\to Y$ is a [[Random Element]]. We say $X_{n} \\to X$ [[Almost Sure Convergence|almost surely]] if $X_{n}$ [[Almost Everywhere Pointwise Convergence|converges almost everywhere]] to $X$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Almost-Sure-Convergence-does-not-imply-Lp-Convergence":{"title":"Untitled Page","content":"# Statement\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]] and $(X_{n})$ is a [[Sequence]] of [[Random Variable]]s on $\\Omega$. Suppose $X_{n} \\to X$ [[Almost Sure Convergence|almost surely]]. Then, it does not follow that $X_{n} \\to X$ in $L^{p}$ for any $p \\geq 1$.\n\n## Proof\nWe prove by constructing a [[Counterexample]]. Suppose $(X_{n})$ is a [[Sequence]] of [[Random Variable]]s defined on [[Probability Space]] $([0,1], \\mathcal{B}([0,1]), \\lambda)$ and for $x \\in [0,1]$\n\n$$X_{n}(x) = \\begin{cases} n^{2} \u0026 x \\in [0, \\frac{1}{n}) \\\\ 0 \u0026 x \\in [\\frac{1}{n}, 1] \\end{cases}$$\nThen for $x \\in (0, 1]$, if $n \\geq \\lceil{\\frac{1}{x}}\\rceil$, we have that $X_{n}(x) = 0$. Thus on $(0, 1]$, $X_{n} \\to 0$ [[Pointwise Convergence|pointwise]]. Since $(0, 1]^{C} = \\{0\\}$ is a [[Null Set]], we have that $X_{n} \\to 0$ [[Almost Sure Convergence|almost surely]].\n\nNow suppose $X_{n} \\overset{L^{p}}{\\to} X$ for some $p \\geq 1$. Recall that [[Lp Spaces are Banach]] so $X \\in L^{p}$. Then, because [[Norms are Continous]]:\n\n$$\\mathbb{E}(|X_{n}|^{p})^{\\frac{1}{p}} \\to \\mathbb{E}(|X|^{p})^{\\frac{1}{p}}$$\nand therefore\n$$\\mathbb{E}(|X_{n}|^{p}) \\to \\mathbb{E}(|X|^{p})$$\nNow observe that,\n$$\\begin{align*}\n\\mathbb{E}(|X_{n}|^{p}) \u0026= \\frac{n^{2p}}{n}\\\\\n\u0026=n^{2p-1}\\\\\n\u0026\\geq n^{1} \u0026(\\text{since }p \\geq 1)\\\\\n\u0026\\to \\infty\\\\\n\\end{align*}$$\nThis [[Proof by Contradiction|contradicts]] $X_{n} \\overset{L^{p}}{\\to} X$ and therefore $X_{n}$ does not [[Lp Convergence|converge in Lp]]. $\\blacksquare$\n\n\n# Other Outlinks\n* [[Lp Convergence]]\n* [[Borel Sigma Algebra]]\n* [[Lebesgue Measure]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Andr%C3%A9s-E.-Caicedo":{"title":"Untitled Page","content":"[Math StackExchange User](https://math.stackexchange.com/users/462/andr%c3%a9s-e-caicedo)","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Augmenting-Network-Flow-Paths-for-Bipartite-Matching-Flow-Formulation-are-Matching-Augmenting-Paths":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Augmenting-Path-Theorem-of-Matchings":{"title":"Untitled Page","content":"# Statement\nSuppose $G = (V, E)$ is an [[Undirected Graph]] and $M$ is a [[Matching]] on $G$. Then $M$ is a [[Maximum Matching]] on $G$ [[If and Only If]] there is no $M$-[[Matching Augmenting Path|augmenting path]].\n\n## Proof\n[[TODO]] - See [[Cook - Combinatorial Optimization]] page 129 for details.\n\n# Sources\n* [[Cook - Combinatorial Optimization]] - Ch 5, Section 5.1, page 129\n* [Berge (1957)](https://drive.google.com/file/d/1OWKvZt7TBlg_jgA_LesuBMRLJubmYYvN/view?usp=sharing) - original source\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Baby-Skorohod-Theorem":{"title":"Untitled Page","content":"# Statement\nLet $\\{X_{n} : n \\geq 0\\}$ be a [[Sequence]] of [[Random Variable]]s (not necessarily defined on the same [[Probability Space]]) so that $X_{n} \\Rightarrow X_{0}$. Then there exist $\\{X^{\\#}_{n} : n \\geq 0 \\}$ defined on [[Probability Space]] $([0,1], \\mathcal{B}([0,1]), \\lambda)$ (where $\\lambda$ is the [[Lebesgue Measure]]) so that \n\n$$\\begin{align*}\n\u0026X_{n}^{\\#} \\overset{d}= X_{n} \\text{ for } n \\geq 0\\\\\n\u0026X_{n}^{\\#} \\to X_{0}^{\\#} \\text{ a.s.}\\\\\n\\end{align*}$$\n\n\n# Proof\n\n# Other Outlinks\n* [[Convergence in Distribution]]\n* [[Almost Sure Convergence]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bayes-Classifier":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. The [[Bayes Classifier]] is a [[Classifier]] $f: \\mathcal{D} \\to [n]$ so that for $x \\in \\mathcal{D}$\n\n$$f(x) = \\arg\\min_{y \\in [n]} \\mathbb{P}(Y=y|X=x)$$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bayes-Classifier-is-Risk-Minimizer":{"title":"Untitled Page","content":"# Statement\nLet $X, Y, n$ be a [[Classification Problem]]. Then the [[Bayes Classifier]] minimizes [[Risk]]. It is the unique minimizer up to modifications on $\\mathbb{P}$-[[Null Set]]s of $\\mathcal{D}$ and subsets of $E = \\{x \\in \\mathcal{D} : \\mathbb{P}(Y=y|X) = \\frac{1}{n} \\forall y \\in [n]\\}$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bayes-Soft-Classifier-is-Cross-Entropy-Minimizer":{"title":"Untitled Page","content":"# Statement\nLet $X, Y, n$ be a [[Classification Problem]]. Then the [[Likelihood Soft Classifier]] minimizes [[Cross-Entropy]]. It is the unique minimizer up to modifications on $\\mathbb{P}$-[[Null Set]]s of $\\mathcal{D}$.\n\n## Proof\nRecall that we can write [[Cross-Entropy]] as\n$$CE(g) = \\mathbb{H}(Y|X) + \\mathbb{E}\\Big[D_{KL}(\\mathbb{P}(Y|X) || g(X) )$$\n\n[[Gibb's Inequality]] tells us that $D_{KL}(\\mathbb{P}(Y|X) || g(X) )$ is minimized precisely when $g(X) = \\mathbb{P}(Y|X)$, which is the [[Likelihood Soft Classifier]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bayes-Theorem":{"title":"Untitled Page","content":"# Statement 1\nSuppose $(\\Omega, \\mathcal{B}, \\mathbb{P})$ is a [[Probability Space]] and $A, B \\in \\mathcal{B}$ so that $\\mathbb{P}(B), \\mathbb{P}(A) \u003e 0$. Then\n\n$$\\mathbb{P}(B | A) = \\frac{\\mathbb{P}(A | B) \\mathbb{P}(B)}{\\mathbb{P}(A)}$$\n\nNote that all values are [[Well-Defined]] by are assumption that $A,B$ are not [[Null Set]]s.\n\n## Proof\nThis follows from Definition 2 of [[Conditional Probability]]:\n\n$$\\mathbb{P}(B | A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} = \\frac{\\mathbb{P}(A | B) \\mathbb{P}(B)}{\\mathbb{P}(A)}$$\n\n$\\blacksquare$\n\n# Statement 2\n\n\n# Other Outlinks\n* [[Conditional Probability]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Binary-Cross-Entropy":{"title":"Untitled Page","content":"# Definition\nThis is just [[Cross-Entropy]] when $|\\mathcal{L}| = 2$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Binary-Entropy":{"title":"Untitled Page","content":"[[Shannon Entropy]] of a [[Bernoulli Distribution]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bipartite-Graph":{"title":"Untitled Page","content":"# Definition\nA [[Bipartite Graph]] is an [[Undirected Graph]] $(V, E)$ so that there exists a [[Partition]] $\\{A, B\\}$ of $V$ s.t. $\\forall P \\in \\{A, B\\}$, $\\forall v, w \\in P$, $\\{v, w\\} \\not\\in E$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bit-Error-Rate":{"title":"Untitled Page","content":"[[Risk]] on each bit of the input sequence.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bolzano-Weierstrass-Theorem":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Borel-Measureable-Function":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Bounded-Monotone-Convergence-Theorem":{"title":"Untitled Page","content":"A special case of [[Every Sequence on the Reals contains a Monotone Subsequence]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Characteristic-Function":{"title":"Untitled Page","content":"# Definition\nSuppose $X$ is a [[Random Variable]] on [[Probability Space]] $(\\Omega, \\mathcal{M}, \\mathbb{P})$. The [[Characteristic Function]] of $X$ is a function $\\phi_{X}: \\mathbb{R} \\to \\mathbb{C}$ defined as\n\n$$\\phi_{X}(t) = \\mathbb{E}(e^{itX})$$\n\n## Elementary Properties\n1. $X$ is a [[Random Variable]]. $\\phi_{X}(t)$ exists from all $t \\in \\mathbb{R}$. This follows because $|e^{itX}| = 1$. Thus, $\\mathbb{E}(|e^{itX}|) = \\mathbb{E}(1) = 1$ and $e^{itX} \\in L^{1}(\\Omega)$.\n2. $X, Y$ are [[Independence|Independent]] [[Random Variable]]s. Then\n\t$$\\begin{align*}\n\\phi_{X+Y}(t) \u0026= \\mathbb{E}(e^{it(X+Y)})\\\\\n\u0026=\\mathbb{E}(e^{itX} e^{itY})\\\\\n\u0026=\\mathbb{E}(e^{itX}) \\mathbb{E}(e^{itY}) \u0026 \\text{(Independence)}\\\\\n\u0026=\\phi_{X}(t) \\phi_{Y}(t)\n\\end{align*}$$\n3. $X$ is a [[Random Variable]], $a, b \\in \\mathbb{R}$. Then\n\t$$\\begin{align*}\n\\phi_{aX+b}(t) \u0026= \\mathbb{E}(e^{it(aX+b)})\\\\\\\\\n\u0026=\\mathbb{E}(e^{itaX} e^{itb})\\\\\n\u0026=e^{itb}\\mathbb{E}(e^{i(ta)X})\\\\\n\u0026=e^{itb} \\phi_{X}(ta)\n\\end{align*}$$\n4. $X$ is a [[Random Variable]]. Applying [[Euler's Formula]]\n\n### Passing the [[Expectation]] into the [[Taylor Expansion of the Exponential]]\n\n$X$ is a [[Random Variable]]. We would like to understand how we can apply the [[Taylor Expansion of the Exponential]] to our definition of a [[Characteristic Function]].\n$$\\begin{align*}\n\\phi_{X}(t) \u0026= \\mathbb{E}(e^{itX})\\\\\n\u0026=\\mathbb{E}\\left(1 + \\sum\\limits_{k=1}^{\\infty} \\frac{(itX)^{k}}{k!}\\right)\n\\end{align*}$$\nwhere the last equality makes sense because the [[Taylor Expansion of the Exponential has unbounded Radius of Convergence]]. However, it does not necessarily follow that we can \"apply\" [[Linearity of Expectation]] to the sum. For example, the example from [[Expectation does not always exist]] has characteristic function value for each $t \\in \\mathbb{R}$ by property (1), but passing the expectation through the [[Taylor Expansion of the Exponential|taylor expansion]] has no clear meaning.\n\nHowever, with some more assumptions we can say more:\n\n#### Existence of [[Moment Generating Function]]\nSuppose $m_{X}(t) = \\mathbb{E}(e^{tX})$ exists for some $t \u003e 0$. Observe that \n$$\\begin{align*}\ne^{|tX|} \u0026= 1 + \\sum\\limits_{k=1}^{\\infty} \\frac{|tX|^{k}}{k!} \\\\\n\u0026= 1 + \\sum\\limits_{k=1}^{\\infty} \\frac{|itX|^{k}}{k!}\\\\\n\u0026= 1 + \\sum\\limits_{k=1}^{n} \\frac{|itX|^{k}}{k!}\\\\\n\u0026\\geq \\bigg| 1 + \\sum\\limits_{k=1}^{n} \\frac{(itX)^{k}}{k!} \\bigg| \\\\\n\\end{align*}$$\n[[Existence of Moment Generating Functions imply Existence of Moments]], so [[TODO]].\n\n$$\\begin{align*}\n\u0026\\mathbb{E}(e^{tX} 1_{X \\geq 0}) \\leq \\mathbb{E}(e^{tX})\\\\\n\u0026\\mathbb{E}(e^{-tX} 1_{X \u003c 0}) \\leq \\mathbb{E}(e^{-tX})\\\\\n\u0026\\mathbb{E}(e^{tX} 1_{X \\geq 0}) + \\mathbb{E}(e^{-tX} 1_{X \u003c 0}) = \\mathbb{E}(e^{t|X|}) \\leq m_{X}(t) + m_{X}(-t)\n\\end{align*}$$\n\n\n\n\n# Other Outlinks\n* [[Complex Numbers]]\n* [[Complex Integration]]\n* [[Real Numbers]]\n* [[L1 Integrable Functions]]\n# Encounters\n1. [[Resnick - A Probability Path]] - Ch 9\n2. [[Revuz - Continuous Martingales and Brownian Motion]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Characteristic-Function-of-Standard-Gaussian-Random-Variable":{"title":"Untitled Page","content":"# Statement\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]] and $X$ is a [[Standard Gaussian Random Variable]]. Then\n\n$$\\phi_{X}(t) = \\exp (\\frac{t^{2}}{2})$$\n\n## Proof\nSince $X$ is a [[Standard Gaussian Random Variable]], we know $X \\sim \\mathcal{N}(0, 1)$. Applying the formula for the [[Characteristic Function of a Gaussian Random Variable]]:\n\n$$\\begin{align*}\n\\phi_{X}(t) \u0026= \\exp (i t 0 - \\frac{1^{2} t^{2}}{2})\\\\\n\u0026=\\exp (\\frac{t^{2}}{2})\n\\end{align*}$$\n$\\blacksquare$\n\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Characteristic-Function-of-a-Gaussian-Random-Variable":{"title":"Untitled Page","content":"# Statement\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]] and $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. Then\n\n$$\\phi_{X}(t) = \\exp (i t \\mu - \\frac{\\sigma^{2} t^{2}}{2})$$\n\n## Proof\n\n\n# Other Outlinks\n* [[Gaussian Random Variable]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Characterization-of-Convex-Polytope-Faces":{"title":"Untitled Page","content":"# Statement\n[[TODO]] - [[Convex Polytope Face]]s are intersection of subset of [[Convex Polytope]] and solutions to subsystem equlaity","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Characterization-of-Minimal-Faces":{"title":"Untitled Page","content":"# Statement\n[[TODO]] - fully defined by a subsystem of equality constraints\n* [[Polytope]]\n* [[Minimal Face]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Class":{"title":"Untitled Page","content":"# Defintion\nA [[Class]] is an association of [[Object]]s (which are not [[Class]]es) that share some property. ","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Classification-Problem":{"title":"Untitled Page","content":"# Definition\nLet $(\\Omega, \\mathcal{M}, \\mathbb{P})$ be a [[Probability Space]]. Let $(\\mathcal{D}, \\Sigma_{1})$ and $([n], \\mathcal{P}([n]))$ be [[Measure Space]]s (for $n \\in \\mathbb{N}$). Let $X: \\Omega \\to \\mathcal{D}$ and $Y: \\Omega \\to [n]$ be [[Random Element]]s representing [[Data]] and [[Label]]s respectively. A [[Classification Problem]] is a [[Tuple]] $(\\Omega, \\mathcal{M}, \\mathbb{P}, \\mathcal{D}, \\Sigma_{1}, n, X, Y)$.\n\n## Remarks\n1. The goal of this problem is to find a [[Function]] $f: \\mathcal{D} \\to [n]$ so that the [[Risk]] of $f$ is minimized. We say $f$ is a [[Classifier]]\n2. This definition is a bit unwieldy. We usually just refer to the problem as $(X, Y, n)$ or $(X, Y)$ instantiating all other [[Object]]s implicitly.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Classifier":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. Then a [[Classifier]] is a [[Function]] $f: \\mathcal{D} \\to [n]$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closed":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Then $K \\subset X$ is [[Closed]] if $K^{C}$ is [[Open]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closed-Halfspace":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$. A [[Closed Halfspace]] is a [[Set]] of the form\n$$H = \\{x : \\langle x, a \\rangle \\geq b\\}$$\nfor any $a \\in V$ and $b \\in \\mathbb{R}$.\n\n## Remarks\n1. A [[Closed Halfspace]] is [[Closed]]. [[TODO]] - prove this.\n2. If $H$ is an [[Closed Halfspace]], then $\\text{int} H$ is an [[Open Halfspace]] - [[TODO]] prove this.\n3. The [[Boundary]] of a [[Closed Halfspace]] is a [[Hyperplane]] - [[TODO]] prove this.\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Closure]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closed-Line-Segment":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $x_{1}, x_{2} \\in V$. The [[Closed Line Segment]] $L \\subset V$ between $x_{1}$ and $x_{2}$ the [[Set]] $$L = \\{\\lambda x_{1} + (1 - \\lambda) x_{2} : \\lambda \\in [0,1]\\}$$\n## Remarks\n1. $L$ can also be expressed as $L = \\{x_{2} + \\lambda (x_{1} - x_{2}) : \\lambda \\in [0,1] \\}$. This follows from the observation that $$\\lambda x_{1} + (1 - \\lambda) x_{2} = x_{2} + \\lambda x_{1} - \\lambda x_{2} = x_{2} + \\lambda (x_{1} - x_{2})$$\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closure":{"title":"Untitled Page","content":"# Definition\nLet $X, \\tau$ be a [[Topological Space]] and let $S \\subset X$. Then the [[Closure]] of $S$, denoted $\\text{cl}S$ is\n\n$$\\text{cl}S := \\bigcap\\limits \\{K \\subset X : K \\supset S, K \\text{ is closed in } X\\}$$\n\n## Properties\n1. $\\text{cl} S$ is [[Closed]] in $X$ since an [[Intersection of Closed Sets is Closed]]\n2. $\\text{cl}S = \\bar{S} := \\bigcup\\limits \\{x \\in X : x \\text{ is a limit point of } S\\}$. That is, [[Closure of a Set is all its Limit Points]].\n\n2. $\\text{cl}S = X \\setminus \\text{int} S^{C}$\n\n### Proof\n[[TODO]]\n\n# Other Outlinks\n* [[Closed]]\n* [[Set Intersection]]\n* [[Limit Point]]\n* [[Interior]]\n* [[Set]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closure-of-a-Set-in-a-First-Countable-Space-is-all-its-Sequential-Limits":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[First Countable Space]]. Suppose $S \\subset X$ Then $$\\text{cl} S = \\{x :  \\exists (x_n) \\subset S \\text{ s.t. } x_{n} \\to x\\}$$\n\n## Proof\nDenote $T = \\{x :  \\exists (x_n) \\subset S \\text{ s.t. } x_{n} \\to x\\}$. Since the [[Closure of a Set is all its Limit Points]] and [[A point in a First Countable Space is a Limit Point iff it is a Sequential Limit]], we see\n$$\\text{cl} S = \\bar{S} = T$$\n$\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Closure-of-a-Set-is-all-its-Limit-Points":{"title":"Untitled Page","content":"# Statement\nLet $X, \\tau$ be a [[Topological Space]] and let $S \\subset X$. Then $$\\text{cl} S = \\bar{S}$$\n\n## Proof\nFirst we show $\\bar{S}$ is [[Closed]]. Because [[All points in a Set are Limit Points]], $\\bar{S} \\supset S$, so if we show $\\bar{S}$ is [[Closed]], then $\\bar{S} \\supset \\text{cl}S$. To see this, we will shoe $\\bar{S}^{C}$ is [[Open]]. Suppose $y \\in \\bar{S}^{C}$. Then, by definition of $\\bar{S}$, we know that there exists $U_{y} \\in \\tau$ s.t. $y \\in U_{y}$ but $U_{y} \\cap S = \\emptyset$. Then\n$$U = \\bigcup\\limits_{y \\in \\bar{S}}^{C} U_{y}$$\nis [[Open]]. We see $U \\supset \\bar{S}^{C}$ since for $y \\in \\bar{S}^{C}$ there is a $U_{y}$ that contains it. To see $U \\subset \\bar{S}^{C}$, observe that if it weren't then it would contain some [[Limit Point]] $x$ of $S$. This means there is some $y \\in \\bar{S}^{C}$ so that $x \\in U_{y}$. By definition of [[Limit Point]], we have that $U_{y} \\cap S \\neq \\emptyset$, [[Proof by Contradiction|contradicting]] our assumption that $U_{y}  \\cap S = \\emptyset$. Thus $U = \\bar{S}^{C}$ and $\\bar{S}$ is [[Closed]]. \n\nNext we show every [[Closed]] set containing $S$ contains all of the [[Limit Point]]s of $S$. Suppose $K \\supset S$ is [[Closed]]. Since $K$ is [[Closed]], $K^{C}$ is [[Open]]. Suppose there was some [[Limit Point]] $x$ of $S$ so that $x \\not\\in K$. Then $x \\in K^{C}$. Thus, by definition of [[Limit Point]], $K^{C} \\cap S \\neq \\emptyset$. But then $K^{C} \\cap K \\neq \\emptyset$, giving us a [[Proof by Contradiction|contradiction]]. Thus $\\bar{S} \\subset K$. Since $K$ was arbitrary, we have that $\\bar{S} \\subset \\text{cl}S$. \n\nThus, we conclude that $\\bar{S} = \\text{cl}S$. $\\blacksquare$\n\n# Other Outlinks\n* [[Closure]]\n* [[Limit Point]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Commutativity":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Commuting-Transforms":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Compact-Set":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Compactification":{"title":"Untitled Page","content":"![](attachments/tao.compactness.pdf)","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Complete-Bipartite-Graph":{"title":"Untitled Page","content":"# Definition\nLet $G=(V, E)$ be a [[Bipartite Graph]] with partition $\\{A, B\\} \\subset \\mathcal{P}(V)$. Then we say that $G$ is a [[Complete Bipartite Graph]] if\n$$E = \\{\\{u, v\\} \\in \\mathcal{P}(V) : u \\in A, v \\in B\\}.$$\nIf $V = [n]$ for some $n \\in \\mathbb{N}$, then we notate $G = B_{n}$.\n\n# Other Outlinks\n* [[Natural Numbers]]\n* [[Power Set]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Complete-Graph":{"title":"Untitled Page","content":"# Definition\nA [[Complete Graph]] is an [[Undirected Graph]] $(V, E)$ s.t. $E = \\{\\{v, w\\} : v,w \\in V, v \\neq w\\}$. We denote it $K_{V}$. If $V$ is taken to be $[n]$ for some $n \\in \\mathbb{N}$, then we just denote it $K_{n}$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Complete-Ordering":{"title":"Untitled Page","content":"# Definition\nLet $(T, \\leq)$ be a [[Total Ordering]] on $T$. Then $\\leq$ is a [[Complete Ordering]] [[Order Relation|relation]] if for every [[Set]] $A \\subset T$ s.t. there exists $x \\in T$ for which $A \\leq x$ ($y \\leq x$ $\\forall y \\in A$), then $\\sup A$ exists in $T$.\n\nAlso known as the [[Least Upper Bound Property]].\n\n# Other Outlinks\n* [[Supremum]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Completeness-of-the-Real-Numbers":{"title":"Untitled Page","content":"# Statement\nWe can state this in the following 6 ways all of which are equivalent:\n1. $\\mathbb{R}$ with its usual [[Total Ordering]] $\\leq$ is a [[Complete Ordering]]\n2. Every [[Cauchy Sequence]] of $\\mathbb{R}$ [[Sequence Convergence|converges]] in $\\mathbb{R}$.\n3. $\\mathbb{R}$ satisfies the [[Nested Intervals Theorem]]\n4. $\\mathbb{R}$ satisfies the [[Bounded Monotone Convergence Theorem]].\n5. $\\mathbb{R}$ satisfies the [[Bolzano-Weierstrass Theorem]].\n6. $\\mathbb{R}$ satisfies the [[Intermediate Value Theorem]]\n\n# Completing $\\mathbb{R}$\n## Approach 1: Completion by [[Cauchy Sequence]]s\n[[TODO]] In this approach we define a notion of [[Cauchy Sequence]]s on $\\mathbb{Q}$ and represent $\\mathbb{R}$ as [[Cauchy Sequence]]s on $\\mathbb{Q}$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Concave-Function":{"title":"Untitled Page","content":"# Definition\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Entropy":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation":{"title":"Untitled Page","content":"# Definition 1\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] in $\\bar{L^{1}}(\\mathcal{B})$. Let $\\mathcal{G} \\subset \\mathcal{B}$ be a sub-[[Sigma Algebra]]. Then the [[Conditional Expectation]] of $X$ (denoted $\\mathbb{E}[X | \\mathcal{G}]$) with respect to $\\mathcal{G}$ is a  [[Random Variable]] in $\\bar{L^{1}}(\\mathcal{G})$ such that $\\forall A \\in \\mathcal{G}$\n\n$$\\int\\limits_{A} \\mathbb{E}[X | \\mathcal{G}] d \\mathbb{P}(\\omega) = \\int\\limits_{A} X d \\mathbb{P}(\\omega)$$\n\nWhen we talk about [[Conditional Expectation]] with respect to another [[Random Variable]] $Y$, use the shorthand\n$$\\mathbb{E}[X | Y] = \\mathbb{E}[X | \\sigma(Y)]$$\n\nWhen we talk about [[Conditional Expectation]] with respect to  a [[Random Variable]] $Y$ equal to $y \\in \\mathbb{R}$, we use the shorthand\n$$\\mathbb{E}[X | Y=y] = \\mathbb{E}[X | Y](\\omega)$$\nfor (any) $\\omega \\in Y^{-1}(\\{y\\})$. Note that this value may not be [[Well-Defined]], but given a particular [[Conditional Expectation]], we can get a reasonable answer [[Almost Surely]].\n\nWhen we talk about [[Conditional Expectation]] with respect to a [[Set]] $B \\in \\mathcal{B}$, we use the shorthand:\n$$\\mathbb{E}[X|B] := \\mathbb{E}[X|1_{B}](\\omega)$$\nfor (any) $\\omega \\in B$. Note that this value is only [[Well-Defined]] if $\\mathbb{P}(B) \u003e 0$. For details on its exact form, see [[Conditional Expectation with respect to a Set is Integral divided by Probability]].\n\n## Remarks\n1. We require $X$ to be in $\\bar{L^{1}}$ so the above integral is defined. \n2. With [[Conditional Expectation]] with respect to a [[Set]], it does not matter what $\\omega \\in B$ we choose, the value is the same. To see this, recall that $\\sigma(1_{B}) = \\{\\emptyset, B, B^{C}, \\Omega\\}$. Since $\\mathbb{E}[X|1_{B}]$ is $\\sigma(1_{B})$-[[Borel Measureable Function|measureable]], $\\sigma(\\mathbb{E}[X|1_{B}]) \\subset \\sigma(1_{B})$. Let $\\omega \\in B$. Since [[Singletons are Closed in the Real Numbers]], we know $\\{\\mathbb{E}[X|1_{B}](\\omega)\\} \\in \\mathcal{B}(\\mathbb{R})$. Thus we have that $\\mathbb{E}[X|1_{B}]^{-1}(\\{\\mathbb{E}[X|1_{B}](\\omega)\\}) \\in \\sigma(1_{B})$ and it is not [[Disjoint Sets|disjoint]] with $B$. The only two sets that satisfy this criterion in $\\sigma(1_{B})$ are $\\Omega$ and $B$ itself. In either case, $\\mathbb{E}[X|1_{B}](B) = \\{\\mathbb{E}[X|1_{B}](\\omega)\\}$ and $\\mathbb{E}[X|1_{B}]$ is constant on $B$. $\\blacksquare$\n\n## Properties\n1. [[Conditional Expectation Exists and is Almost Surely Unique]]\n2. [[Conditioning on known information is Idempotent]]\n3. [[Conditional Expectation over the Trivial Sigma Algebra is Expectation]]\n4. [[Smoothing]]\n5. [[Conditional Expectation is Linear]]\n6. [[Conditional Expectation is Non-Decreasing]]\n7. [[Conditional Expectation satisfies Jensen's Inequality]]\n8. [[Conditional Expectation with respect to a Set is Integral divided by Probability]]\n \n\nI want to connect to the ways I think of [[Conditional Expectation]]. For example\n1. I think of conditional expectation (especially in the sense discrete random variables) as expectation over the  conditional probability. Maybe it would be good to define conditional probability then connect it to conditional expectation through there.\n# Definition 2\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Discrete Random Variable]] on $\\Omega$ with [[Support]] $\\{k_{n}\\}_{n} \\subset \\mathbb{R}$. Let $Y$ \n \n# Other Outlinks\n* [[Extended L1 Functions]]\n* [[Sigma Algebra induced by Random Variable]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation-Algorithm-for-Max-Cut-approximation":{"title":"Untitled Page","content":"[[TODO]] - see [[MCS 521 - Combinatorial Optimization]] Homework 3","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation-Exists-and-is-Almost-Surely-Unique":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] on $\\Omega$. Let $\\mathcal{G} \\subset \\mathcal{B}$ be a sub-[[Sigma Algebra]]. Then there exists a [[Conditional Expectation]]  $Y$ for $X$ with respect to $\\mathcal{G}$. If $Y'$ is another such [[Conditional Expectation]] then $$Y = Y' \\text{ almost surely}$$\n\nTherefore, it makes sense to refer to $\\mathbb{E}[X|\\mathcal{G}]$ as **the** [[Conditional Expectation]] of $X$ with respect to $\\mathcal{G}$.\n\n## Proof\nWrite for $A \\in \\mathcal{G}$\n\n$$\\nu(A) = \\int\\limits_{A} X d \\mathbb{P}(\\omega)$$\n\nThen, because [[Integration defines an Absolutely Continuous Measure]], we know that $\\nu$ is a [[Measure]] on $\\mathcal{G}$ and $\\nu \u003c\u003c \\mathbb{P} {\\big|}_{\\mathcal{G}}$. Thus, there exists a $\\mathbb{P}$-[[Almost Surely]] $\\mathcal{G}$-[[Measureable Function|measureable]] unique [[Radon-Nikodym Derivative]] $Z = \\frac{d\\nu}{d \\mathbb{P} {\\big|}_{\\mathcal{G}}}$ so that for $A \\in \\mathcal{G}$\n\n$$\\nu(A) = \\int\\limits_{A} Z d \\mathbb{P}(\\omega) = \\int\\limits_{A} X d \\mathbb{P}(\\omega)$$\n\nThus $Z$ satisfies the definition of [[Conditional Expectation]]. Since $Z$ is $\\mathbb{P}$-[[Almost Surely]] unique, any other $\\mathcal{G}$-[[Measureable Function|measureable]] [[Random Variable]] satisfying the definition of [[Conditional Expectation]] is also a [[Radon-Nikodym Derivative]] and is thus [[Almost Surely]] $Z$. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation-is-Linear":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X, Y$ be [[Random Variable]]s in $\\bar{L^{1}}(\\mathcal{B})$. Let $\\mathcal{G} \\subset \\mathcal{B}$ be a sub-[[Sigma Algebra]] of $\\mathcal{B}$. Then,\n\n$$\\mathbb{E}(cX + Y | \\mathcal{G}) = c \\mathbb{E}(X|\\mathcal{G}) + \\mathbb{E}(Y|\\mathcal{G})$$\n## Proof\n[[TODO]] ","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation-over-the-Trivial-Sigma-Algebra-is-Expectation":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] on $\\Omega$. Then\n\n$$\\mathbb{E}[X| \\{\\emptyset, \\Omega\\}] = \\mathbb{E}[X]$$\n\n## Proof\nObserve that $\\mathbb{E}[X]$ is indeed trivially measureable since [[Constant Functions are Measureable]]. Furthermore\n\n$$\\int\\limits_{\\Omega} X d \\mathbb{P}(\\omega) = \\mathbb{E}[X] = \\int\\limits_{\\Omega} \\mathbb{E}[X]$$\n\nand \n\n$$\\int\\limits_{\\emptyset} X d \\mathbb{P}(\\omega) = 0 = \\int\\limits_{\\emptyset} \\mathbb{E}[X]$$\n\nso $\\mathbb{E}[X]$ is the [[Conditional Expectation]] of $X$ with respect to $\\{\\emptyset, \\Omega\\}$. $\\blacksquare$\n\n# Other Outlinks\n* [[Conditional Expectation]]\n* [[Expectation]]\n* [[Trivial Sigma Algebra]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Expectation-with-respect-to-a-Set-is-Integral-divided-by-Probability":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] in $\\bar{L^{1}}(\\mathcal{B})$. Suppose $B \\in \\mathcal{B}$ so that $\\mathbb{P}(B) \\neq 0$. Then \n\n$$\\mathbb{E}(X|B) = \\frac{1}{\\mathbb{P}(B)} \\int\\limits_{B} X d \\mathbb{P}(\\omega) = \\frac{\\mathbb{E}(1_{B} X)}{\\mathbb{P}(B)}$$\n## Proof\nRecall that $\\mathbb{E}(X|B) = \\mathbb{E}(X|1_{B})(\\omega)$ for any $\\omega \\in B$. So we are really checking that \n$$\\mathbb{E}(X|1_{B})(\\omega) = \n\t\\begin{cases} \n\t\t\\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} \u0026\\text{if } \\omega \\in B\\\\\n\t\t\\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} \u0026\\text{if } \\omega \\in B^{C}\n\t\\end{cases} \n= \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} + \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} 1_{B^{C}}$$\nAll we need to do is check our candidate for $\\mathbb{E}(X|1_{B})$ satisfies the definition of [[Conditional Expectation]]. That is, we must check that for all [[Set]]s in $\\sigma(1_{B}) = \\{\\emptyset, B, B^{C}, \\Omega\\}$ we satisfy the integral criterion of [[Conditional Expectation]]. To see this is indeed the case observe that\n1. $$\\int\\limits_{\\emptyset} \\Big( \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} + \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} 1_{B^{C}} \\Big) d \\mathbb{P}(\\omega) = 0 = \\int\\limits_{\\emptyset} X d \\mathbb{P}(\\omega) \\text{ }\\checkmark$$\n2. $$\\begin{align*}\n\\int\\limits_{B} \\Big( \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} + \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} 1_{B^{C}} \\Big) d \\mathbb{P}(\\omega) \u0026= \\int\\limits \\Big( \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} 1_{B} + \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} 1_{B^{C}}  1_{B} \\Big) d \\mathbb{P}(\\omega)\\\\\\\\\n\u0026= \\int\\limits \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} d \\mathbb{P}(\\omega)\\\\\n\u0026= \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} \\mathbb{P}(B)\\\\\n\u0026=\\mathbb{E}(1_{B}X)\\\\\n\u0026=\\int\\limits_{B} X d \\mathbb{P}(\\omega) \\checkmark\n\\end{align*}$$\n3. $B^{C}$ works almost exactly the same as $B$.\n4. $$\\begin{align*}\n\\int\\limits_{\\Omega} \\Big( \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} 1_{B} + \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} 1_{B^{C}} \\Big) d \\mathbb{P}(\\omega) \u0026= \\int\\limits_{B} \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)} d \\mathbb{P}(\\omega) + \\int\\limits_{B^{C}} \\frac{\\mathbb{E}(1_{B^{C}}X)}{\\mathbb{P}(B^{C})} d \\mathbb{P}(\\omega) \\\\\n\u0026=\\int\\limits_{B} X d \\mathbb{P}(\\omega) + \\int\\limits_{B^{C}} X d \\mathbb{P}(\\omega) \u0026\\text{by (2) and (3)}\\\\\n\u0026= \\int\\limits_{\\Omega} X d \\mathbb{P}(\\omega) \\checkmark\n\\end{align*}$$\n\nTherefore our candidate for $\\mathbb{E}(X|1_{B})$ is indeed the [[Conditional Expectation]] and we have (with $\\omega \\in B$)\n$$\\mathbb{E}(X|B) = \\mathbb{E}(X|1_{B})(\\omega) = \\frac{\\mathbb{E}(1_{B}X)}{\\mathbb{P}(B)}$$\n$\\blacksquare$\n\n# Other Outlinks\n* [[Sigma Algebra induced by an Indicator Function]]\n* [[Integration of Simple Functions]]\n* [[Integration is Linear]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditional-Probability":{"title":"Untitled Page","content":"# Definition 1\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $A \\in \\mathcal{B}$ be an [[Event]]. Let $\\mathcal{G} \\subset \\mathcal{B}$ be a sub-[[Sigma Algebra]]. Then the [[Conditional Probability]] of $A$ given $\\mathcal{G}$ is\n$$\\mathbb{P}(A|\\mathcal{G}) = \\mathbb{E}(1_{A}|\\mathcal{G})$$\n\nSimilarly to [[Conditional Expectation]], if $Y$ is a [[Random Variable]] on $\\Omega$ then\n$$\\mathbb{P}(A|Y) = \\mathbb{P}(A|\\sigma(Y))$$\nand if $B \\in \\mathcal{B}$ then\n$$\\mathbb{P}(A|B) = \\mathbb{P}(A|1_{B})(\\omega)$$\nfor (any) $\\omega \\in B$.\n\n# Definition 2\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $A, B \\in \\mathcal{B}$ be [[Event]]s such that $\\mathbb{P}(B) \\neq 0$. Then the [[Conditional Probability]] of $A$ given $B$ is\n$$\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$$\n\n\n# Remarks\n1. Since [[Indicator Functions on Measureable Sets are in L+]] we know $1_{A} \\in L^{+}(\\mathcal{B}) \\subset \\bar{L^{1}}(\\mathcal{B})$, and using [[Conditional Expectation]] makes sense in our definition.\n2. We can also write the two auxiliary definitions in Definition 1 as $$\\mathbb{P}(A|Y) = \\mathbb{E}(1_{A}|Y)$$ and $$\\mathbb{P}(A|B) = \\mathbb{E}(1_{A}|B)$$\n3. We should check that Definition 1 naturally leads to Definition 2:\n\n\t**Proof**: Suppose $A, B \\in \\mathcal{B}$ so that $\\mathbb{P}(B) \\neq 0$. Then $$\\mathbb{P}(A|B) = \\mathbb{E}(1_{A}|B) = \\frac{\\mathbb{E}(1_{B}1_{A})}{\\mathbb{P}(B)} = \\frac{\\mathbb{E}(1_{B \\cap A})}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B \\cap A)}{\\mathbb{P}(B)}$$where the second equality follows because [[Conditional Expectation with respect to a Set is Integral divided by Probability]]. $\\blacksquare$\n\n\tThus, we can use whichever definition is more convenient.\n4. Suppose $B \\in \\mathcal{B}$ so that $\\mathbb{P}(B) \\neq 0$. Then $(\\Omega, \\mathcal{B}, \\mathbb{P}(\\cdot | B))$ is a [[Probability Space]] on $\\Omega$.\n\t\n\t**Proof**: All we need do is check $\\mathbb{P}(\\cdot|B)$ is a valid [[Probability Measure]]. We will rely on the fact that $\\mathbb{P}$ is a probability measure\n\t1. If $A \\in \\mathcal{B}$, $\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\in [0,1]$ since $A \\cap B \\subset B$ so $0 \\leq \\mathbb{P}( A \\cap B) \\leq \\mathbb{P}(B)$. $\\checkmark$\n\t2. $\\mathbb{P}(\\Omega|B) = \\frac{\\mathbb{P}(\\Omega \\cap B)}{\\mathbb{P}(B)} =  \\frac{\\mathbb{P}(B)}{\\mathbb{P}(B)} = 1$. $\\checkmark$\n\t3. Suppose $(A_{n})_{n=1}^{\\infty} \\subset \\mathcal{B}$ are [[Mutually Disjoint]] [[Event]]s. Then $$\\mathbb{P}(\\bigsqcup\\limits_{n \\in \\mathbb{N}}A_{n}|B) = \\frac{\\mathbb{P}\\Big( \\big(\\bigsqcup\\limits_{n \\in \\mathbb{N}}A_{n} \\big) \\cap B \\Big)}{\\mathbb{P}(B)} =  \\frac{\\mathbb{P}\\Big( \\bigsqcup\\limits_{n \\in \\mathbb{N}}(A_{n} \\cap B) \\Big)}{\\mathbb{P}(B)} = \\sum\\limits_{n=1}^{\\infty} \\frac{\\mathbb{P}(A_{n} \\cap B)}{\\mathbb{P}(B)} = \\sum\\limits_{n=1}^{\\infty} \\mathbb{P}(A|B)$$\n\t\t$\\checkmark$\n\t\n\tThus $\\mathbb{P}(\\cdot | B)$ is a [[Probability Measure]]. $\\blacksquare$\n1. I used to think about [[Conditional Expectation]] as just an [[Expectation]] taken with the [[Conditional Probability]]s. Here, however we've taken [[Conditional Expectation]] to be more fundamental. My old way of thinking about this is still valid, as I show in [[TODO]]\n\n# Other Outlinks\n* [[Extended L1 Functions]]\n* [[Null Set]]\n* [[Product of Indicator Functions is Intersection]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Conditioning-on-known-information-is-Idempotent":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{G}, \\mathbb{P})$ be a [[Probability Space]] and let $\\mathbb{B} \\subset \\mathcal{G}$ be a sub-[[Sigma Algebra]] of $\\mathbb{G}$. Suppose $X$ is a $\\mathcal{B}$-[[Measureable Function|measureable]] [[Random Variable]] on $\\Omega$. Then\n\n$$\\mathbb{E}[X|\\mathcal{G}] = X$$\n\n[[Almost Surely|almost surely]].\n## Proof\nBecause $X$ is $\\mathcal{B}$-[[Measureable Function|measureable]], we know that $\\forall S \\in \\mathcal{B}(\\mathbb{R})$ $X^{-1}(S) \\in \\mathcal{B}$. Since $\\mathcal{B} \\subset \\mathcal{G}$, we see $X^{-1}(S) \\in \\mathcal{G}$. Thus $X$ is $\\mathcal{G}$-measureable and it satisfies $\\forall A \\in \\mathcal{G}$:\n\n$$\\int\\limits_{A} X d \\mathbb{P}(\\omega) = \\int\\limits_{A} X d \\mathbb{P}(\\omega)$$\n\nso $X$ is a [[Conditional Expectation]] of $X$ with respect to $\\mathcal{G}$. The last statement follows becuase [[Conditional Expectation Exists and is Almost Surely Unique]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Cone":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and suppose $S \\subset V$. $S$ is a [[Cone]] if $\\forall \\mathbf{u} \\in S$ and $\\forall a \\geq 0$, $$a \\mathbf{u} \\in S$$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Connected-Component":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Connected-Graph":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Constant-Functions-are-Measureable":{"title":"Untitled Page","content":"# Statement\nLet $X$ be a [[Set]] and let $\\mathcal{M}$ be any [[Sigma Algebra]] on $X$. Likewise, let $Y$ be a [[Set]] and let $\\mathcal{N}$ be any [[Sigma Algebra]] on $Y$. Suppose $f: X \\to Y$ is such that $\\forall x \\in X$ $f(x) = c$ for some $c \\in Y$. Then $f$ is $(\\mathcal{M}, \\mathcal{N})$-[[Measureable Function|measureable]].\n\n## Proof\nSuppose $S \\in \\mathcal{N}$. If $c \\in S$ then every element of $X$ maps into  \n$S$ and $f^{-1}(S) = X$. If $c \\not\\in S$, then no element of $X$ maps into $S$ and $f^{-1}(S) = \\emptyset$. Since $\\{\\emptyset, X\\} \\subset \\mathcal{M}$ by definition of [[Sigma Algebra]], we see that $f$ is $(\\mathcal{M}, \\mathcal{N})$-[[Measureable Function|measureable]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Continuous":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\tau), (Y, \\rho)$ be [[Topological Space]]s and let $f: X \\to Y$ be a [[Function]]. Then $f$ is [[Continuous]] if $\\forall V \\in \\rho$\n$$f^{-1}(V) \\in \\tau$$\n\n# Other Outlinks\n* [[Function Preimage]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Continuous-Functions-Preserve-Limit-Points":{"title":"Untitled Page","content":"# Statement\nLet $(X, \\tau), (Y, \\rho)$ be [[Topological Space]]s and suppose $f: X \\to Y$ is [[Continuous]]. Suppose $S \\subset X$. If $x \\in X$ is a [[Limit Point]] of $S$, then $f(x)$ is a [[Limit Point]] of $f(S)$.\n\n## Proof\nLet $T = f(S)$ and $y = f(x)$. Suppose $V \\subset Y$ is [[Open]] and $y \\in V$. Then $f^{-1}(V)$ is [[Open]] ($f$ is [[Continuous]]) in $X$ and $x \\in f^{-1}(V)$ since $f(x) = y \\in V$. Since $x$ is a [[Limit Point]] of $S$, we know $f^{-1}(V) \\cap S \\neq \\emptyset$. Thus there exists $z \\in S$ so that $f(z) \\in V$. Since $z \\in S$, we also see $f(z) \\in f(S) = T$. Thus $V \\cap T \\supset \\{z\\}$. Since $V$ was arbitrary, we have that $y$ is a [[Limit Point]] of $T$. $\\blacksquare$\n\n# Other Outlinks\n* [[Function Image]]\n* [[Function Preimage]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Continuous-Functions-Preserve-Sequence-Limits":{"title":"Untitled Page","content":"# Statement\nLet $(X, \\tau), (Y, \\rho)$ be [[Topological Space]]s and suppose $f: X \\to Y$ is [[Continuous]]. Suppose $(x_n) \\subset X$. If $x_{n} \\to x \\in X$ , then $f(x_{n}) \\to f(x) \\in Y$.\n\n## Proof\nSuppose $V \\subset Y$ is [[Open]] and $f(x) \\in V$. Then $f^{-1}(V)$ is [[Open]] in $X$ and $x \\in f^{-1}(V)$. Thus, there exists $N \\in \\mathbb{N}$ so that $\\forall n \\geq N$ we have that $x_{n} \\in f^{-1}(V)$. Thus $\\forall n \\geq N$ we have  $f(x_{n}) \\in V$. Since $V$ was arbitrary, $f(x_{n}) \\to f(x)$. $\\blacksquare$\n\n# Other Outlinks\n* [[Sequence Convergence]]\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Continuous-Functions-are-Determined-on-Dense-Sets":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$, $(Y, \\rho)$  are [[Topological Space]]s, $X$ is a [[First Countable Space]], $Y$ is [[Hausdorff Space|Hausdorff]], and $f: X \\to Y$, $g: X \\to Y$ are [[Continuous|continuous functions]]. Suppose $D \\subset X$ is [[Dense]] in $X$. If $$g {\\big|}_{D} = f {\\big|}_{D}$$ we have that $g = f$.\n\n## Proof\nFirst observe that since $D$ is [[Dense]] its [[Closure]] is $X$. Since $X$ is [[First Countable Space|first countable]], [[Closure of a Set in a First Countable Space is all its Sequential Limits]], so for every $x \\in X$, there exists $(x_n) \\subset D$ so that $x_{n} \\to x$. Recall that [[Continuous Functions Preserve Sequence Limits]], so $f(x_{n}) \\to f(x)$ and $g(x_{n}) \\to g(x)$. [[Sequence Limits are unique in Hausdorff Spaces]] so, because $f(x_{n}) = g(x_{n})$, we must have that $f(x) = g(x)$. $x \\in X$ was arbitrary so $f = g$. $\\blacksquare$\n\n## Remarks\n1. This is not true in general. Suppose $X=\\{1, 2, 3\\}$ is equipped with the [[Indiscrete Topology]]. Suppose $f = \\text{id}_{X}$ and $g: X \\to X$ s.t. $$g(x) = \\begin{cases} 1 \u0026 \\text{if } x = 1\\\\ 3 \u0026 \\text{if } x = 2\\\\ 2 \u0026 \\text{if } x = 3\\\\ \\end{cases}$$ Then $f$ is [[Continuous]] since [[Identity Functions are Continuous]]. $g$ is continuous since [[All Surjective Functions to the Indiscrete Topology are Continous]]. [[All Nonempty Subsets of the Indiscrete Topology are Dense]], so $\\{1\\}$ is [[Dense]]. We see $g(1) = f(1)$, but $g(2) = 3 \\neq 2 = f(2)$. $\\blacksquare$\n2. Since [[Metric Spaces are First Countable and Hausdorff]], [[Continuous|continuous functions]] between [[Metric Space]]s are determined on a [[Dense|dense set]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convergence-Almost-Surely-does-not-imply-convergence-of-moments":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{M}, \\mathbb{P})$ be a [[Probability Space]]. There exists a sequence of [[Random Variable]]s $(X_n)_{n=1}^{\\infty}$ on $\\Omega$ s.t. $X_{n} \\to X$ but $\\mathbb{E}|X_{n}| \\not\\to \\mathbb{E}|X|$.\n\n## Proof\nWe construct the example. Let $([0,1], \\mathcal{B}([0,1]), \\lambda)$ be our probability space. Suppose $X_{n}$ is defined as\n$$X_{n} = 0*1_{x \\leq 1 - \\frac{1}{n}} + 7n1_{x \u003e 1 - \\frac{1}{n}}$$\n\nThen $X_{n} \\to 0$ [[Almost Sure Convergence|almost surely]]. To see this, observe that for $s \\in [0, 1)$, there exists $N \\in \\mathbb{N}$ s.t. $1 - \\frac{1}{N} \u003e s$ (by the [[Archimedean]] property of the [[Natural Numbers]]). For all $n \\geq N$, we have $\\frac{1}{n} \\leq \\frac{1}{N}$ so $$X_{n}(s) = 0*1_{x \\leq 1 - \\frac{1}{n}}(s) + 7n1_{x \u003e 1 - \\frac{1}{n}}(s) = 0$$\n\nThus $\\mathbb{P}[X_{n} \\to 0] = \\lambda([0, 1)) = 1 \\checkmark$ .\n\nOn the other hand, observe that\n\n$$\\mathbb{E}|X_{n}| = 0*\\lambda([0, 1 - \\frac{1}{n}]) + 7n \\lambda\\left(\\left(1-\\frac{1}{n}, 1\\right)\\right) = 0 + \\frac{7n}{n} = 7$$\n\nWhile\n\n$$\\mathbb{E}|X| = 0$$\n\nSince $7 \\not\\to 0$ we have that the [[Moment]]s do not converge. $\\blacksquare$\n\n## Remarks\n1. In this example, all random variables are non-negative, so the construction also shows that almost sure convergence does not imply expectation converges.\n2. Any form of convergence implied by almost sure ([[Convergence in Distribution]], [[Convergence in Probability]]) will also have this counterexample.\n3. This is an example where [[Fatou's Lemma]] is strict. \n4. This example also works as a general example for generic [[Measure Space]]s.\n\n## Source\n1. Adapted from [Does Convergence in Distribution Imply Convergence of Expectation](https://math.stackexchange.com/questions/153293/does-convergence-in-distribution-implies-convergence-of-expectation)\n\n# Other Outlinks\n* [[Lebesgue Measure]]\n* [[Real Numbers]]\n* [[Closed Interval]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convergence-of-a-Bounded-Sequence-is-determined-by-Convergent-Subsequences":{"title":"Untitled Page","content":"# Statement\nLet $(a_{n}) \\subset \\mathbb{R}$ be a [[Bounded Sequence]]. Then $a_{n} \\to a \\in \\mathbb{R}$  [[If and Only If|iff]] every [[Sequence Convergence|convergent]] [[Subsequence]] of $(a_{n})$ converges to $a$.\n\n# Proof\n($\\Rightarrow$) Suppose $a_{n} \\to a$. Let $(a_{n_{k}})$ be a convergent subsequence of $(a_{n})$. Because [[Every Subsequence of a Convergent Sequence converges to the same Limit]], we know $a_{n_{k}} \\to a$.\n\n($\\Leftarrow$) Suppose every convergent subsequence of $(a_{n})$ converges to $a \\in \\mathbb{R}$. First we show that such a subsequence exists. \n\nLet $M \\in \\mathbb{R}$ be s.t. $|a_{n}| \\leq M$ $\\forall n \\in \\mathbb{N}$. By the [[Bolzano-Weierstrass Theorem]], we know there exists some convergent subsequence.\n\nNext we show that $(a_{n}) \\to a$. Suppose not. Then there exists an $\\epsilon \u003e 0$ s.t. $\\forall N \\in \\mathbb{N}$, $n \\geq N$ s.t. $|a_{n} - a| \\geq \\epsilon$. Let $n_{1}$ be such an element for $N = 1$. Next let $n_{2}$ be such an element for $N = 2$. Continue in this way to get a subsequence $(a_{n_{k}})$ s.t.\n\n$$|a_{n_{k}} - a| \\geq \\epsilon \\text{ for all }k \\in \\mathbb{N}$$\nBecause $\\{a_{n_{k}}\\} \\subset \\{a_{n}\\}$, it has the same bound of $M$. Thus by the [[Bolzano-Weierstrass Theorem]], $(a_{n_{k}})$ has a convergent subsequence. This subsequence is also a subsequence of $(a_{n})$ so it must also converge to $a$ by our given. If we denote this sub-subsequence $(a_{n_{k_{l}}})$, we have that there exists an $L \\in \\mathbb{N}$ s.t.\n\n$$|a_{n_{k_{l}}} - a| \u003c \\epsilon \\text{ for all }l \\in \\mathbb{N}$$\nBut this [[Proof by Contradiction|contradicts]] our construction of $(a_{n_{k}})$. Therefore $(a_{n}) \\to a$. $\\blacksquare$\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Natural Numbers]]\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Combination":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $x_{1}, \\dots, x_{n} \\in V$ for some $n \\in \\mathbb{N}$. Suppose $\\lambda_{1}, \\dots, \\lambda_{n} \\in [0,1]$ and $\\sum\\limits_{i=1}^{n} \\lambda_{i} = 1$. Then $$\\lambda_{1} x_{1} + \\cdots + \\lambda_{n}x_{n}$$ is a [[Convex Combination]] of $x_{1}, \\dots, x_{n}$.\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Cone":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and let $S \\subset V$. Then $S$ is a [[Convex Cone]] if $\\forall a, b \\geq 0$ we have that $$a \\mathbf{u} + b \\mathbf{v} \\in S$$ $\\forall \\mathbf{u}, \\mathbf{v} \\in S$.\n\n## Properties\n1. [[A Convex Cone is a Convex Set]] - this justifies the name.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Function":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$, let $S \\subset V$ be a [[Convex Set]], and suppose $f: S \\to \\mathbb{R}$. Then $f$ is a [[Strictly Convex Function]] if for all $\\lambda \\in [0, 1]$ and all $x,y \\in S$\n$$f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1- \\lambda) f(y)$$\n## Remarks\n1. This means that the value of $f$ is below any line segment connecting wrapping points.\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Function]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Hull":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. Then the [[Convex Hull]] of $S$, denoted $\\mathbf{conv} S$, is defined as $$\\mathbf{conv} S := \\bigcap\\limits \\{T \\subset V : S \\subset T, T \\text{ is convex}\\}.$$ \n## Remarks\n1. $\\mathbf{conv} S$ is the smallest [[Convex Set]] containing $S$ (since [[Intersection of Convex Sets is Convex]]).\n2. [[The Set of all Convex Combinations is the Convex Hull]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Polytope":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$. $P \\subset V$ is a [[Convex Polytope]] if there exist [[Closed Halfspace]]s $H_{1}, \\dots, H_{n}$ for $n \\in \\mathbb{N}$ so that $$P = \\bigcap\\limits_{i = 1}^{n} H_{i}$$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Polytope-Face":{"title":"Untitled Page","content":"# Definition 1\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$ and let $P \\subset V$ be a [[Convex Polytope]]. Then a [[Set]] $F \\subset P$ is a [[Convex Polytope Face|face]] of $P$ if any of the following is true:\n1. $F = P$\n2. $F = \\emptyset$\n3. there exists a [[Supporting Hyperplane]] $H \\subset V$ so that $F = P \\cap H$.\n\n## Properties\n1. [[Characterization of Convex Polytope Faces]]\n\n# Definition 2\n[[TODO]] - definition in terms of [[Valid Inequality]]s.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Set":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $S \\subset V$. $S$ is a [[Convex Set]] if for all $\\lambda \\in [0,1]$ and all $u, v \\in S$\n$$\\lambda u + (1 - \\lambda) v \\in S$$\n## Remarks\n1. $S$ is a [[Convex Set]] [[If and Only If]] $S$ contains all [[Closed Line Segment]]s between its points. This is just a restatement of the definition of a [[Convex Set]].\n2. [[A Set is Convex iff it contains all of its Convex Combinations]]\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Convex-Sets-contain-their-Expectation":{"title":"Untitled Page","content":"[[TODO]] - This will pull together [[Vector Space]], [[Probability Space]], and [[Expectation]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Correspondence-of-Lebesgue-Stieltjes-Measures-and-Distribution-Functions":{"title":"Untitled Page","content":"# Statement\nIf $F: \\mathbb{R} \\to \\mathbb{R}$ is a [[Distribution Function]], then there is a unique [[Lebesgue-Stieltjes Measure]] $\\mu_{F}$ on $\\mathbb{R}$ such that $\\mu_{F}((a,b]) = F(b) - F(a)$ for all $b \u003e a \\in \\mathbb{R}$. If $G$ is another such [[Function]], then $F - G$ is a constant.\n\n[[Converse|Conversely]], if $\\mu$ is a [[Lebesgue-Stieltjes Measure]] and we define\n\n$$F(x) = \n\\begin{cases} \n-\\mu((x, 0]) \u0026 x \u003c 0 \\\\\n0 \u0026 x = 0 \\\\\n\\mu((0, x]) \u0026 x \u003e 0 \\\\\n\\end{cases}\n$$\nthen $F$ is a [[Distribution Function]] and $\\mu_{F}=\\mu$.\n\n# Proof\n[[TODO]] \n\n# Proof 2 (Informal)\nWe can define the following [[Category]]s:\n1. Let $\\mu$ a be a [[Lebesgue-Stieltjes Measure]] on $\\mathbb{R}$. Then construct the [[Category]] with\n\t1. [[Object]]s: elements of $\\mathbb{R}$.\n\t2. [[Morphism]]s: Let $a, b \\in \\mathbb{R}$. The [[Morphism]] from $a$ to $b$ is\n\t\t1. If $a \u003c b$, then the morphism is $\\mu((a, b])$.\n\t\t2. If $a = b$, then the morphism is $\\mu(\\emptyset) = 0$.\n\t\t3. If $b \u003c a$, then the morphism is $-\\mu((b, a])$.\n\n\t  Then [[Composition]] of [[Morphism]]s [[Commuting Transforms|commutes]] because [[Measure]]s are $\\sigma$-additive. That is if we have $a, b, c \\in \\mathbb{R}$, then composing their morphisms by addition gets us the morphism from $a$ to $c$. Because $\\mu$ is a [[Lebesgue-Stieltjes Measure]], we have that each morphism is finite.\n2. Let $F$ be a [[Distribution Function]]. Then construct the [[Category]] with\n\t1. [[Object]]s: elements of $\\mathbb{R}$.\n\t2. [[Morphism]]s: Let $a, b \\in \\mathbb{R}$. The [[Morphism]] from $a$ to $b$ is $F(b) - F(a)$. This obviously composes as addition. \n\nWe can [[Natural Transformation|naturally transform]] from one to the other. The [[Non-Decreasing]] monotonicity of $F$ ensures that $\\mu$ is non-negative on all sets. [[Caratheodory's Theorem]] allows us to take the [[Category]] of $\\mu$ to a unique [[Complete Measure]] (since $\\mu$ will be [[Sigma-Finite]]).\n\n# Remarks\n1. Observe that $G$ must be a [[Distribution Function]] since the [[Set]] of [[Distribution Function]]s is closed under addition of constants.\n\n# Encounters\n1. [[Folland - Real Analysis]] - pg 35\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Sigma Additive]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Correspondence-of-Surjective-Functions-Partitions-and-Equivalence-Relations":{"title":"Untitled Page","content":"# Statement\nLet $X$ be a [[Set]]. Then the following [[Class]]es can be naturally transformed from one to the other:\n1. [[Partition]]s of $X$.\n2. [[Equivalence Relation]]s on $X$\n3. Let $\\mathcal{G}$ be the [[Class]] of all [[Surjection]]s from $X$ to some [[Set]] $Y$. Idenitfy [[Surjection]]s if there exists a [[Bijection]] between their [[Function Image]]s of $X$ and call this class $\\mathcal{G}'$. Then $\\mathcal{G}'$ is in one-to-one correspondence with the (1) and (2).\n\nFurthermore, composing any of these transformations recovers the direct one. That is, these transformations [[Commuting Transforms|Commute]].\n# Proof\n[[TODO]] ~~redo this to apply the transformation from 1 class to the other and back (or in a loop). Show the transformation is well-defined (easy) and that two different input classes do not map to same output class.~~  I think I will wait on this until I learn more [[Category Theory]]. Basically (and it's not hard to see), there is a natural way to get from any of these to the other. We can create the following [[Category|categories]]:\n1. [[Partition]]: [[Object]]s are elements of $X$ and [[Morphism]]s are [[Isomorphism]]s between elements in the same [[Partition]].\n2. [[Equivalence Relation]]: [[Object]]s are elements of $X$ and [[Morphism]]s are [[Isomorphism]]s between elements that are equivalent.\n3. [[Surjection]] from $X$: [[Object]]s are elements of $X$ and [[Morphism]]s are [[Isomorphism]]s between elements that map to same element in the [[Codomain]].\n\nIt's not hard to see that these three [[Category]]s have the same underlying structure. For example, if we change a given [[Category]] from one of a [[Partition]] to one of an [[Equivalence Relation]] then the [[Object]]s and [[Morphism]]s do not change. $\\square$\n\n((1) $\\Rightarrow$ (3)) Let $\\mathcal{S}$ be a [[Partition]] of $X$. Let $g: X \\to \\mathcal{S}$ be defined so that $g(x) = A$ s.t. $x \\in A$. This [[Function]] is [[Well-Defined]] since $\\exists ! A \\in \\mathcal{S}$ s.t. $x \\in A$ by definition of a [[Partition]]. This function is a [[Surjection]] because for every $A \\in \\mathcal{S}$ $\\exists x \\in A$ so $g(x) = A$. \n\n((3) $\\Rightarrow$ (1)) Let $g: X \\to Y$ be a [[Surjection]]. Let $\\mathcal{S} = \\{g^{-1}(\\{y\\}) : y \\in Y\\}$. We claim $\\mathcal{S}$ is a [[Partition]] of $X$. We check the 3 criteria:\n1. Since $g$ is a [[Surjection]], we know that every $g^{-1}(\\{y\\}) \\neq \\emptyset$.\n2. By properties of [[Function Preimage]]s, we know if $A \\subset Y$ and $B \\subset Y$ are [[Disjoint Sets]] then $g^{-1}(A) \\cap g^{-1}(B) = \\emptyset$. So for $y, z \\in Y$, either\n\t1. $y \\neq z$: then $\\{y\\} \\cap \\{z\\} = \\emptyset$ and $g^{-1}(\\{y\\}) \\cap g^{-1}(\\{z\\}) = \\emptyset$.\n\t2. $y = z$: then $g^{-1}(\\{y\\}) = g^{-1}(\\{z\\})$\n3. $X = g^{-1}(Y) = g^{-1}(\\bigcup\\limits_{y \\in Y} \\{y\\}) = \\bigcup\\limits_{y \\in Y} g^{-1}(\\{y\\})$\n\nWe show that each object from one [[Class]] corresponds to a unique object in the other class.\n\n((1) $\\Rightarrow$ (2)) Let $\\mathcal{S}$ be a [[Partition]] of $X$. Define $\\sim$ so that for $x, y \\in X$, $x \\sim y$ [[If and Only If]] $\\exists A \\in \\mathcal{S}$ s.t. $x,y \\in A$. We claim $\\sim$ is an [[Equivalence Relation]]. We check the 3 criteria:\n1. **Reflexitivity**: Let $x \\in X$. Since $\\mathcal{S}$ is a [[Partition]], we know there exists $A \\in \\mathcal{S}$ so that $x \\in A$. Then, since $x,x \\in A$ we have that $x \\sim x$ and $\\sim$ is reflexive.\n2. **Symmetry**: Let $x, y \\in X$ and suppose $x \\sim y$. Then we know there exists $A \\in \\mathcal{S}$ so that $x, y \\in A$. Then $y, x \\in A$ and $y \\sim x$.\n3. **Transitivity**: Let $x,y,z \\in X$ and suppose $x \\sim y$ and $y \\sim z$. Then we know there exists $A \\in \\mathcal{S}$ so that $x,y \\in A$ and we know there exists $B \\in \\mathcal{S}$ so that $y, z \\in B$. But since $\\mathcal{S}$ is a [[Partition]], if $A \\neq B$, we must have that $A \\cap B = \\emptyset$. Since $A \\cap B \\supset \\{y\\}$ we must have that $A = B$. Thus, $x, z \\in A$ and $x \\sim z$.\n\nSuppose another such [[Equivalence Relation]] relation existed and call it $\\sim'$. Then we have that $x \\sim y$ [[If and Only If]] there exists $A \\in \\mathcal{S}$ so that $x, y \\in A$ [[If and Only If]] $x \\sim' y$. Therefore $\\sim = \\sim'$.\n\n((2) $\\Rightarrow$ (1)) Let $\\sim$ be an [[Equivalence Relation]] on $X$. Define $\\forall x \\in X$, denote $A_{x} = \\{y \\in X : x \\sim y\\}$. Then let $\\mathcal{S} = \\{A_{x} \\subset \\mathcal{P}(X) : x \\in X\\}$. We claim that $\\mathcal{S}$ is a [[Partition]] of $X$. We check the 3 criteria:\n1. Let $A \\in \\mathcal{S}$. By construction of $\\mathcal{S}$, we know there exists $x \\in X$ so that $A = A_{x}$. Since $\\sim$ is an [[Equivalence Relation]], we know $x \\sim x$. Thus $x \\in A_{x} = A$ and $A \\neq \\emptyset$. Thus, $\\emptyset \\not\\in \\mathcal{S}$.\n2. Suppose $A, B \\in \\mathcal{S}$. Suppose $A \\cap B \\neq \\emptyset$ . Then there exists $y \\in A, B$. We know by construction of $\\mathcal{S}$ that there exists $x, z \\in X$ so that $A = A_{x}$ and $B = A_{z}$. Now let $a \\in A$. We get the following chain of equivalences (after adjusting appropriately using symmetry of $\\sim$):\n\t$$a \\sim x \\sim y \\sim z$$\n\tTransitivity and symmetry gives us that $z \\sim a$ so $a \\in B$. A similar argument shows that if $b \\in B$ then $b \\in A$ as well. Thus $B = A$.\n3. Consider $\\bigcup\\limits_{A_{x} \\in \\mathcal{S}} A_{x}$. Since $\\forall x \\in X$ there exists $A_{x} \\in \\mathcal{S}$ so that $x \\in A_{x}$, we have that \n\t$$\\bigcup\\limits_{A_{x} \\in \\mathcal{S}} A_{x} \\supset X$$\n\tOn the other hand, since $\\mathcal{S} \\subset \\mathcal{P}(X)$, we have the opposite subset relation showing us\n\t$$\\bigcup\\limits_{A_{x} \\in \\mathcal{S}} A_{x} = X$$\nNow suppose $\\mathcal{S}'$ \n\n((1) $\\Rightarrow$ (3)) Let $\\mathcal{S}$ be a [[Partition]] of $X$. Construct a part","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Counterexample-showing-that-optimizing-BLER-is-not-the-same-as-optimization-CE":{"title":"Untitled Page","content":"Dates: [[2022-03-10]]\n\nConsider the following setup:\n\n1. [[Probability Space]] $(\\Omega, \\mathcal{F}, \\mathbb{P})$ \n2. [[Random Variable]] $U: \\Omega \\to \\{1,2\\}$. with uniform probability. That is $\\mathbb{P}(U=1) = \\mathbb{P}(U=2) = \\frac{1}{2}$. \n3. Measureable Functions \n\t1. Encoder $f: \\{1, 2\\} \\to [n]$\n\t2. Noise Function $q: [n] \\times \\Omega \\to [n]$ \n\t3. Likelihood Function $l: [n] \\to [0,1]^{2}$. That is $l(i)_{j} = \\mathbb{P}(U=j|Y=i)$.\n\t4. Generic soft decoder $g: [n] \\to [0,1]^{2}$. $g$ returns a [[Probability Mass Function]] on $[2]$.\n\t5. Hard thresholder $h: [0,1] \\to [2]$, where $h(x) = \\begin{cases} 1 \u0026\\text{if }x \\geq 0.5\\\\ 0 \u0026\\text{otherwise} \\end{cases}$\n4. Denote [[Random Variable]]s\n\t1. $X = f \\circ U$. This is the encoded value.\n\t2. $Y(\\omega) = q(X(\\omega), \\omega)$. This is the noisy value\n\t3. $L = l \\circ Y$. This a random variable of the likelihood for $U=1$ conditioned on $Y$.\n\t4. $R = g \\circ Y$. This is a random variable of the soft decoded value conditioned on $Y$.\n\t6.  $\\hat{U} = h \\circ L$. This is a random variable representing ML decoded value.\n\t7. $\\hat{V} = h \\circ R$. This is a random variable representing the decoded value determined by hard thresholding of the soft decoded value.\n\n\n\u003e **Definition 1**: The [[Block Error Rate]] of an encoder $f$, soft decoder $g$ given some noise model $q$ is \n\u003e $$BLER(f, g|q) = \\mathbb{E}\\big[ 1_{\\hat{V} \\neq U} \\big]$$\n\nNote that this is really just the [[Risk]] for the [[Classification Problem]] of classifying $U$ using $Y$. So it follows that the [[Bayes Classifier is Risk Minimizer|Bayes Classifier minimizes BLER]].\n\n\u003e **Definition 2**: The [[Cross-Entropy]] of an encoder $f$, soft decoder $g$ given some noise model $q$ is\n\u003e $$CE(f, g|q) = \\mathbb{E}\\Big[-\\sum\\limits_{j \\in [2]} 1_{U=j} \\log g(Y)_{j} \\Big]$$\n\nRecall that we can write the [[Cross-Entropy]] as\n$$CE(f, g|q) = \\mathbb{H}(U|Y) + \\mathbb{E}\\big[ D_{KL}(\\mathbb{P}(U|Y) || g(Y) ) \\big]$$\n\nFrom here we see that $g=l$ minimizes $CE(f, g|q)$ for any choice of $f, q$. $l$ induces the [[Bayes Classifier]] using the hard thresholder $h$, so we can modify our definitions to focus on $f$:\n\n\u003e **Definition 1.1**: The [[Block Error Rate]] of an encoder $f$ given some noise model $q$ is \n\u003e $$BLER(f|q) = \\mathbb{E}\\big[ 1_{\\hat{U} \\neq U} \\big] = \\mathbb{P}(\\hat{U} \\neq U)$$\n\nand\n\u003e **Definition 2.1**: The [[Cross-Entropy]] of an encoder $f$ given some noise model $q$ is\n\u003e $$CE(f|q) = \\mathbb{E}\\Big[-\\sum\\limits_{j \\in [2]} 1_{U=j} \\log l(Y)_{j} \\Big ]= \\mathbb{H}(U|Y)$$\n\n\nWe can simplify our definition of [[Block Error Rate]] by observing that\n$$BLER(f|q) = \\mathbb{P}(\\hat{U} \\neq U) = 1 - \\frac{\\mathbb{P}(\\hat{U} =1| U=1)}{2} - \\frac{\\mathbb{P}(\\hat{U} =0| U=0)}{2}$$\n\nWe can decompose these probabilities as\n\n$$\\begin{align*}\n\\mathbb{P}(\\hat{U}=1|U=1) \u0026= \\mathbb{P}(1_{\\mathbb{P}(U=1|Y) \\geq  \\mathbb{P}(U=2|Y)}=1|U=1)\\\\\n\u0026=\\sum\\limits_{y \\in [n]} \\mathbb{P}(1_{\\mathbb{P}(U=1|Y) \\geq  \\mathbb{P}(U=2|Y)}=1|Y=y)\\mathbb{P}(Y=y|U=1)\\\\\n\u0026=\\sum\\limits_{y \\in \\{i:\\mathbb{P}(U=1|Y=i) \\geq  \\mathbb{P}(U=2|Y=i)\\}} \\mathbb{P}(Y=y|U=1)\\\\\n\u0026=\\sum\\limits_{y \\in \\{i:\\mathbb{P}(U=1|Y=i) \\geq  \\mathbb{P}(U=2|Y=i)\\}} \\mathbb{P}(Y=y|X=f(1))\n\\end{align*}$$\n\nLet's rewrite $\\mathbb{P}(U=j|Y=y)$ using [[Bayes Theorem]]\n$$\\begin{align*}\n\\mathbb{P}(U=j|Y=y) \u0026= \\frac{\\mathbb{P}(Y=y|U=j)\\mathbb{P}(U=j)}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|U=k)\\mathbb{P}(U=k)}\\\\\n\u0026= \\frac{\\frac{\\mathbb{P}(Y=i|X=f(j))}{2}}{\\frac{1}{2}\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))}\\\\\n\u0026= \\frac{\\mathbb{P}(Y=i|X=f(j))}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))}\n\\end{align*}$$\n\nThus,\n$$\\begin{align*}\n\u0026\\mathbb{P}(U=1|Y=i) \\geq  \\mathbb{P}(U=2|Y=i)\\\\\n\u0026\\Rightarrow \\frac{\\mathbb{P}(Y=i|X=f(1))}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))} \\geq  \\frac{\\mathbb{P}(Y=i|X=f(2))}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))}\\\\\n\u0026\\Rightarrow \\mathbb{P}(Y=i|X=f(1)) \\geq \\mathbb{P}(Y=i|X=f(2))\n\\end{align*}$$\nSo\n$$\\begin{align*}\n\\mathbb{P}(\\hat{U}=1|U=1) \n\u0026=\\sum\\limits_{y \\in \\{i:\\mathbb{P}(Y=i|X=f(1)) \\geq \\mathbb{P}(Y=i|X=f(2))\\}} \\mathbb{P}(Y=y|X=f(1))\n\\end{align*}$$\nAnd likewise for $U=2$. Thus\n\n$$2BLER(f|q) = 2 - \\sum\\limits_{y \\in \\{i:\\mathbb{P}(Y=i|X=f(1)) \\geq \\mathbb{P}(Y=i|X=f(2))\\}} \\mathbb{P}(Y=y|X=f(1)) - \\sum\\limits_{y \\in \\{i:\\mathbb{P}(Y=i|X=f(1)) \u003c \\mathbb{P}(Y=i|X=f(2))\\}} \\mathbb{P}(Y=y|X=f(2)) \\checkmark$$\n\nNow, to simplify our [[Conditional Entropy]], we can use our [[Bayes Theorem]] computation from above\n$$\\begin{align*}\nCE(f|q) \u0026=  \\mathbb{H}(U|Y)\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{P}(Y=y) \\sum\\limits_{j \\in [2]} \\frac{\\mathbb{P}(Y=y|X=f(j))}{2\\mathbb{P}(Y=y)} \\log \\frac{\\mathbb{P}(Y=y|X=f(j))}{2\\mathbb{P}(Y=y)}\\\\\n\u0026=-\\frac{1}{2} \\sum\\limits_{y \\in [n]} \\sum\\limits_{j \\in [2]} \\mathbb{P}(Y=y|X=f(j)) \\log \\frac{\\mathbb{P}(Y=y|X=f(j))}{2\\mathbb{P}(Y=y)}\\\\\n\u0026=-\\frac{1}{2} \\sum\\limits_{y \\in [n]} \\sum\\limits_{j \\in [2]} \\mathbb{P}(Y=y|X=f(j)) \\log \\frac{\\mathbb{P}(Y=y|X=f(j))}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))} \\checkmark\\\\\n\\end{align*}$$\n\nWe want to construct $q$ so the 2 best possibilities are\n\n$$\\mathbb{P}(\\hat{U}=1|U=1) = .5, \\mathbb{P}(\\hat{U}=2|U=2) = .7$$\nand \n$$\\mathbb{P}(\\hat{U}=1|U=1) = .505, \\mathbb{P}(\\hat{U}=2|U=2) = .694$$\n\n*Note: This choice above does not use the right reasoning, since cross-entropy is not in terms of $\\mathbb{P}(\\hat{U}=j|U=j)$. However, I got lucky and my constructed noise model ended up working out.*\n\nOur noise model can be represented as a [[Markov Model]] on $n$ vertices. This can be represented as $n \\times n$ transition matrix (so each row must sum to 1). $f$ will select two vertices (corresponding to rows). We can denote this matrix $T = (p_{ij})$.\n\nLet's try this with $n=4$. Then\n\n$$T = \\begin{pmatrix}p_{11}\u0026p_{12}\u0026p_{13}\u0026p_{14} \\\\ p_{21}\u0026p_{22}\u0026p_{23}\u0026p_{24} \\\\ p_{31}\u0026p_{32}\u0026p_{33}\u0026p_{34} \\\\ p_{41}\u0026p_{42}\u0026p_{43}\u0026p_{44}\\end{pmatrix}$$\n\nThen our expressions for $BLER$ and $CE$ are\n\n$$\\begin{align*}\n2BLER(f|q) \u0026= 2 - \\sum\\limits_{y \\in \\{i:\\mathbb{P}(Y=i|X=f(1)) \\geq \\mathbb{P}(Y=i|X=f(2))\\}} \\mathbb{P}(Y=y|X=f(1)) - \\sum\\limits_{y \\in \\{i:\\mathbb{P}(Y=i|X=f(1)) \u003c \\mathbb{P}(Y=i|X=f(2))\\}} \\mathbb{P}(Y=y|X=f(2))\\\\\n\u0026=2 - \\sum\\limits_{y \\in \\{i:p_{f(1)i} \\geq p_{f(2)i}\\}} p_{f(1)y} - \\sum\\limits_{y \\in \\{i:p_{f(1)i} \u003c p_{f(2)i}\\}} p_{f(2)y}\\\\\n\u0026=2 - \\sum\\limits_{y \\in [4]} \\max(p_{f(1)y},p_{f(2)y})\n\\end{align*}$$\n\nand $$\\begin{align*}\n2CE(f|q) \u0026= -\\sum\\limits_{y \\in [n]} \\sum\\limits_{j \\in [2]} \\mathbb{P}(Y=y|X=f(j)) \\log \\frac{\\mathbb{P}(Y=y|X=f(j))}{\\sum\\limits_{k \\in [2]} \\mathbb{P}(Y=y|X=f(k))}\\\\\n\u0026= -\\sum\\limits_{y \\in [n]} \\sum\\limits_{j \\in [2]} p_{f(j)y} \\log \\frac{p_{f(j)y}}{\\sum\\limits_{k \\in [2]} p_{f(k)y}}\n\\end{align*}$$\n\nWe'll try the matrix\n\n$$T = \\begin{pmatrix}.24\u0026.26\u0026.2605\u0026.2395 \\\\ .15\u0026.15\u0026.35\u0026.35 \\\\.24\u0026.26\u0026.2605\u0026.2395 \\\\ .056\u0026.343\u0026.25\u0026.351 \\\\\\end{pmatrix}$$\n\nI ran over the 6 possibilities in [[Python]]:\n\n```\nf1 = [0, 1]\n2*CE(f) = 1.3438750586638888\n2*BLER(f) = 0.8\nf2 = [0, 2]\n2*CE(f) = 1.3862943611198906\n2*BLER(f) = 1.0\nf3 = [0, 3]\n2*CE(f) = 1.308267372143544\n2*BLER(f) = 0.8055000000000001\nf4 = [1, 2]\n2*CE(f) = 1.3438750586638888\n2*BLER(f) = 0.8\nf5 = [1, 3]\n2*CE(f) = 1.3168531071861709\n2*BLER(f) = 0.806\nf6 = [2, 3]\n2*CE(f) = 1.308267372143544\n2*BLER(f) = 0.8055000000000001\n```\n\nSo we see $f = (0, 1)$ is optimal for BLER but $f = (2, 3)$ is optimal for CE. They are not optimal for the other. Therefore, these optimization problems are not the same. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Cross-Entropy":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. Then the [[Cross-Entropy]] of a [[Soft Classifier]] $g$ is\n$$\\begin{align*}\nCE(g) \u0026= \\mathbb{E}\\Big[-\\sum\\limits_{y \\in [n]} 1_{Y=y} \\log g(X)_{y} \\Big]\n\\end{align*}$$\n\n## Remarks\n1. We can rewrite the [[Cross-Entropy]] in terms of [[Conditional Entropy]] and [[Kullback–Leibler Divergence]]: $$\\begin{align*}\nCE(g) \u0026= \\mathbb{E}\\Big[-\\sum\\limits_{y \\in [n]} 1_{Y=y} \\log g(X)_{y} \\Big]\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ 1_{Y=y} \\log g(X)_{y} \\Big] \u0026\\text{(Linearity)}\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ \\mathbb{E}\\big[ 1_{Y=y} \\log g(X)_{y} | X \\big] \\Big] \u0026\\text{(Smoothing)}\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ \\mathbb{E}\\big[ 1_{Y=y} | X \\big] \\log g(X)_{y} \\Big] \u0026(g(X) \\text{ is }Y-\\text{measureable})\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ \\mathbb{P}(Y=y|X) \\log g(X)_{y} \\Big]\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ \\mathbb{P}(Y=y|X) \\log \\big( \\frac{g(X)_{y}}{\\mathbb{P}(Y=y|X)} \\mathbb{P}(Y=y|X) \\big) \\Big]\\\\\n\u0026=-\\sum\\limits_{y \\in [n]} \\mathbb{E} \\Big[ \\mathbb{P}(Y=y|X) \\log \\mathbb{P}(Y=y|X) + \\mathbb{P}(Y=y|X) \\log \\big( \\frac{g(X)_{y}}{\\mathbb{P}(Y=y|X)} \\big) \\Big]\\\\\n\u0026=\\mathbb{E}\\Big[ -\\sum\\limits_{y \\in [n]} \\mathbb{P}(Y=y|X) \\log \\mathbb{P}(Y=y|X) \\Big] + \\mathbb{E}\\Big[ -\\sum\\limits_{y \\in [n]} \\mathbb{P}(Y=y|X) \\log \\big( \\frac{g(X)_{y}}{\\mathbb{P}(Y=y|X)} \\big) \\Big]\\\\\n\u0026=\\mathbb{H}(Y|X) + \\mathbb{E}\\Big[D_{KL}(\\mathbb{P}(Y|X) || g(X) ) \\Big]\n\\end{align*}$$\n\tThe only term here that depends on $g$ is the [[Kullback–Leibler Divergence]] term.\n\n# Other Outlinks\n* [[Smoothing]]\n* [[Linearity of Expectation]]\n* [[Conditional Probability]]\n* [[Conditional Expectation]]\n* [[Measureable Function]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Daily-Note-Template":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Dense":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\tau)$ be a [[Topological Space]]. Then $D \\subset X$ is **Dense** if the [[Closure]] of $D$ is $X$. \n\n\n## Properties\n1. [[The following are Equivalent]]\n\t- $D$ is [[Dense]]\n\t- $X$ is the only [[Closed|closed set]] containing $D$\n\t- The [[Interior]] of the [[Complement]] of $D$ is [[Empty Set|empty]]\n\t- $\\forall x \\in X$, either $x \\in D$ or $x$ is a [[Limit Point]] of $D$\n\t- $\\forall x \\in X$, every [[Neighborhood]] $U$ of $x$ is not [[Mutually Disjoint]] with $D$. That is $U \\cap D \\neq \\emptyset$.\n\t- $D \\cap U \\neq \\emptyset$ $\\forall U \\in \\tau$\n2. Suppose $\\mathcal{B} \\subset \\tau$ is a [[Topological Basis]] for $(X, \\tau)$. Then [[The following are Equivalent]]\n\t- $D$ is [[Dense]]\n\t- For every $x \\in X$, every [[Basic Neighborhood]] $B \\in \\mathcal{B}$ containing $x$ is not [[Mutually Disjoint]] with $D$. That is $B \\cap D \\neq \\emptyset$.\n\t- $D \\cap B \\neq \\emptyset$ $\\forall B \\in \\mathcal{B}$\n4. Suppose $X, \\tau$ is the [[Metric Topology]] generated by [[Metric Space]] $(X, d)$.\n\t- $D$ is [[Dense]]\n\t- $\\{x \\in X : x = \\lim\\limits_{n \\to \\infty} y_{n}, (y_{n}) \\subset D\\} = X$. This is just the [[Closure]] of $D$ in a [[Metric Space]].\n\n### Proofs\n[[TODO]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Directed-Graph":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Set]] and suppose $E \\subset V \\times V$. Then $(V, E)$ is a [[Directed Graph]] with [[Graph Vertex|vertices]] $v \\in V$ and [[Graph Edge|edges]] $e \\in E$.\n\n## Remarks\n1. 1. Sometimes we say $G$ is an [[Directed Graph]] and refer to its [[Graph Vertex|vertices]] and [[Graph Edge|edges]] as $V(G)$ and $E(G)$ respectively.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Distribution":{"title":"Untitled Page","content":"Suppose $X$ is a [[Random Variable]] on [[Probability Space]] $(\\Omega, \\mathcal{M}, \\mathbb{P})$ with [[Distribution Function]] $F_{X}$ and [[Induced Probability Measure]] $\\mu$. The [[Distribution]] is really just an overloaded shorthand for these two terms. That is, if $G$ is the [[Distribution]] then\n\n1. $G(x) = F_{X}(x)$ $\\forall x \\in \\mathbb{R}$\n2. $G(S) = \\mu(S)$ $\\forall S \\in \\mathcal{B}(\\mathbb{R})$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Distribution-Function":{"title":"Untitled Page","content":"# Definition\nLet $F: \\mathbb{R} \\to \\mathbb{R}$. Then $F$ is a distribution function if\n1. $F$ is [[Non-Decreasing]]\n2. $F$ is [[Right-Continuous]]\n\nDenote $F(\\infty)= \\lim_{x \\to \\infty}F(x)$ and $F(-\\infty)= \\lim_{x \\to -\\infty}F(x)$. \n# Encounters\n* [[2022-02-24]] - [[Resnick - A Probability Path]] - pg247","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Dot-Product-of-Random-Vector-and-Independent-Entry-Random-Vector":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{M}, \\mathbb{P})$ be a [[Probability Space]]. Suppose $(X_n)$ is a [[Sequence]] of [[Random Variable]]s and $(Y_{n})$ is a [[Sequence]] of [[Independence|Independent]] [[Random Variable]]s that is also independent with $X_{n}$ for all $n \\in \\mathbb{N}$. Denote \n$$\\begin{align*}\n\u0026\\mathbb{E}(X_{n}) = \\mu_{n} \u0026 \\mathbb{E}(Y_{n}) = \\nu_{n}\\\\\n\u0026\\text{Var}(X_{n})\u0026a\\\\\n\u0026S_{n} = \\sum\\limits_{n=1}^{\\infty} X_{n} Y_{n}\\\\\n\\end{align*}$$\n\n[[TODO]] \n\n.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Empirically-Minimizing-Conditional-Entropy":{"title":"Untitled Page","content":"Dates: [[2022-04-21]]\n\nRecall from [[Minimizing Conditional Entropy]] the following setup:\n\n1. [[Probability Space]] $(\\Omega, \\mathcal{F}, \\mathbb{P})$ \n2. [[Random Vector]] $U: \\Omega \\to \\mathbb{F}_{2}^{n}$ for $n \\in \\mathbb{N}$. This represents the input sequence. For shorthand $U_{i}=\\pi_{i}\\circ U$, where $\\pi_i$ is the $i$th [[Projection Map]] for $i \\in [n]$.\n3. [[Measureable Function]]s \n\t1. Encoder $f: \\mathbb{F}^{n}_{2} \\to M$ where $(M, d)$ is a [[Metric Space]].\n\t2. Noise Function $g: M \\times \\Omega \\to M$ \n\t3. Decoder $h: M \\to \\mathbb{F}^{n}_{2}$\n4. Denote\n\t1. $F = f \\circ U$\n\t2. $G(\\omega) = g(F(\\omega), \\omega)$\n\t3.  $H = h \\circ G$\n\nWe are interested in ways to empirically minimize the quantity\n\n$$R_{g}(f) = \\frac{1}{n}\\sum\\limits_{i =1}^{n}  \\mathbb{H}(U_{i} | G)$$\n\nWhere the [[Conditional Entropy]] $\\mathbb{H}(U_{i} | G)$ is defined as\n\n$$\\begin{align*}\n\u0026\\mathbb{E}[\\mathbb{H}_{2}(\\mathbb{P}(U_{i} | G))]\\\\\n\u0026=\\mathbb{E}[-\\mathbb{P}(U_{i}=1 | G) \\lg \\mathbb{P}(U_{i}=1 | G) - (1 - \\mathbb{P}(U_{i}=1 | G)) \\lg (1 - \\mathbb{P}(U_{i}=1 | G))]\n\\end{align*}$$\n\nWe first establish the following theorem\n\n**Theorem 1**: Minimizing $\\mathbb{H}(U_{i}|G)$ is equivalent to minimizing\n\n$$\\mathbb{H}(G|U_{i}) - \\mathbb{H}(G)$$\n\nwhere $\\mathbb{H}(G) = \\mathbb{E}()$\n\n**Proof**: $A \\subset \\mathbb{R}$ so that $\\mathbb{P}(U_{i} = 1, G \\in A) \u003e 0$. By [[Bayes Theorem]], we have that\n\n$$\\begin{align*}\n\\mathbb{P}(U_{i}=1 | G \\in A) \u0026= \\frac{\\mathbb{P}(U_{i}=1, G \\in A)}{\\mathbb{P}(G \\in A)}\\\\\n\u0026=\\frac{\\mathbb{P}(G \\in A | U_{i} = 1) \\mathbb{P}(U_{i}=1)}{\\mathbb{P}(G \\in A)}\n\\end{align*}$$\n\nThen ... #TODO I'll come back to this after my [[Interpreting Deep Codes]] meeting. For now I just need a basic algorithm with the assumption that $G$ is [[Additive White Gaussian Noise Channel]].\n\nLet $Y$ be the received noisy sequence corrupted by the [[Additive White Gaussian Noise Channel]] channel with [[Variance]] $\\sigma^2$. By [[Bayes Theorem]] we get that\n\n$$\\mathbb{P}(U_{i}=b|Y=y) = \\frac{f_{Y|U_{i}=b}(y) \\mathbb{P}(U_{i}=b)}{f_{Y}(y)}$$\n\nThen $$\\lg \\mathbb{P}(U_{i}=b|Y=y) = \\lg f_{Y|U_{i}=b}(y) + \\lg \\mathbb{P}(U_{i}=b) - \\lg f_{Y}(y)$$ and we get\n\n$$\\begin{align*}\n\u0026\\mathbb{E}[-\\mathbb{P}(U_{i}=1 | G) \\lg \\mathbb{P}(U_{i}=1 | G) - (1 - \\mathbb{P}(U_{i}=1 | G)) \\lg (1 - \\mathbb{P}(U_{i}=1 | G))]\\\\\n\u0026=\\int\\limits_\\mathbb{R} \\frac{f_{Y}(y)}{f_{Y}(y)} \\Big[-f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg f_{Y|U_{i}=1}(y) -f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg f_{Y|U_{i}=0}(y) \\\\\n\u0026- f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg \\mathbb{P}(U_{i}=1) - f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg \\mathbb{P}(U_{i}=0) \\\\\n\u0026+ f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg f_{Y}(y) + f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg f_{Y}(y) \\Big] dy\\\\\n\u0026=\\Big[\\int\\limits_\\mathbb{R} -f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg f_{Y|U_{i}=1}(y)dy + \\int\\limits_\\mathbb{R} -f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg f_{Y|U_{i}=0}(y)dy \\\\\n\u0026+ \\int\\limits_\\mathbb{R} - f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg \\mathbb{P}(U_{i}=1)dy + \\int\\limits_\\mathbb{R} - f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg \\mathbb{P}(U_{i}=0)dy \\\\\n\u0026- \\int\\limits_\\mathbb{R} - f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) \\lg f_{Y}(y)dy + \\int\\limits_\\mathbb{R} - f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\lg f_{Y}(y)dy \\Big]\\\\\n\u0026= \\Bigg[ \\mathbb{E}\\Big(\\int\\limits_\\mathbb{R} -f_{Y|U_{i}}(y) \\lg f_{Y|U_{i}}(y)dy\\Big) \\\\\n\u0026+ \\mathbb{E}\\Big( \\lg \\mathbb{P}(U_{i})\\int\\limits_\\mathbb{R} - f_{Y|U_{i}}(y)dy \\Big) \\\\\n\u0026- \\int\\limits_\\mathbb{R} \\Big[ - f_{Y|U_{i}=1}(y) \\mathbb{P}(U_{i}=1) - f_{Y|U_{i}=0}(y) \\mathbb{P}(U_{i}=0) \\Big]\\lg f_{Y}(y)dy \\Bigg]\\\\\n\u0026=\\Bigg[ \\mathbb{E}\\Big(\\mathbb{E}( - \\lg f_{Y|U_{i}}|U_{i})\\Big) \\\\\n\u0026 + \\mathbb{E}\\Big( \\lg \\mathbb{P}(U_{i})\\Big)\\\\\n\u0026-\\mathbb{E} \\Big( - \\lg f_{Y} \\Big) \\Bigg]\\\\\n\u0026=\\Bigg[ \\mathbb{H}(Y|U_{i}) + \\mathbb{H}(U_{i}) -\\mathbb{H}(Y)\\Bigg]\n\n\\end{align*}$$\n\n$\\mathbb{H}(U_{i})$ is a constant, so we really just want to minimize\n\n$$\\frac{1}{n} \\sum\\limits_{i=1}^{n} \\Big[ \\mathbb{H}(Y|U_{i}) \\Big] - \\mathbb{H}(Y)$$\n\nSo how do we estimate these two terms. For $\\mathbb{H}(Y)$, this is straightforward, we sample $m$ samples independently, call them $u^{(j)}$ for $j \\in [m]$, encode them, then noise them to get $y^{(j)}$ for $j \\in [m]$. These constitute $m$ independent samples of noise. Then we compute\n\n$$\\begin{align*}\n\u0026\\mathbb{H}(Y) = \\mathbb{E} \\Big( - \\lg f_{Y} \\Big)\\\\\n\u0026\\approx \\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg f_{Y}(y^{(j)})\\\\\n\u0026=\\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg \\Big[ \\mathbb{E}(f_{Y|X}(y^{(j)})) \\Big]\\\\\n\u0026\\approx \\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg \\Big[ \\frac{1}{m}\\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\u0026= \\frac{1}{m} \\sum\\limits_{j=1}^{m}\\lg m -\\lg \\Big[ \\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\u0026=\\lg m - \\frac{1}{m} \\sum\\limits_{j=1}^{m}\\lg \\Big[ \\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\\end{align*}$$\n\nFor the other term, for each $i \\in [n]$, split our samples into groups $\\{j : u^{(j)}_{i} = b\\}$ for $b \\in \\mathbb{F}_{2}$. From there, we compute a similar computation as above on each of these subsets, then average them together based on the proportion in the subset.\n\n**Algorithm**:\n1. Sample $\\{u^{(j)} : j \\in [m]\\}$ [[Independently and Identically Distributed]] from $\\text{Unif}\\{0, 1\\}$.\n2. Sample $\\{\\nu^{(j)} : j \\in [m]\\}$ [[Independently and Identically Distributed]] from $\\mathcal{N}(0, \\sigma^2)$.\n3. Compute $x^{(j)} = C(u^{(j)})$ for $j \\in [m]$.\n4. Compute $y^{(j)} = x^{(j)} + \\nu^{(j)}$ for $j \\in [m]$.\n5. Compute $f_{Y|X=x^{(k)}}(y^{(j)}) = \\mathcal{N}_{\\sigma^{2}}(y^{(j)} - x^{(j)})$ for $j, k \\in [m]^{2}$.\n6. Compute $\\hat{\\mathbb{H}} = \\hat{\\mathbb{H}}(\\{y^{(j)}: j \\in [m]\\}) = \\ln m - \\frac{1}{m} \\sum\\limits_{j=1}^{m}\\ln \\Big[ \\sum\\limits_{j=1}^{m}(f_{Y|X=x^{(j)}}(y^{(j)})) \\Big]$.\n8. For $i \\in [n]$:\n\t1. Let $B_{0} = \\{j : u^{(j)}_{i} = 0\\}$ and $B_{1} = \\{j : u^{(j)}_{i} = 1\\}$\n\t2. Compute $\\hat{\\mathbb{H}}(\\{y^{(j)}: j \\in B_{0}\\}) = \\ln |B_{0}| - \\frac{1}{|B_{0}|} \\sum\\limits_{j \\in B_{0}} \\ln \\Big[ \\sum\\limits_{k \\in B_{0}}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]$.\n\t3.  Compute $\\hat{\\mathbb{H}}(\\{y^{(j)}: j \\in B_{1}\\}) = \\ln |B_{1}| - \\frac{1}{|B_{1}|} \\sum\\limits_{j \\in B_{1}} \\ln \\Big[ \\sum\\limits_{k \\in B_{1}}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]$\n\t4. Compute $\\hat{\\mathbb{H}}_{i} = \\frac{|B_{0}|}{m} \\hat{\\mathbb{H}}(\\{y^{(j)}: j \\in B_{0}\\}) + \\frac{|B_{1}|}{m} \\hat{\\mathbb{H}}(\\{y^{(j)}: j \\in B_{1}\\}$.\n\t\t- Replace the fractions with $\\mathbb{P}(u_{i} = 1) = \\frac{1}{2}$?\n\t5. Compute $\\mathbb{H}(u_{i}) = \\ln 2$\n1. Return $\\frac{1}{n} \\sum\\limits_{i=1}^{n} \\Big[ \\hat{\\mathbb{H}}_{i} \\Big] - \\hat{\\mathbb{H}} + \\ln 2$\n\n### Further inspection of entropy estimate\n$$\\begin{align*}\n\u0026\\mathbb{H}(Y) = \\mathbb{E} \\Big( - \\lg f_{Y} \\Big)\\\\\n\u0026\\approx \\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg f_{Y}(y^{(j)})\\\\\n\u0026=\\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg \\Big[ \\mathbb{E}(f_{Y|X}(y^{(j)})) \\Big]\\\\\n\u0026\\approx \\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg \\Big[ \\frac{1}{m}\\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\u0026= \\frac{1}{m} \\sum\\limits_{j=1}^{m}\\lg m -\\lg \\Big[ \\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\u0026=\\lg m - \\frac{1}{m} \\sum\\limits_{j=1}^{m}\\lg \\Big[ \\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\Big]\\\\\n\\end{align*}$$\nThis will not be unbiased since in general\n\n$$\\begin{align*}\n\u0026\\mathbb{E} \\left[ \\frac{1}{m} \\sum\\limits_{j=1}^{m} -\\lg \\left[ \\frac{1}{m}\\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\right] \\right] \\\\\n\u0026=\\mathbb{E} \\left[-\\lg \\left( \\frac{1}{m}\\sum\\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \\right) \\right]\\\\\n\u0026\\neq \\mathbb{E}\\Bigg[ -\\lg \\Big[ \\mathbb{E}(f_{Y|X}(Y)|Y) \\Big] \\Bigg] \\\\\n\u0026= \\mathbb{E} \\Big( - \\lg f_{Y} \\Big) = \\mathbb{H}(Y)\n\\end{align*}$$\nIf we use the expression of the log of the [[Gaussian Density Function]]:\n$$\\begin{align*}\n\\ln f_{Y|X=x^{(k)}}(y^{(j)}) \u0026= \\ln \\left[ \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-\\frac{(y^{(j)}_{i} - x^{(k)}_{i})^{2}}{\\sigma^{2}} }\\right]\\\\\n\u0026=\\ln \\left[ \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{n}{2}}} \\prod_{i=1}^{n} e^{-\\frac{(y^{(j)}_{i} - x^{(k)}_{i})^{2}}{\\sigma^{2}} }\\right]\\\\\n\u0026=\\ln \\left[ \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{n}{2}}} e^{-\\frac{\\sum\\limits_{i=1}^{n} (y^{(j)}_{i} - x^{(k)}_{i})^{2}}{\\sigma^{2}} }\\right]\\\\\n\u0026=-\\frac{n}{2}\\ln (2 \\pi \\sigma^{2}) - \\frac{\\sum\\limits_{i=1}^{n} (y^{(j)}_{i} - x^{(k)}_{i})^{2}}{\\sigma^{2}}\\\\\n\u0026=-\\frac{n}{2}\\ln (2 \\pi \\sigma^{2}) - \\frac{1}{\\sigma^{2}}||y^{(j)} - x^{(k)}||^{2}\n\\end{align*}$$\nSo the log conditional density matrix can be computed by computing pairwise distances between $y$ and $x$. Furthermore observe that since\n$$||y -x ||^{2} = \\langle y - x, y- x \\rangle = ||y||^{2} - 2\\langle y, x \\rangle + ||x||^{2}$$\nand if we write the matrices $Y$ and $X$ both of dimension $m \\times n$, we could compute $Y X^{T}$ to get the inner products. Computing the norms of the rows of $Y$ and $X$ and applying the above formula with broadcasting gives us an efficient way to compute the above distances. Noting that $y = x + \\nu$, we see\n\n$$||y||^{2} = \\langle x' + \\nu, x' + \\nu \\rangle = ||x'||^{2} + 2 \\langle x', \\nu \\rangle + ||\\nu||^{2}$$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Epigraph":{"title":"Untitled Page","content":"# Definition\nSuppose $X$ is a [[Set]], $(Y, \\leq)$ is a [[Partial Ordering]], $f: X \\to Y$. The [[Epigraph]] of $f$ is the [[Set]]\n$$\\text{Epi}(f) = \\{(x, y) \\in X \\times Y : y \\geq f(x)\\}$$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Epsilon-Principle":{"title":"Untitled Page","content":"# Statement\nLet $a,b \\in \\mathbb{R}$. Suppose for all $\\epsilon \u003e 0$, we have that $a \\geq b - \\epsilon$. Then $a \\geq b$.\n\n# Proof\nSuppose not. Then $a \u003c b$ and there exists some $\\delta \u003e 0$ s.t. $a = b - \\delta$. But then $a \u003c b - \\frac{\\delta}{2}$ which [[Proof by Contradiction|contradicts]] our assumption that $a \\geq b - \\epsilon$ for all $\\epsilon \u003e 0$. $\\blacksquare$\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Equal-in-Distribution":{"title":"Untitled Page","content":"# Definition\nLet $X, Y$ be [[Random Variable]]s and let $F_{X}, F_{Y}: \\mathbb{R} \\to \\mathbb{R}$ be their respective [[Probability Distribution Function]]s. Then $X \\overset{\\text{d}}{=} Y$ ($X, Y$ are [[Equal in Distribution]]), if $F_{X}$ = $F_{Y}$.\n\n# Properties\n1. If two [[Random Variable]]s are [[Equal in Distribution]], then their induced [[Induced Probability Measure]]s on $\\mathbb{R}$ must be the same because of the [[Correspondence of Lebesgue-Stieltjes Measures and Distribution Functions]].\n2. Suppose $X \\overset{d}{=} Y$. Overload $F_{X} = F = F_{Y}$ as both the [[Distribution Function]]s and induced [[Induced Probability Measure]]s for $X, Y$ respectively. Then for some [[Borel Measureable Function]] $h: \\mathbb{R} \\to \\mathbb{R}$, we have that\n\n\t$$\\int\\limits_{A} h(x)F_{X}(dx) = \\int\\limits_{A} h(x)F(dx) = \\int\\limits_{A} h(y)F_{Y}(dy)$$\n\tIn particular $\\forall k \\in \\mathbb{N}$\n\t$$\\mathbb{E}|X|^{k} = \\mathbb{E}|Y|^{k}$$\n\tThat is, $X$ has the same [[Moment]]s as $Y$.\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Equivalence-Class":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]] and let $\\sim$ be an [[Equivalence Relation]] on $X$. If $\\mathcal{S}$ is the [[Partition]] of $X$ corresponding to $\\sim$ (as per [[Correspondence of Surjective Functions, Partitions, and Equivalence Relations]]), then we call the elements $\\mathcal{S}$ the [[Equivalence Class]]es of $\\sim$.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Equivalence-Relation":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]] and let $R \\subset X \\times X$ be a [[Relation]] on $X$. Then $R$ is an [[Equivalence Relation]] if it is a [[Preorder Relation]] with the following additional property:\n* **Symmetry**: If $x, y \\in X$ and $xRy$, then $yRx$.\n\n# Remarks\n1. As with [[Preorder]]s, $\\pi_{1}(R) = \\pi_{2}(R) = X$.\n2. We often denote $R$ by $\\sim$ or $=$.\n3. If $(x, y) \\not\\in R$, then we write $x \\not\\sim y$ or $x \\neq y$.\n4. [[Correspondence of Surjective Functions, Partitions, and Equivalence Relations]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Equivalent-Conditions-for-Convexity":{"title":"Untitled Page","content":"# Statement\nSuppose $V$ is a [[Vector Space]] of $\\mathbb{R}$ and $f: V \\to \\mathbb{R}$. Then [[The following are Equivalent]]:\n1. $f$ is a [[Convex Function]]\n2. The [[Epigraph]] of $f$, $\\text{Epi}(f) \\subset V \\times \\mathbb{R}$ (endowed with the [[Product Vector Space]]), is a [[Convex Set]].\n3. (Check this - [[TODO]]) [[First Order Condition for Convexity]]: Further suppose $V$ is a [[Normed Vector Space]] and $f$ has [[Frechet Derivative]] $D_{v} f$ at all $v \\in V$. For all $x, y \\in V$ $$f(x) + D_{x}f(y-x) \\leq f(y)$$\n4. (Check this - [[TODO]]) [[Second Order Condition for Convexity]]: Further  suppose $V$ is a [[Normed Vector Space]]  and $f$ has 2nd [[Frechet Derivative]] $D_{v}^{2}f$ at all $v \\in V$. For all $x, y \\in V$ $$D^{2}_{x}f(y)(y) \\geq 0$$\n## Proof\n(1) $\\Rightarrow$ (2): Suppose $(u, a), (v, b) \\in \\text{Epi}(f)$. Then $f(u) \\leq a$ and $f(v) \\leq b$. Let $\\lambda \\in [0,1]$. Then $$f(\\lambda u + (1-\\lambda)v) \\leq \\lambda f(u) + (1-\\lambda) f(v) \\leq \\lambda a + (1- \\lambda) b$$  so $$\\Big( \\lambda u + (1- \\lambda) v, \\lambda a + (1-\\lambda)b \\Big) = \\lambda (u, a) + (1- \\lambda) (v, b) \\in \\text{Epi}(f)$$ and $\\text{Epi}(f)$ is a [[Convex Set]]. $\\checkmark$\n\n(2) $\\Rightarrow$ (1): Suppose $u, v \\in V$ and $\\lambda \\in [0,1]$. Then, by definition of [[Epigraph]], $(u, f(u)), (v, f(v)) \\in \\text{Epi}(f)$. Since $\\text{Epi}(f)$ is [[Convex Set|Convex]], we know $$S \\ni \\lambda (u, f(u)) + (1 - \\lambda) (v, f(v)) = \\Big(\\lambda u + (1- \\lambda)v, \\lambda f(u) + (1- \\lambda)f(v)\\Big)$$ Thus, by definition of [[Epigraph]], we have that $$f(\\lambda u + (1-\\lambda)v) \\leq \\lambda f(u) + (1-\\lambda) f(v)$$ and $f$ is a [[Convex Function]]. $\\checkmark$\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Eulers-Formula":{"title":"Untitled Page","content":"# Statement\nSuppose $r, \\theta \\in \\mathbb{R}$ so that $r \\geq 0$. Then\n$$re^{it \\theta} = r \\cos \\theta + i r \\sin \\theta$$\n## Proof\n[[TODO]] but basic idea is to apply the [[Taylor Expansion]]s of the terms.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Every-Open-Set-in-the-Metric-Topology-contains-a-Ball-around-each-Point":{"title":"Untitled Page","content":"# Statement\nLet $(M, d)$ be a [[Metric Space]] endowed with the [[Metric Topology]]. Suppose $U \\subset M$ is [[Open]]. Then $\\forall x \\in U$, there exists $\\epsilon \u003e 0$ s.t. $B_{\\epsilon}(x) \\subset U$.\n\n## Proof\nSince we are in the [[Metric Topology]], there exists [[Index Set]] $I$ and $\\{B_{\\epsilon_\\alpha}(x_\\alpha)\\}_{\\alpha \\in I}$ so that \n$$U = \\bigcup\\limits_{\\alpha \\in I} B_{\\epsilon_\\alpha}(x_\\alpha)$$\nLet $y \\in U$. Then there exists $\\alpha \\in I$ so that $y \\in B_{\\epsilon_\\alpha}(x_\\alpha)$. Now apply [[Finite Intersections of Open Balls contain Open Balls about each point]] to $B_{\\epsilon_\\alpha}(x_\\alpha) \\cap B_{\\epsilon_\\alpha}(x_\\alpha)$ to get $\\zeta \u003e 0$ so that $B_{\\zeta}(y) \\subset B_{\\epsilon_\\alpha}(x_{\\alpha}) \\subset U$. $\\blacksquare$\n\n# Other Outlinks\n* [[Open Ball]]\n* [[Real Numbers]]\n* [[Set Intersection]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Every-Preorder-Relation-induces-an-Order-Relation":{"title":"Untitled Page","content":"# Statement\nLet $(P, R)$ be a [[Preorder]]. Then there exists an [[Equivalence Relation]] $\\sim$ on $P$ so that $(P/\\sim,R/\\sim)$ is a [[Partial Ordering]].\n\n# Proof\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Every-Sequence-in-a-Compact-Set-has-a-Convergent-Subsequence":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Every-Sequence-on-the-Reals-contains-a-Monotone-Subsequence":{"title":"Untitled Page","content":"# Statement\nLet $(a_{n}) \\subset \\mathbb{R}$ be a [[Sequence]]. Then there exists a [[Subsequence]] $(a_{n_{k}})$ that is [[Monotone Function|monotone]].\n\n# Proof 1\nConsider the [[Complete Graph]] on $\\mathbb{N}$, $K_{\\mathbb{N}}$. Each edge is a pair $\\{i, j\\} \\in \\mathbb{N}^{2}$.  As a convention, when we write edge $\\{i, j\\}$, we will write it with $i \u003c j$. Now color each edge $\\{i,j\\}$ RED if $a_{i}\u003c a_{j}$ and BLUE if $a_{i} \\geq a_{j}$.\n\nDenote $A_{1}$ to be an [[Infinite Set|infinite]] subset of $\\mathbb{N} \\setminus \\{1\\}$ so the edges $\\{1, j\\}$ for $j \\in A_{1}$ are [[Monochromatic]], letting $i_{1}=1$. This is possible by [[Infinite Pigeonhole]]. Now set $i_{2} = \\min(A_{1})$. Since $A_{1} \\cap (\\mathbb{N} \\setminus \\{1,2\\})$ is infinite, by [[Infinite Pigeonhole]] there exists $A_{2} \\subset A_{1} \\cap (\\mathbb{N} \\setminus [i_{2}])$ s.t. the edges $\\{i_{2}, j\\}$ for $j \\in A_{2}$ are monochromatic. Continue in this way to get a non-increasing sequence of sets $A_{1} \\supset A_{2} \\supset \\cdots$ along with a sequence of indicies $(i_{k})$ s.t. $i_{k} \\in A_{k-1}$ $\\forall k \u003e 1$.\n\nNow examine $(i_{k})$. For each $k \\in \\mathbb{N}$ there exists a color $c_{k} \\in \\{\\text{RED}, \\text{BLUE}\\}$ so that $\\{i_{k}, i_{j}\\}$ is colored $c_{k}$ $\\forall j \u003e k$. Thus we get a corresponding sequence $(c_{k}) \\in \\{\\text{RED}, \\text{BLUE}\\}^{\\mathbb{N}}$. By [[Infinite Pigeonhole]], there exists a single-valued subsequence $(c_{k_{j}})$. Observe that \n1. If this subsequence is RED, then every $\\{i_{k_{j_{1}}}, i_{k_{j_{2}}}\\}$ is RED, so $(a_{i_{k_{j}}})$ forms an increasing sequence.\n2. If this subsequence is BLUE, then every $\\{i_{k_{j_{1}}}, i_{k_{j_{2}}}\\}$ is BLUE, so $(a_{i_{k_{j}}})$ forms a non-increasing sequence.\n\nIn either case we have our monotone subsequence. $\\blacksquare$\n\n## Remarks\n1. This proof is a [[Ramsey Theory]]-like proof. We are pairing finiteness of our labelling with infiniteness of our sequence to show we must contain some particular structure. [[Infinite Pigeonhole]] plays a key role here.\n2. This proof will hold for any [[Total Ordering]].\n\n## Credit\n[[Andrés E. Caicedo]] - [Answer to Math SE question](https://math.stackexchange.com/a/716484)\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Natural Numbers]]\n\n# Encounters\n1. [[Pugh - Real Mathematical Analysis]] - Ch 2 exercise, unknown number\n2. [Math SE Question](https://math.stackexchange.com/questions/716461/proof-verification-every-sequence-in-bbb-r-contains-a-monotone-sub-sequence)\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Every-Subsequence-of-a-Convergent-Sequence-converges-to-the-same-Limit":{"title":"Untitled Page","content":"# Statement\nLet $(a_{n}) \\subset X$ be a sequence in [[Metric Space]] $(X, d)$. If $a_{n} \\to a \\in X$  every [[Subsequence]] of $(a_{n})$ [[Sequence Convergence|converges]] to $a$.\n\n# Proof\nSuppose $a_{n} \\to a$. Let $(a_{n_{k}})$ be a [[Subsequence]] of $(a_{n})$. Since $a_{n} \\to a$, we know $\\forall \\epsilon \u003e 0$, $\\exists \\mathbb{N}\\in N$ s.t. $\\forall n \\geq N$ we have that\n$$d(a_{n}, a) \u003c \\epsilon$$\nLet $K = \\min\\{k \\in \\mathbb{N} : n_{k} \\geq N\\}$. Then we have that \n$$d(a_{n_{k}}, a) \u003c \\epsilon$$\nfor all $k \\geq K$ since $n_{k} \\geq N$ by our choice of $K$. Thus $a_{n_{k}} \\to a$ as well. $\\blacksquare$\n# Other Outlinks\n* [[Natural Numbers]]\n* [[Natural Numbers are Well-Ordered]]\n\n# Encounters\n1. [[Pugh - Real Mathematical Analysis]] - Ch 2, pg unknown","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Existence-of-Moment-Generating-Functions-imply-Existence-of-Moments":{"title":"Untitled Page","content":"# Statement\n[[TODO]]. You want to show that I can take derivatives of the MGF wrt $t$, then evaluate them at $t=0$ to get the moments. You might want to show this for $|X|$ by bounding its MGF with the MGF of $X$. ","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Expectation":{"title":"Untitled Page","content":"# Definition\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] on $\\Omega$. The [[Expectation]] of $X$ is defined as \n\n$$\\mathbb{E}(X) := \\int\\limits_{\\Omega} X d \\mathbb{P}(\\omega)$$\n\nso long as the right hand quantity exists. If it does not, we say $\\mathbb{E}(X)$ does not exist.\n\n# Other Outlinks\n* [[Integration]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Expectation-does-not-always-exist":{"title":"Untitled Page","content":"# Statement\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]]. There exists a [[Random Variable]] $X$ on $\\Omega$ so that $X \\not\\in \\bar{L^{1}}(\\Omega)$. In other words, $\\mathbb{E}(X)$ does not exist and we cannot even assign it a value of $\\infty$ or $-\\infty$ in the [[Compactification]] of the [[Real Numbers]].\n\n## Proof\nConsider the [[Probability Space]] $([-1, 1], \\mathcal{B}([-1, 1]), \\frac{1}{2}\\lambda)$, where $\\lambda$ is the [[Lebesgue Measure]]. Consider [[Random Variable]]\n\n$$X = \\begin{cases} \\frac{1}{x} \u0026 \\text{if } x \\neq 0\\\\\n0 \u0026 \\text{if } x=0\\end{cases}$$\nThen $$X^{+} = \\begin{cases} \\frac{1}{x} \u0026 \\text{if } x \u003e 0\\\\ 0 \u0026 \\text{if } x \\leq 0\\end{cases}$$ and $$X^{-} = \\begin{cases} -\\frac{1}{x} \u0026 \\text{if } x \u003c 0\\\\ 0 \u0026 \\text{if } x \\geq 0\\end{cases}$$\nThen\n\n$$\\begin{align*}\n\\frac{1}{2} \\int\\limits X^{+} d \\lambda \u0026= \\frac{1}{2} \\int\\limits_{(0,1]} \\frac{1}{x} d \\lambda\\\\\n\u0026=\\lim\\limits_{\\epsilon \\downarrow 0}\\left[-\\frac{1}{2} \\frac{1}{x^{2}}\\right]_{\\epsilon}^{1} \\\\\n\u0026=-\\frac{1}{2} + \\lim\\limits_{\\epsilon \\downarrow 0}\\frac{1}{2\\epsilon^{2}}\\\\\n\u0026= \\infty\n\\end{align*} $$\nLikewise\n$$\\begin{align*}\n\\frac{1}{2} \\int\\limits X^{-} d \\lambda \u0026= \\frac{1}{2} \\int\\limits_{[-1,0)} -\\frac{1}{x} d \\lambda\\\\\n\u0026=\\lim\\limits_{\\epsilon \\downarrow 0}\\left[\\frac{1}{2} \\frac{1}{x^{2}}\\right]_{-1}^{-\\epsilon} \\\\\n\u0026=-\\frac{1}{2} + \\lim\\limits_{\\epsilon \\downarrow 0}\\frac{1}{2\\epsilon^{2}}\\\\\n\u0026= \\infty\n\\end{align*} $$\nThus, by definition of [[Extended L1 Functions]], $X \\not\\in \\bar{L^{1}}$. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Expectation-is-Non-Decreasing":{"title":"Untitled Page","content":"[[Integration is Non-Decreasing]]\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Extended-L1-Functions":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\mathcal{M}, \\mu)$ be a [[Measure Space]].\n$$\\bar{L^{1}}(X) := \\{f: X \\to \\bar{\\mathbb{R}} : f \\text{ is } \\mathcal{M}\\text{-measureable}, \\int\\limits_{X} f^{+} d \\mu \u003c \\infty \\text{ or } \\int\\limits_{X} f^{-} d \\mu \u003c \\infty\\}$$\nIf $f \\in \\bar{L^{1}}$ and $\\int\\limits_{X} |f| d \\mu = \\infty$, then\n1. if $\\int\\limits_{X} f^{+} d \\mu = \\infty$, we define $\\int\\limits_{X} f d\\mu = \\infty$ \n2. if $\\int\\limits_{X} f^{-} d \\mu = \\infty$, we define $\\int\\limits_{X} f d \\mu = -\\infty$\n\n# Other Outlinks\n* [[L-Plus Functions]]\n* [[L1 Integrable Functions]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Extended-Real-Numbers":{"title":"Untitled Page","content":"# Definition 1\nWe define the [[Extended Real Numbers]] as\n\n$$\\bar{R} := \\mathbb{R} \\cup \\{-\\infty, \\infty\\}$$\nwhere $-\\infty \u003c x \u003c \\infty$ $\\forall x \\in \\mathbb{R}$. If we want to work with an interval in $\\bar{\\mathbb{R}}$ that includes $\\infty$, we put a square bracket on $\\infty$. For example, to write the [[Closed Interval]] from $x \\in \\mathbb{R}$ to (including) $\\infty$, we write $[x, \\infty]$. Likewise for $-\\infty$.\n\nSome arithmetic rules\n* $\\infty + x = \\infty$ $\\forall x \\in (-\\infty, \\infty]$\n* $- \\infty + x = - \\infty$ $\\forall x \\in [-\\infty, \\infty)$\n* $\\pm \\infty * x = \\pm \\text{sgn}(x) \\infty$ $\\forall x \\in \\bar{\\mathbb{R}} \\setminus \\{0\\}$\n* $\\frac{x}{\\pm \\infty} = 0$ $\\forall x \\in \\mathbb{R}$\n\nThe remaining arithmetic possiblities are undefined:\n* $\\infty - \\infty$\n* $\\pm \\infty * 0$\n* $\\frac{\\infty}{\\infty}$ for any choice of signs for $\\infty$\n\n# Definition 2\nWe can view the [[Extended Real Numbers]] as a [[Compactification]] of $\\mathbb{R}$. We want to attach points $\\{- \\infty, \\infty\\}$ to $\\mathbb{R}$ so that [[TODO]]\n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Extremum":{"title":"Untitled Page","content":"# Definition\n[[Infimum]] or [[Supremum]] depending on context.","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Fanos-Inequality":{"title":"Untitled Page","content":"Connects [[Conditional Entropy]] to [[Risk]]\n\nhttps://en.wikipedia.org/wiki/Fano%27s_inequality","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Field":{"title":"Untitled Page","content":"# Definition\nSuppose $X$ is a [[Nonempty]] [[Set]], and $+: X \\times X \\to X$, $*: X \\times X \\to X$ are two [[Operation]]s on $X$. Then $(X, +, *)$ is a [[Field]] if it satisfies the following properties:\n1. [[Additive Identity]]: There exists $0 \\in X$ so that for all $x \\in X$, $0 + x = x$.\n2. [[Associativity]] of $+$: For all $x,y,z \\in X$, $(x + y) + z = x + (y + z)$\n3. [[Commutativity]] of $+$: For all $x, y \\in X$, $x + y = y + x$ \n4. [[Additive Inverse]]s: If $x \\in X$, then there exists $-x \\in X$ so that $x + (-x) = 0$\n5. [[Multiplicative Identity]]: There exists $1 \\in X$ so that for all $x \\in X$, $1 * x = x$\n6. [[Associativity]] of $*$: For all $x,y,z \\in X$, $(x * y) * z = x * (y * z)$\n7. [[Commutativity]] of $*$: For all $x, y \\in X$, $x * y = y * x$\n8. [[Multiplicative Inverse]]s: If $x \\in X$ and $x \\neq 0$, then there exists $x^{-1} \\in X$ so that $x * x^{-1} = 1$\n9. [[Distributivity]] of $*$ over $+$: For all $x, y, z \\in X$, $x(y + z) = x*y + x*z$ \n\n## Remarks\n1. In other words, a [[Field]] is a [[Commutative Ring]] with [[Multiplicative Inverse]]s. Thus, a [[Field]] carries with it all properties of [[Commutative Ring]]s and its subclasses (like [[Group]]s).","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Finite-Extremums-get-arbitrarily-close":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Finite-Intersections-of-Open-Balls-contain-Open-Balls-about-each-point":{"title":"Untitled Page","content":"# Statement\nLet $(M, d)$ be a [[Metric Space]]. Let $n \\in \\mathbb{N}$, $(x_{i})_{i=1}^{n} \\subset M$, and $(\\epsilon_{i})_{i=1}^{n} \\subset [0, \\infty)$. Then for each $y \\in \\bigcap\\limits_{i \\in [n]} B_{\\epsilon_{i}}(x_{i})$, there exists $\\zeta \u003e 0$ so that $$B_{\\zeta}(y) \\subset \\bigcap\\limits_{i \\in [n]} B_{\\epsilon_{i}}(x_{i})$$\n\n## Proof\nWe first show for two [[Open Ball]]s, then extend by [[Induction]]. Let $x, y \\in M$ and let $\\epsilon, \\delta \u003e 0$. Suppose $z \\in B_\\epsilon(x) \\cap B_{\\delta}(y)$. Then $d(x, z) \u003c \\epsilon$ and $d(y, z) \u003c \\delta$. Let $\\zeta = \\min(\\epsilon - d(x, z), \\delta - d(y, z))$. Then, if $w \\in B_{\\zeta}(z)$ then:\n$$\\begin{align*}\nd(w, x) \u0026\\leq d(x, z)  + d(z, w)\\\\\n\u0026\u003c d(x, z) + \\zeta\\\\\n\u0026\\leq d(x, z) + \\epsilon - d(x, z)\\\\\n\u0026=\\epsilon\n\\end{align*}$$\nand $$\\begin{align*}\nd(w, y) \u0026\\leq d(y, z)  + d(z, w)\\\\\n\u0026\u003c d(y, z) + \\zeta\\\\\n\u0026\\leq d(y, z) + \\delta - d(y, z)\\\\\n\u0026=\\delta\n\\end{align*}$$\nThus $w \\in B_{\\epsilon}(x) \\cap B_{\\delta}(y)$ and $B_{\\zeta}(z) \\subset B_{\\epsilon}(x) \\cap B_{\\delta}(y)$.\n\n[[TODO]] *make this more rigorous* Given $n$ [[Open Ball]]s and a point in their [[Set Intersection]], we can get an [[Open Ball]] around it that is contained in the first $n-1$ [[Open Ball]]s. Then we apply our argument above to find a smaller ball within its intersection with ball $n$. $\\blacksquare$\n\t\n\t","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/First-Countable-Space":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Then $X$ is a [[First Countable Space]] if  for each $x \\in X$, there exists a [[Countable]] [[Neighborhood Basis]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Frechet-Derivative":{"title":"Untitled Page","content":"# Definition\nSuppose $V, W$ are [[Normed Vector Space]]s and $f: V \\to W$. Suppose for $v \\in V$ there exists $T \\in BL(V, W)$ s.t.\n\n$$\\lim\\limits_{h \\to 0} \\frac{|f(v + h) - f(v) - Th|_{W}}{|h|_{V}} = 0$$\n\nThen we say $f$ has [[Frechet Derivative]] $D_{v} f =T$.\n\n## Properties\n### Continuity of $f$\n#### Statement\nIf $D_{v} f$ exists for $v \\in V$, then $f$ is [[Continuous]] at $v$. [[TODO]]\n#### Proof\nExistence of the [[Frechet Derivative]] means\n$$\\begin{align*}\n\u0026\u0026\\lim\\limits_{h \\to 0} \\frac{|f(v + h) - f(v) - Th|_{W}}{|h|_{V}} = 0\\\\\n\u0026\\Rightarrow\u0026 0 = \\lim\\limits_{h \\to 0} \\Big| \\big( f(v + h) - f(v) - Th\\big) - \\mathbf{0}_{W} \\Big|_{W}\\\\\n\\end{align*}$$\nBy definition of [[Function Limit]], we have that\n$$\\begin{align*}\n\u0026\\lim\\limits_{h \\to 0} \\big( f(v + h) - f(v) - Th\\big) = \\mathbf{0}_{W}\\\\\n\u0026\\Rightarrow \\lim\\limits_{h \\to 0} \\big( f(v + h) - f(v) \\big) - \\lim\\limits_{h \\to 0} Th = \\mathbf{0}_{W}\\\\\n\u0026\\Rightarrow \\lim\\limits_{h \\to 0} \\big( f(v + h) - f(v) \\big) = \\mathbf{0}_{W}\n\\end{align*}$$\nWhere the last line follows because [[Bounded Linear Maps are Continuous]] (how to prove this?). The last line is the definition of [[Continuous|continuity]] and $f$ is [[Continuous]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Frobenius-Inner-Product":{"title":"Untitled Page","content":"# Statement\nLet $n, m \\in \\mathbb{N}$. Then $$\\langle A, B \\rangle := \\text{tr} (A^{T} B)$$. This defines an [[Inner Product]] on $\\mathbb{R}^{n \\times m}$.\n\n## Proof\nWe need to establish that we indeed have an [[Inner Product]]. We check the conditions: [[TODO]]\n\n## Properties\n1. Observe that $\\langle A, B \\rangle = \\text{tr} (A^{T} B) = \\sum\\limits_{j=1}^{m} A_{\\cdot j} \\cdot B_{\\cdot j} = \\sum\\limits_{j=1}^{m} \\sum\\limits_{i=1}^{n} A_{ij} B_{ij}$, where $A_{\\cdot j}$ denotes the $j$th column of $A$. This establishes that this [[Inner Product]] is just the [[Dot Product]] when $A$ and $B$ are rolled out into vectors of $\\mathbb{R}^{nm}$.\n\n# Other Outlinks\n* [[Euclidean Vector Space]]\n* [[Natural Numbers]]\n* [[Real Matrix Vector Space]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Frobenius-Inner-Product-Space":{"title":"Untitled Page","content":"# Definition\n Let $\\mathbb{R}^{n \\times m}$ be the [[Matrix Vector Space]] over $\\mathbb{R}$, and $\\langle \\cdot, \\cdot \\rangle$ be the [[Frobenius Inner Product]]. We call the [[Inner Product Space]] $(\\mathbb{R}^{n \\times m}, \\langle \\cdot, \\cdot \\rangle)$ the [[Frobenius Inner Product Space]].","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Frustrated-Alternating-Tree":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is an [[Undirected Graph]], $M$ is a [[Matching]] on $G$, and $(T, r)$ is an $M$-[[Matching Alternating Tree|alternating tree]]. We say $T$ is a [[Frustrated Alternating Tree]] if $\\forall e \\in E(G)$ s.t. $e \\cap B(T) \\neq \\emptyset$, we have that $e \\cap A(T) \\neq \\emptyset$. That is, each [[Graph Edge]] with a [[Graph Vertex|endpoint]] in $B(T)$ has its other endpoint in $A(T)$.\n\n## Results\n* [[If a Graph has a Frustrated Tree then it has no Perfect Matching]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Function":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Function-Limit":{"title":"Untitled Page","content":"# Definition\nLet $(M,d_{1})$ and $(N, d_{2})$ be two [[Metric Space]]s. Let $f: M \\to N$ be a [[Function]].  Suppose there exists $y \\in N$ s.t. $\\forall \\epsilon \u003e 0$ $\\exists \\delta \u003e0$ s.t. $\\forall u \\in B_{\\delta}(x) \\setminus \\{x\\}$\n$$d_{2}(f(u), f(y)) \u003c \\epsilon$$\nThen we call $y$ the [[Function Limit]] of $f$ at $x$.\n\n# Other Outlinks\n - [[Real Numbers]]\n ","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Functions-to-a-Field-form-a-Vector-Space":{"title":"Untitled Page","content":"# Statement\nLet $S$ be a [[Set]] and $F$ some [[Field]]. Then $\\mathcal{F} = \\{f : S \\to F\\}$ is a [[Vector Space]] with\n* $(f +_\\mathcal{F} g)(x) = f(x) +_F g(x)$ for $x \\in S$, $f,g \\in \\mathcal{F}$.\n* $(c *_\\mathcal{F} f)(x) = c *_{F} f(x)$ for $c \\in F$, $f \\in \\mathcal{F}$.\n## Proof\nWe verify the definition of a [[Vector Space]]. Let $\\mathbf{0}(x) = 0$ for all $x \\in S$.\n1. [[Abelian Group]]:\n\t1. $(\\mathbf{0} + f)(x) = 0 + f(x) = f(x)$ for $x \\in S$. $\\checkmark$\n\t2. Let $f, g, h \\in \\mathcal{F}$. Then $$\\begin{align*}\n\t((f+g) + h)(x) \u0026= (f + g)(x) + h(x) \\\\\n\t\u0026= f(x) + g(x) + h(x) \\\\\n\t\u0026= f(x) + (g + h)(x) \\\\\n\t\u0026= (f + (g + h))(x).\n\t\\end{align*}$$ $\\checkmark$\n\t3. Let $f, g \\in \\mathcal{F}$. Then $$\\begin{align*}\n\t(f + g)(x) \u0026= f(x) + g(x) \\\\\n\t\u0026= g(x) + f(x) \\\\\n\t\u0026= (g + f)(x)\n\t\\end{align*}$$ for $x \\in S$. $\\checkmark$\n\t4. Let $f \\in \\mathcal{F}$. Then $$\\begin{align*}\n\t(f + (-f))(x) \u0026= f(x) - f(x) \\\\\n\t\u0026= 0 \\\\\n\t\u0026= \\mathbf{0}(x)\n\t\\end{align*}$$ for $x \\in S$. $\\checkmark$\n1. Compatibility with [[Scalar Multiplication]], $f, g \\in \\mathcal{F}$:\n\t1. $(1f)(x) = 1 f(x) = f(x)$ for $x \\in S$. $\\checkmark$\n\t2. $c_{1}, c_{2} \\in F$ then $$\\begin{align*}\n\t((c_{1}c_{2})f)(x) \u0026= c_{1}c_{2}f(x) \\\\\n\t\u0026= c_{1}(c_{2}f)(x) \\\\\n\t\u0026= (c_{1}(c_{2}f))(x)\n\t\\end{align*}$$ for $x \\in S$. $\\checkmark$\n\t3. $c \\in F$, then $$\\begin{align*}\n\t(c(f + g))(x) \u0026= c(f+g)(x) \\\\\n\t\u0026= cf(x) + cg(x) \\\\\n\t\u0026= (cf)(x) + (cg)(x)\n\t\\end{align*}$$ for $x \\in S$. $\\checkmark$\n\t4. $c, d \\in F$, then $$\\begin{align*}\n\t((c + d)f)(x) \u0026= (c + d)f(x) \\\\\n\t\u0026= cf(x) + df(x) \\\\\n\t\u0026= (cf + df)(x)\n\t\\end{align*}$$ for $x  \\in S$. $\\checkmark$\n\n$\\blacksquare$ \n","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Gaussian-Random-Variable":{"title":"Untitled Page","content":"# Definition\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]]. Then $X$ is a [[Gaussian Random Variable]] on $\\Omega$ if it has [[Distribution]] determined by [[Probability Density Function]]\n\n$$f_{X}(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{- \\frac{x- \\mu^{2}}{\\sigma^{2}}}$$\n\nwhere $\\sigma$ and $\\mu$ are parameters for the distribution. We write $X \\sim  \\mathcal{N}(\\mu, \\sigma^{2})$.\n\nThe [[Probability Density Function]] is taken with respect to the [[Lebesgue Measure]] on $\\mathcal{B}(\\mathbb{R})$.\n\n## Properties\n1. $\\mu = \\mathbb{E}(X)$ **Proof**: [[TODO]]\n2. $\\sigma^{2}= \\text{Var}(X)$ **Proof**: [[TODO]]\n3. [[Gaussian Distributions are Symmetric about their Mean]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Graph-Connected-Component":{"title":"Untitled Page","content":"# Definition\nLet $G = (V, E)$ be a [[Directed Graph]]. Then [[TODO]]:\n* need to define [[Subgraph]]\n* [[Connected Graph]]\n* Highlight the connection with [[Connected Component]] - topological","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Graph-Deficiency":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $\\mathcal{M} = \\{M \\subset E(G) : M \\text{ is a Matching}\\}$. Then the [[Graph Deficiency]] of $G$, denoted $\\text{def}(G)$, is $$\\text{def}(G) = \\min \\{|\\{v \\in V : v \\text{ is exposed by }M\\}| \\leq |V(G)| : M \\in \\mathcal{M}\\}$$\n## Remarks\n1. For [[Infinite Set]]s, $\\text{def}(G)$ is [[Well-Defined]] because [[Cardinals form a Well-Ordering]].\n2. If our $V(G)$ is a [[Finite Set]] (as is the case in most relevant cases), then $\\text{def}(G) = |V(G)| - \\nu(G)$.\n\n# Other Outlinks\n* [[Matching]]\n* [[Graph Edge]]\n* [[Matching Exposed]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Graph-Path":{"title":"Untitled Page","content":"# Definition 1\nLet $G = (V, E)$ be a [[Directed Graph]]. A [[Graph Path]] of length $n \\in \\mathbb{N}$ from [[Graph Vertex|vertex]] $u \\in V$ to $v \\in V$ is a [[Tuple]] $(e_{1}, e_{2}, \\dots, e_{n}) \\in E^{n}$ so that \n1. $e_{1} = (u, x)$ for some $x \\in V$\n2. $e_{2} = (y, v)$ for some $y \\in V$\n3. $e_{i} = (x, y)$ and $e_{i+1} = (y, z)$ for some $x,y,z \\in V$ for all $i \\in [n]$\n\n## Remarks\n1. The [[Empty Tuple]] $()$ is a [[Graph Path]] from $v \\in V$ to itself for all $v \\in V$. It has length $0$.\n2. Suppose $P$ is a [[Graph Path]] of length $n \\in \\mathbb{N}$ from $u \\in V$ to $v \\in V$. We let $V(P)$ signify the [[Tuple]] $(e_{1,1} = u, e_{2,1}, \\dots, e_{n,1}, v) \\subset V^{n+1}$.\n# Definition 2\nLet $G$ be an [[Undirected Graph]]. Then a [[Graph Path]] of length $n \\in \\mathbb{N}$ from from [[Graph Vertex|vertex]] $u \\in V$ to $v \\in V$ is a [[Graph Path]] of length $n \\in \\mathbb{N}$ on the [[Represent an Undirected Graph as a Directed Graph|Directed Graph representation]] of $G$.\n\n# Other Outlinks\n* [[Natural Numbers]]\n* [[Graph Edge]]","lastmodified":"2022-04-25T04:32:04.810518431Z","tags":null},"/Group":{"title":"Untitled Page","content":"# Definition\nSuppose $X$ is a [[Nonempty]] [[Set]] and $+: X \\times X \\to X$ is an [[Operation]] on $X$. Then $X$ is a [[Group]] if it satisfies the following properties:\n1. [[Additive Identity]]: There exists $0 \\in X$ so that for all $x \\in X$, $0 + x = x$.\n2. [[Associativity]] of $+$: For all $x,y,z \\in X$, $(x + y) + z = x + (y + z)$\n3. [[Additive Inverse]]s: If $x \\in X$, then there exists $-x \\in X$ so that $x + (-x) = 0 = (-x) + x$\n\n## Remarks\n1. In other words, a [[Group]] is a [[Monoid]] with [[Additive Inverse]]s.\n2. When the context is clear, we often write $a + b$ as $ab$. This helps clear up confusion with the usual understanding of $+$.\n3. If a [[Group]] also satisfies [[Commutativity]] of $+$, then it is an [[Abelian Group]]. \n4. [[The inverse in a Group is unique]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Halfspaces-are-Convex-Sets":{"title":"Untitled Page","content":"# Statement 1\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$, let $a \\in V$, $b \\in \\mathbb{R}$, and let $H$ be the [[Closed Halfspace]] $$H = \\{x : \\langle x, a \\rangle \\geq b\\}.$$ Then $H$ is a [[Convex Set]].\n\n## Proof\n[[TODO]] This should follow from the fact that [[Inner Products are Linear in their first argument]]\n\n# Statement 2\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$, let $a \\in V$, $b \\in \\mathbb{R}$, and let $H$ be the [[Open Halfspace]] $$H = \\{x : \\langle x, a \\rangle \u003e b\\}.$$ Then $H$ is a [[Convex Set]].\n\n## Proof\n[[TODO]] follows from statement 1 and [[Interior of Convex Set is Convex]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Hausdorff-Space":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\tau)$ be a [[Topological Space]]. Then $X$ is a [[Hausdorff Space]] if for every $x, y \\in X$ there exist $U, V \\in \\tau$ s.t. $x \\in U$, $y \\in V$, and $U \\cap V = \\emptyset$\n\nAlso known as a \n* *T2 Space*\n* *Seperated Space*\n# Other Outlinks\n* [[Mutually Disjoint]]\n* [[Empty Set]]\n* [[Open]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Heine-Borel-Theorem":{"title":"Untitled Page","content":"\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Hypergraph-Edge":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Hyperplane":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$. A [[Hyperplane]] is a [[Set]] of the form\n$$H = \\{x : \\langle x, a \\rangle = b\\}$$\nfor any $a \\in V$ and $b \\in \\mathbb{R}$.\n\n## Remarks\n1. [[TODO]] - talk about the geometric interpretation, see [[Boyd - Convex Optimization]] - sect 2.2.1 page 27\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Hyperplanes-are-Affine-Sets":{"title":"Untitled Page","content":"# Statement\n[[TODO]] - [[Hyperplane]] is an [[Affine Set]].\n\n## Proof\n[[TODO]] - Reference [[Inner Products are Linear in their first argument]], then [[Solution set of Linear Equations is an Affine Set]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Ideas":{"title":"Untitled Page","content":"* [[Oracle Government]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Identity-Function":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]]. The [[Identity Function]] on $X$ is the [[Function]] $\\text{id}_{X}: X \\to X$ so that $\\forall x \\in X$, $\\text{id}_{X}(x) = x$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Improper-Face":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$ and let $P \\subset V$ be a [[Convex Polytope]]. An [[Improper Face]] of $P$ is a [[Convex Polytope Face|face]] of $P$ that is not a [[Proper Face]]. That is, the [[Convex Polytope Face|face]] is $P$ or $\\emptyset$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Indicator-Functions-on-Measureable-Sets-are-in-L+":{"title":"Untitled Page","content":"# Statement\nLet $(X, \\mathcal{M})$ be a [[Measure Space]]. Suppose $A \\in \\mathcal{M}$ . Then $1_{A} \\in L^{+}(\\mathcal{M})$.\n\n## Proof\n[[Indicator Function]]s have by definition the [[Codomain]] $\\{0, 1\\}$, so they are non-negative. Thus, it suffices to check that $1_{A}$ is $\\mathcal{M}$-[[Borel Measureable Function|measureable]]. Checking the definition of [[Measureable Function]] and recalling the form of a [[Sigma Algebra induced by an Indicator Function]]:\n$$\\sigma(1_{A}) = \\{\\emptyset, A, A^{C}, X\\} \\subset \\mathcal{M}$$\n\nsince $A \\in \\mathcal{M}$. Thus $1_{A}$ is $\\mathcal{M}$-measureable. $\\blacksquare$\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[L-Plus Functions]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Induced-Probability-Measure":{"title":"Untitled Page","content":"# Definition\nThe [[Induced Probability Measure]] of a [[Random Variable]] $X$ is the [[Lebesgue-Stieltjes Measure]] determined by the [[Probability Distribution Function]] of $X$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Induced-Subgraph":{"title":"Untitled Page","content":"# Definition\nLet $G = (V, E)$ be a [[Directed Graph]] or [[Undirected Graph]]. Let $A \\subset V$. Then the [[Induced Subgraph|Subgraph induced by]] $A$, denoted $G[A]$, is $$G[A] = (A, E(A)),$$ where $E(A) = \\{e \\in E : e \\text{ has both endpoints in }A\\}$. That is, if\n* $G$ is an [[Undirected Graph]], $E(A) = \\{e \\in E: e \\cap A = e}$\n* $G$ is a [[Directed Graph]], $E(A) = \\{(u, v) \\in E : \\{u, v\\} \\cap A = \\{u, v\\}\\}$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Infimums-are-Non-Increasing-Set-Functions":{"title":"Untitled Page","content":"# Statement\nLet $(T, \\leq)$ be a [[Total Ordering]] and suppose $A \\subset B \\subset T$ such that $\\inf A$ and $\\inf B$ both exist. Then $\\inf A \\geq \\inf B$.\n\n# Proof\nLet $L_{A}$ be the [[Set]] of [[Lower Bound]]s for $A$ and let $L_{B}$ be likewise for $B$. Observe that if $x \\in L_{B}$, then $x \\leq b$ $\\forall b \\in B$. Since every $a \\in A$ is also in $B$, we have that $x \\leq a$ $\\forall a \\in A$. Thus $x$ is a [[Lower Bound]] for $A$ and $L_{B} \\subset L_{A}$. Since $\\inf A = \\max L_{A}$ and $\\inf B = \\max L_{B}$, we know that\n$$\\inf A = \\max L_{A} \\geq \\max L_{B} = \\inf B$$\nsince [[Maximums are Non-Decreasing Set Functions]]. $\\blacksquare$\n# Other Outlinks\n* [[Infimum]]\n* [[Maximum]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Infinite-Pigeonhole":{"title":"Untitled Page","content":"# Statement\nLet $X$ be an [[Infinite Set]]. Let $\\bigsqcup\\limits_{i \\in [n]} A_{i} = X$ be a [[Finite Set|finite]] [[Partition]] of $X$ for $n \\in \\mathbb{N}$. Then $\\exists j \\in [n]$ s.t. $A_{j}$ is infinite.\n\n# Proof\nSuppose not. Then each $A_{i}$ is finite for $i \\in [n]$. Denote $|A_{i}|= c_{i}$. Since $A_{i}$ are [[Disjoint Sets]], we have that $\\infty \u003e \\sum\\limits_{i \\in [n]} c_{i} = \\big|\\bigsqcup\\limits_{i \\in [n]} A_{i} \\big|  = |X|$, [[Proof by Contradiction|contradicting]] our assumption that $X$ was infinite. $\\blacksquare$\n\n# Other Outlinks\n* [[Cardinal Arithmetic]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Information-Space":{"title":"Untitled Page","content":"# Definition\nSuppose $\\Omega$ is a [[Set]] with [[Sigma Algebra]] $\\mathcal{M}$. Suppose $\\mu$ is a [[Measure]] on $\\mathcal{M}$. Consider the set $$\\mathcal{R} =\\{(A, B) \\in  \\mathcal{M} \\times \\mathcal{M}: \\mu(A) \\in \\mathbb{R} \\text{ or } \\mu(A) \\neq \\mu(B) \\}$$ and mapping $\\imath: \\mathcal{R} \\to [-\\infty, \\infty]$ defined as\n$$\\imath(A||B) := \\imath(A, B) = \\log \\frac{1}{\\mu(A)} - \\log \\frac{1}{\\mu(B)}$$\nThen $(\\Omega, )$\n\n## Properties\n1. $\\imath(A||A) = 0$\n2. $imath$\n\n\nI really want a space that captures what it means to gain information from learning about an event that allows for cases where probability distributions do not apply (like [[Uniform Distribution]] over all of $\\mathbb{R}$).\n\nI also want this way of assigning information quantities to not depend on scaling of the underlying mass distribution. I was thinking that relative information could help with that since it would not be affected by scaling (relative measures of surprisal).\n\nAnother way to think about is in terms of conditionals. If I knew the true value was on some finite measure set $A$, then this information quantity would determine a probability distribution on $A$. \n\nConsider the [[Thought Experiment]] where person $A$ places a $x \\in \\mathbb{R}$ into a hat, then asks $B$ to \n* Guess what number it is?\n* Guess what interval $[n, n+1)$ for $n \\in \\mathbb{N}$ that is is in?\n* $A$ successively reveals what intervals its in and asks $B$ to update guesses.\n\nI think its easier to start with the 3rd case. Some possibilities\n1. $A$ first tells $B$ $x \\in [0, \\infty)$, then $A$ tells $B$ that $x \\in [-1, \\infty)$. The second piece of information does not provide any new information given the first. Here $S_{2} = [-1, \\infty)$ gives us no new information when we know that $x \\in S_{1} = [0, \\infty)$\n2. $A$ tells $B$:\n\t1. $x \\in S_{1} = [0, \\infty)$\n\t2. $x \\in S_{2} = (-\\infty, 1]$\n\n\t$S_{2}$ gives $B$ additional information allowing $B$ to conclude that $x \\in [0, 1] = S_{1} \\cap S_{2}$.\n3. $A$ tells $B$:\n\t1. $x \\in S_{1} = [0, \\infty)$\n\t2. $x \\in S_{2} = (-\\infty, -1]$\n\n\tHere we have a contradiction, so we might imagine that $B$ receives an indeterminate information update. Or we could also say that $x \\in \\emptyset$.\n\nLets keep track of the evolution of the [[Sigma Algebra]] representing my knowledge about $x$ as we go. At the beginning $\\mathcal{B}_{0} = \\{\\emptyset, \\mathbb{R}\\}$.\n1. Case 1:\n\t1. $\\mathcal{B}_{1} = \\{\\emptyset, [0, \\infty)\\}$... This is not really making sense\n\n\nWhat I really want to formalize is this statement that, only knowing that $x \\in \\mathbb{R}$, I should give equal preference to any hypothesis about the value of $x$.\n\nI think a generalized [[Kullback–Leibler Divergence]] works here. That is, given some prior [[Measure]] $\\mu$, and some updated [[Measure]] $\\nu$ s.t. $\\nu \u003c\u003c \\mu$ all on $X$, we say \n$$D_{KL}(\\nu || \\mu) = \\int\\limits_{X} \\log \\left(\\frac{d\\nu}{d \\mu}\\right) d \\nu = \\int\\limits_{X} \\log \\left(\\frac{d\\nu}{d \\mu}\\right) \\frac{d\\nu}{d \\mu} d \\mu$$\nIf we assume our prior is $\\mu = \\lambda$, then in case\n1. \n\t1. $D_{KL}(\\nu_{1}||\\mu) = \\int\\limits_X$ ... This doesn't work out nicely unless things are rescaled to be probability distributions.\n2. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Injection":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Inner-Product":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $F$, which is either the [[Real Numbers]] or [[Complex Numbers]]. Then an [[Inner Product]] on $V$ is a [[Function]] $\\langle \\cdot, \\cdot \\rangle: V \\times V \\to F$ that satisfies the following properties\n1. [[Complex Conjugate|Conjugate]] Symmetry: $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}$ for all $\\mathbf{u}, \\mathbf{v} \\in V$.\n2. [[Inner Products are Linear in their first argument]]: $\\langle a \\mathbf{u} + b \\mathbf{v}, \\mathbf{w} \\rangle = a \\langle \\mathbf{u}, \\mathbf{w} \\rangle + b \\langle \\mathbf{v}, \\mathbf{w} \\rangle$\n3. Positive Definiteness: $\\langle \\mathbf{u}, \\mathbf{u} \\rangle \\geq 0$ with equality [[If and Only If|iff]] $\\mathbf{u} = \\mathbf{0}$. The $(\\Leftarrow)$ direction is implied by the other properties.\n\n## Properties\n[[TODO]]\n\n# Other Outlinks","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Inner-Product-Space":{"title":"Untitled Page","content":"# Definition\nSuppose $V$ is a [[Vector Space]] with [[Inner Product]] $\\langle \\cdot, \\cdot, \\rangle$. Then $(V, \\langle \\cdot, \\cdot \\rangle)$ is an [[Inner Product Space]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Inner-Products-are-Linear-in-their-first-argument":{"title":"Untitled Page","content":"[[TODO]] - Uses [[Linear Function]]s, how to phrase this? This is just a axiom of [[Inner Product]]s, but it is useful in its own right.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Integer-Linear-Program":{"title":"Untitled Page","content":"# Definition\nSuppose $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, $F \\subset \\mathbb{Z}$. Then the [[Constrained Optimization Program]] \n\n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\in F\n\\end{align*}$$\nis an [[Integer Linear Program]]\n\n## Properties\n1. An [[Integer Linear Program]] is just a [[Linear Program]] where the nonnegativity constraint is replaced with an [[Integer|integrality]] contraint.\n2. [[Integer Linear Program|Integer Linear Programming]] is [[NP-Complete]].\n\n# Other Outlinks\n* [[Natural Numbers]]\n* [[Real Numbers]]\n* [[Matrix]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Integration-is-Non-Decreasing":{"title":"Untitled Page","content":"# Statement\nLet $(X, \\mathcal{M}, \\mu)$ be a [[Measure Space]] and let $f,g: X \\to \\mathbb{R}$ be [[Borel Measureable Function]]s so that $f \\leq g$ $\\mu$-[[Almost Everywhere]]. Then $$\\int\\limits_{X} f d \\mu \\leq \\int\\limits_{X} g d \\mu$$\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Integration]]\n* [[Non-Decreasing]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Interesting-Topics":{"title":"Untitled Page","content":"* [[Wasserstein Distance]] - [[2022-04-14]]\n\t- AKA [[Earth Mover Distance]]\n\t- distance between [[Probability Distribution Function|probability distributions]].\n* [[Gradient Flow]] - [[2022-04-14]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Interior-of-Convex-Set-is-Convex":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Topological Vector Space]] over $\\mathbb{R}$ and let $S \\subset V$ be a [[Convex Set]]. Then $\\text{int} S$ is a [[Convex Set]].\n\n## Proof\n[[TODO]] - see [Stackexchange - Interior of a convex set is convex](https://math.stackexchange.com/a/471601)\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Interpreting-Deep-Codes":{"title":"Untitled Page","content":"* [[Minimizing Binary Cross-Entropy]]\n* [[Interpreting TurboAE Decoder]]\n* [[Minimizing Conditional Entropy]]\n* [[Counterexample showing that optimizing BLER is not the same as optimization CE]]\n* [[Thinking about the information theory of NNs]]\n* [[Neural Network Differentiation Visualization]]\n* [[Empirically Minimizing Conditional Entropy]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Interpreting-TurboAE-Decoder":{"title":"Untitled Page","content":"Dates: [[2022-03-01]]\n\n# Dimensions to Interpret\n1. Role of iterations\n\t1. Train a classification layer using intermediate representations between iterations. This can be trained at once by freezing TurboAE parameters and training.\n2. How close is TurboAE decoder to true decoder and to BCJR\n\t1. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Intersection-Sigma-Algebra":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\mathcal{M})$ is a [[Measure Space]] and let $A \\subset X$. Then the collection\n$$\\mathcal{M} \\cap A := \\{E \\cap A : E \\in \\mathcal{M}\\}$$\nis the [[Intersection Sigma Algebra]] on $A$.\n\n## Remarks\n1. We should check that $\\mathcal{M} \\cap A$ is indeed a [[Sigma Algebra]]:\n\t\n\t**Proof**: We check the criteria for being a [[Sigma Algebra]]\n\t1. Since $X \\in \\mathcal{M}$ and $X \\cap A = A$, we have that $A \\in \\mathcal{M} \\cap A$. $\\checkmark$\n\t2. Suppose $F \\in \\mathcal{M} \\cap A$. Then there exists $E \\in \\mathcal{M}$ so $F = E \\cap A$. Thus $$\\begin{align*}\nA \\setminus F \u0026= A \\cap (E \\cap A)^{C}\\\\\n\u0026= A \\cap (E^{C} \\cup A^{C})\\\\\n\u0026=(A \\cap E^{C}) \\cup (A \\cap A^{C})\\\\\n\u0026=E^{C} \\cap A \\in \\mathcal{M} \\cap A\n\\end{align*}$$\n\t\tsince $E^{C} \\in \\mathcal{M}$. $\\checkmark$\n\t3. Suppose $(F_n)_{n=1}^{\\infty} \\subset \\mathcal{M} \\cap A$. Then there exists $(E_n)_{n=1}^{\\infty} \\subset \\mathcal{M}$ so that $F_{n} = E_{n} \\cap A$ $\\forall n \\in \\mathbb{N}$. Then $$\\bigcup\\limits_{n \\in \\mathbb{N}} F_{n} = \\bigcup\\limits_{n \\in \\mathbb{N}} (E_{n} \\cap A) =  A \\cap \\bigcup\\limits_{n \\in \\mathbb{N}} E_{n} \\in \\mathcal{M} \\cap A$$\n\t\tsince $\\bigcup\\limits_{n \\in \\mathbb{N}} E_{n} \\in \\mathcal{M}$. $\\checkmark$ $\\blacksquare$\n\t","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Intersection-of-Affine-Sets-is-Affine":{"title":"Untitled Page","content":"# Statement\nLet $V$  be a [[Vector Space]] on $\\mathbb{R}$ and let $\\mathcal{S} \\subset \\mathcal{P}(V)$ be a [[Set|collection]] of [[Affine Set]]s. Then $\\bigcap\\limits_{S \\in \\mathcal{S}}S$ is a [[Affine Set]].\n\n## Proof 1\nLet $T = \\bigcap\\limits_{S \\in \\mathcal{S}}S$ and let $u, v \\in T$. Suppose $\\lambda \\in \\mathbb{R}$. Then $\\forall S \\in \\mathcal{S}$, we have that $\\lambda u + (1 - \\lambda) v \\in S$. Thus $\\lambda u + (1 - \\lambda) v \\in T$ and $T$ is a [[Affine Set]]. $\\blacksquare$\n\n## Proof 2\n[[Intersection of Structures is still a Structure]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Intersection-of-Closed-Sets-is-Closed":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Suppose $\\mathcal{K} \\subset \\mathcal{P}(X)$ is a collection of [[Closed|closed sets]] in $X$. Then $\\bigcap\\limits \\mathcal{K}$ is [[Closed]] in $X$.\n\n## Proof\nThis follows from [[De Morgan's Law]]. [[Index Set|Index]] $\\mathcal{K}$ with $I$. That is, $\\{K_{\\alpha}\\}_{\\alpha \\in I} = \\mathcal{K}$. Then $K_{\\alpha}^{C}$ is [[Open]] in $X$ since $K_{\\alpha}$ is [[Closed]] for all $\\alpha \\in I$. Thus,\n\n$$\\begin{align*}\n\\Big( \\bigcap\\limits_{\\alpha \\in I} K_{\\alpha} \\Big)^{C} \u0026= \\bigcup\\limits_{\\alpha \\in I} K_{\\alpha}^{C} \u0026 \\text{De Morgan's Law}\\\\\n\u0026 \\in \\tau\n\\end{align*}$$\n\nsince $\\tau$ is closed under [[Set Union]]s. Thus $\\bigcap\\limits_{\\alpha \\in I} K_\\alpha$ is [[Closed]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Intersection-of-Convex-Sets-is-Convex":{"title":"Untitled Page","content":"# Statement\nLet $V$  be a [[Vector Space]] on $\\mathbb{R}$ and let $\\mathcal{S} \\subset \\mathcal{P}(V)$ be a [[Set|collection]] of [[Convex Set]]s. Then $\\bigcap\\limits_{S \\in \\mathcal{S}}S$ is a [[Convex Set]].\n\n## Proof 1\nLet $T = \\bigcap\\limits_{S \\in \\mathcal{S}}S$ and let $u, v \\in T$. Suppose $\\lambda \\in [0,1]$. Then $\\forall S \\in \\mathcal{S}$, we have that $\\lambda u + (1 - \\lambda) v \\in S$. Thus $\\lambda u + (1 - \\lambda) v \\in T$ and $T$ is a [[Convex Set]]. $\\blacksquare$\n\n## Proof 2\n[[Intersection of Structures is still a Structure]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Inverting-Right-Continuous-Inverses-does-not-decrease":{"title":"Untitled Page","content":"# Statement\nSuppose $F: \\mathbb{R} \\to \\mathbb{R}$ is a [[Distribution Function]] and denote $F^{\\rightarrow}$ to be its [[Right Continuous Inverse]]. Then $\\forall x \\in \\mathbb{R}$:\n\n$$F(F^{\\rightarrow}(x)) \\geq x$$\n## Proof\n[[TODO]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Konigs-Theorem":{"title":"Untitled Page","content":"# Statement\nSuppose $G = (V, E)$ is a [[Bipartite Graph]]. Then the [[Vertex Cover and Matching Duality]] is strict. That is, $\\tau(G) = \\nu(G)$.\n## Proof\n[[TODO]] - See [[Cook - Combinatorial Optimization]], early on, but I forget what page\n\n# Sources\n* [[Cook - Combinatorial Optimization]]\n* Original source? [[TODO]]\n\n# Other Outlinks\n* [[Maximum Matching]]\n* [[Vertex Cover]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Krickeburg-Decomposition":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/L-Plus-Functions":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\mathcal{M}, \\mu)$ be a [[Measure Space]]. Then the [[Set]] of [[L-Plus Functions]] is defined as\n\n$$L^{+} = \\{f:  X \\to [0, \\infty]: f \\text{ is }\\mathcal{M}\\text{-measureable}\\}$$\n# Other Outlinks\n* [[Extended Real Numbers]]\n* [[Borel Sigma Algebra]]\n* [[Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/L1-Integrable-Functions":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Least-Upper-Bound-Property":{"title":"Untitled Page","content":"See [[Complete Ordering]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Lebesgue-Measure":{"title":"Untitled Page","content":"# Definition\nThe [[Lebesgue-Stieltjes Measure]] with [[Distribution Function]] $\\text{id}_\\mathbb{R}$.\n\n# Other Outlinks\n* [[Correspondence of Lebesgue-Stieltjes Measures and Distribution Functions]]\n* [[Identity Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Lebesgue-Stieltjes-Measure":{"title":"Untitled Page","content":"# Definition\nA [[Lebesgue-Stieltjes Measure]] is a [[Borel Measure]] $\\mu$ on $\\mathcal{B}(\\mathbb{R})$ s.t. $\\mu(I) \u003c \\infty$ for all [[Bounded Set|bounded]] [[Interval]]s of $\\mathbb{R}$.\n\n# Properties\n1. [[Correspondence of Lebesgue-Stieltjes Measures and Distribution Functions]]\n\n# Encounters\n1. [[Folland - Real Analysis]] - pg 35\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Borel Sigma Algebra]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Left-Continuous":{"title":"Untitled Page","content":"# Definition\nA [[Function]] $f: \\mathbb{R} \\to M$ for [[Metric Space]] at $(M, d)$ is [[Left-Continuous]] at $x \\in \\mathbb{R}$ if its [[Left Function Limit]] exists and, \n$$\\lim_{y \\uparrow x} f(y) = f(x)$$\nWe call $f$ [[Left-Continuous]] if it is [[Left-Continuous]] for all $x \\in \\mathbb{R}$.\n\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Left-Continuous-Inverse":{"title":"Untitled Page","content":"# Definition\nLet $F: \\mathbb{R} \\to \\mathbb{R}$ be a [[Distribution Function]]. The [[Left Continuous Inverse]] of $F$ is defined\n\n$$F^{\\leftarrow}(x) = \\inf\\{s \\in \\mathbb{R} : F(s) \\geq x\\}$$\n\n# Properties\n1. For fixed $x \\in \\mathbb{R}$, write $A = \\{s \\in \\mathbb{R} : F(s) \\geq x\\}$. Then $A$ is [[Closed]]. Thus, $F^{\\leftarrow}(x) \\in A$ when $F^{\\leftarrow}(x) \\in \\mathbb{R}$ since [[There Exists a Sequence Converging to Extremum]].\n\n\t**Proof**: Recall that [[A Set is Closed in a Metric Space iff it contains all its Sequential Limits]]. Let $(s_{n}) \\subset A$ be a [[Sequence]]that [[Sequence Convergence|converges]] to $t \\in \\mathbb{R}$.  By definition of $A$\n\t$$F(s_{n}) \\geq x$$\n\tRecall that [[Every Sequence on the Reals contains a Monotone Subsequence]]. Let this [[Subsequence]] be $(s_{n_{k}})$. [[Every Subsequence of a Convergent Sequence converges to the same Limit]], so $s_{n_{k}} \\to t$. Now consider the following two cases:\n\t1. $(s_{n_{k}})$ is [[Non-Decreasing]]: Then, because [[Monotone Sequences converge to their Extremum]], we know $s_{n_{k}} \\to \\sup\\{s_{n_{k}}\\} = t$. Thus $t \\geq s_{n_{1}} \\in A$ (by definition of [[Supremum]]). Since $F$ is [[Non-Decreasing]], we have that $$F(t) \\geq F(s_{n_{1}}) \\geq x$$ and $t \\in A$.\n\t2. $(s_{n_{k}})$ is [[Non-Increasing]]: Then we know $s_{n_{k}} \\to \\inf\\{s_{n_{k}}\\} = t$. Since $F$ is [[Right-Continuous]] and $(s_{n_{k}}) \\subset [t, \\infty)$, we know that $F(s_{n_{k}}) \\to F(t)$. Since $F(s_{n_{k}}) \\geq x$ $\\forall k \\in \\mathbb{N}$, then by the [[Order Limit Theorem]]$$F(t) \\geq x$$ and thus $t \\in A$.\n\t\n\tSince $(s_{n})$ was arbitrary, we have that $A$ is [[Closed]]. $\\blacksquare$\n2. $F^{\\leftarrow}$ is [[Non-Decreasing]].\n\n\t**Proof**: Let $A_{x}= \\{s \\in \\mathbb{R} : F(s) \\geq x\\}$. Suppose $y \\leq x$. Then if $s \\in A_x$, we have that\n\t$$F(s) \\geq x \\geq y$$ and thus $s \\in A_{y}$. Therefore $A_{x} \\subset A_{y}$. Since [[Infimums are Non-Increasing Set Functions]] we see that $F^{\\leftarrow}(x) = \\inf A_{x} \\geq \\inf A_{y} = F^{\\leftarrow}(y)$, proving that $F^{\\leftarrow}$ is [[Non-Decreasing]]. $\\blacksquare$\n\t\n3. $F^{\\leftarrow}$ is [[Left-Continuous]].\n\n\t**Proof**: Let $x \\in \\mathbb{R}$. Let $A_{x}$ be defined as in (2). First we claim that\n\t$$\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} = A_{x}$$\n\tTo see this, first observe that since $A_{x} \\subset A_{x-\\epsilon}$ $\\forall \\epsilon \u003e 0$ (as shown in (2)), we see that\n\t$$\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} \\supset A_{x}$$\n\tOn the other hand, suppose $s \\in A_{x - \\epsilon}$ for all $\\epsilon \u003e 0$. Then $F(s) \\geq x - \\epsilon$ for all $\\epsilon \u003e 0$. By the [[Epsilon Principle]], we have that $F(s) \\geq x$ and $s \\in A_{x}$. Thus\n\t$$\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} \\subset A_{x}$$\n\tPutting those two directions together we get that \n\t$$\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} = A_{x}$$\n\tso their [[Infimum]]s are also the same. Now we claim that\n\t$$\\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} = \\lim\\limits_{\\epsilon \\downarrow 0}\\inf A_{x - \\epsilon}$$\n\tTo see this, first recall from (2) that $\\inf A_{x-\\epsilon} \\leq \\inf A_{x}$ for all $\\epsilon \u003e 0$ . Thus, by the [[Order Limit Theorem]], \n\t$$\\lim\\limits_{\\epsilon \\downarrow  0} \\inf A_{x-\\epsilon} \\leq \\inf A_{x} = \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$$\n\tNow on the other hand, if \n\t$$\\lim\\limits_{\\epsilon \\downarrow  0} \\inf A_{x-\\epsilon} \u003c \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$$\n\tThen there exists $y \\in \\mathbb{R}$ s.t. \n\t$$\\lim\\limits_{\\epsilon \\downarrow  0} \\inf A_{x-\\epsilon} \u003c y \u003c \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$$\n\tSince [[Infimums are Non-Increasing Set Functions]], we have that $y \u003e \\inf A_{x-\\epsilon}$ for all $\\epsilon \u003e 0$ (otherwise the limit would exceed $y$ and never return to fall below $y$). However, since $y \u003c \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$, there exists $z \\in \\mathbb{R}$ s.t. $y \u003c z \u003c \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$ and by definition of [[Infimum]], $z \u003c \\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon}$. Thus there must be some $\\epsilon \u003e 0$ s.t. $z \u003c A_{x - \\epsilon}$ (otherwise, there is always an element at or below $z$; recall $A_{x-\\epsilon}$ is a [[Non-Increasing]] [[Sequence]] of [[Set]]s). But then $y \u003c z \\leq \\inf A_{x - \\epsilon}$, contradicting above. Therefore\n\t$$F^{\\leftarrow}(x) = \\inf A_{x} = \\inf\\bigcap_{\\epsilon\u003e0} A_{x - \\epsilon} = \\lim\\limits_{\\epsilon \\downarrow 0}\\inf A_{x - \\epsilon} = \\lim\\limits_{y \\uparrow x} F^{\\leftarrow}(y)$$\n\tand $F^{\\leftarrow}$ is [[Left-Continuous]]. $\\blacksquare$\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Set Intersection]]\n\n# Encounters\n1. [[Resnick - A Probability Path]] - Ch unknown, pg unknown (I think ch 2)\n\t","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Left-Function-Limit":{"title":"Untitled Page","content":"# Definition\nLet $f: \\mathbb{R} \\to M$ be a [[Function]] where $(M, d)$ is a [[Metric Space]] and let $x \\in \\mathbb{R}$. Suppose there exists $y \\in M$ s.t. \n$$\\lim\\limits_{u \\to x} f{\\big|}_{(-\\infty, x]}(u) = y$$ where $f {\\big|}_{(-\\infty, x]}$ is the [[Function Restriction]] of $f$ to $(-\\infty, x]$. Then we call $y$ the [[Left Function Limit]] of $f$ at $x$. We denote it $$\\lim_{u \\uparrow x} f(u)$$\n# Remarks\n1. 1. Applying the [[Heine Criterion]] to $(-\\infty, x]$ gives us the connection between [[Sequence]]s in $(-\\infty, x]$ converging to $x$ and the [[Left Function Limit]] at $x$.\n# Other Outlinks\n* [[Real Numbers]]\n* [[Function Limit]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Left-and-Right-Inverses-in-an-Abelian-Group-are-the-Unique-Inverse":{"title":"Untitled Page","content":"# Statement\nSuppose $G$ is an [[Abelian Group]] and $u, v \\in G$. Further suppose $v$ is a [[Left Inverse]]/[[Right Inverse]] of $u$. Then $v$ is the unique inverse of $u$.\n\n## Proof\nBecause $G$ is an [[Abelian Group]], we have by [[Commutativity]] that $vu = uv$. One of these will be $e$ by assumption, thus the other must be as well. Therefore $v$ is an inverse of $u$. Because [[The inverse in a Group is unique]], we have uniqueness. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Likelihood-Soft-Classifier":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. The [[Likelihood Soft Classifier]] is a [[Soft Classifier]] $g: \\mathcal{D} \\to [0,1]^{n}$ so that for $x \\in \\mathcal{D}$\n\n$$g(x) = (\\mathbb{P}(Y=y|X=x))_{y=1}^{n}$$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Limit-Point":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Let $S \\subset X$. Then $x \\in X$ is a [[Limit Point]] of $S$ if $\\forall U \\in \\tau$ such that $x \\in U$, we have that $U$ and $S$ are not [[Mutually Disjoint]]. That is, $U \\cap S \\neq \\emptyset$.\n\nWe denote\n$$\\bar{S} := \\bigcup\\limits \\{x \\in X : x \\text{ is a limit point of } S\\}$$\n\n# Other Outlinks\n - [[Set Union]]\n - [[Open]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Limit-Points-of-a-subset-are-Limit-Points-of-the-original-Set":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[Topological Space]] and let $T \\subset S \\subset X$. Then if $x \\in X$ is a [[Limit Point]] of $T$, it is also a [[Limit Point]] of $S$.\n\n## Proof\nSuppose $U \\subset X$ [[Open]] so that $x \\in U$. Then $U \\cap S \\supset U \\cap T \\neq \\emptyset$ and $x$ is a [[Limit Point]] of $S$. $\\blacksquare$\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Line":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and $x_{1}, x_{2} \\in V$. The [[Line]] $L \\subset V$ containing $x_{1}$ and $x_{2}$ is the [[Set]] $$L = \\{\\lambda x_{1} + (1 - \\lambda) x_{2} : \\lambda \\in \\mathbb{R}\\}$$\n## Remarks\n1. $L$ can also be expressed as $L = \\{x_{2} + \\lambda (x_{1} - x_{2}) : \\lambda \\in \\mathbb{R} \\}$. This follows from the observation that $$\\lambda x_{1} + (1 - \\lambda) x_{2} = x_{2} + \\lambda x_{1} - \\lambda x_{2} = x_{2} + \\lambda (x_{1} - x_{2})$$\n# Other Outlinks\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Linear-Function":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Linear-Matrix-Inequality":{"title":"Untitled Page","content":"# Definition\n[[TODO]] see [[Boyd - Convex Optimization]] page 38","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Linear-Program":{"title":"Untitled Page","content":"[[TODO]] Build down deeper into general constrained optimization programs\n# Definition\nSuppose $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$. Then the [[Constrained Optimization Program]] \n\n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\geq 0\n\\end{align*}$$\n\n## Properties\n1. [[The Feasible Set of a Linear Program is a Convex Polytope]].\n2. [[TODO]]:\n\t- Show equivalence between different forms\n\t- [[Dual Linear Program]]\n3. [[Linear Program|Linear Programming]] is in [[Polynomial Time Solvable|P]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/MCS-521-Combinatorial-Optimization":{"title":"Untitled Page","content":"* [[MCS 521 Research Paper Project]] ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/MCS-521-Research-Paper-Project":{"title":"Untitled Page","content":"For this project I will be studying [[Maximum Entropy Gaussian Approximations for the Number of Integer Points and Volumes of Polytopes]]\n\n1. [[Research Paper Project - Notes on Introduction]]\n2. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Masters-Thesis":{"title":"Untitled Page","content":"* [[Masters Thesis Outline]]\n\nDate: [[2022-03-23]]\n\nI have settled on doing [[Branching Brownian Motion]] for the \"go beyond the textbook\" part. One topic here is on extremal points. This appears to be an interesting topic. There is the whole [[FKPP Equation]] piece of the picture I would like to understand. It'll be good to get an outline of the thesis to [[Will Perkins|Will]] as soon as possible. Preferably by [[2022-03-27]] EOD. This may be tough since I'll also be in SF seeing friends. I felt the need to compensate being late on this by giving Will more than he asked for, but that is not effective. I came to the understanding that\n1. Be transparent about where I'm at.\n2. Don't try to alleviate [[Anxiety]] by \"over-preparation\"\n\nThis is the way to ensure success.\n\nI think I will likely have to push into the summer. I'll reach out to [[Julius Ross|Dr. Ross]] monday when I have the outline. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Masters-Thesis-Outline":{"title":"Untitled Page","content":"Date: [[2022-03-28]]\n\n1. Basic Definitions\n\t1. [[Probability Space]], [[Random Variable]]\n\t2. [[Radon-Nikodym Derivative]]s, [[Conditional Expectation]]\n\t3. [[Stochastic Process]]\n\t4. [[Martingale]]\n\t5. [[Brownian Motion]]\n\t6. **[[Branching Brownian Motion]]**\n2. Decomposition and Convergence of Martingales\n\t1. [[Doob Decomposition]] of [[Submartingale]]s\n\t2. [[Convergence of Positive Supermartingales]]\n\t3. [[Krickeburg Decomposition]]\n\t4. [[Doob Submartingale Convergence]]\n3. Application of [[Branching Brownian Motion]] to the [[Fisher-Kolmogorov-Petrovskii-Piskunov Equation]] - Uses [Mckean](https://drive.google.com/file/d/1mZIIjHDZzHVu6EVvYIwuA5Y1076PaXGA/view?usp=sharing) and [Bramson](https://drive.google.com/file/d/1JIsZEfikTjl0Wuw7BMFLTywVef-bvesF/view?usp=sharing)\n\t1. [[Fisher-Kolmogorov-Petrovskii-Piskunov Equation]]\n\t2. [[Feynman-Kac Equation]]\n\t3. [[Kolmogorov-Petrovskii-Piskunov Theorem]]\n4. Convergence of the Centered Maximum - Uses [Lalley and Selke](https://drive.google.com/file/d/10ogmoiapx68Kbj-UDocysmfqf6EmicCN/view?usp=sharing)\n\t1. Derivative Martingale\n\t2. Convergence of Derivative Martingale\n\t3. Lalley and Selke's Result\n5. Extensions to R^n\n\nPrimary Papers:\n1. [Bramson 1978 - Maximal displacement of branching brownian motion](https://drive.google.com/file/d/1V1s-8OCnIZRrBWi6PgBzuzlAmV8TLYpK/view?usp=sharing)\n2. [McKean 1975 - Application of brownian motion to the equation of kolmogorov-petrovskii-piskunov](https://drive.google.com/file/d/1mZIIjHDZzHVu6EVvYIwuA5Y1076PaXGA/view?usp=sharing)\n3. [Bramson 1983 - Convergence of solutions of the Kolmogorov equation to travelling waves](https://drive.google.com/file/d/1JIsZEfikTjl0Wuw7BMFLTywVef-bvesF/view?usp=sharing)\n4. [Lalley and Selke 1987 - A Conditional Limit Theorem for the Frontier of a Branching Brownian Motion](https://drive.google.com/file/d/10ogmoiapx68Kbj-UDocysmfqf6EmicCN/view?usp=sharing)\n\nExtensions to R^n\n1. [2015 - Derivative martingale of the branching Brownian motion in dimension d≥1](https://drive.google.com/file/d/1A51gpOHjAxvbfL09YzvApxOzrYZAowsB/view?usp=sharing)\n2. [Kim 2021 - The maximum of branching Brownian motion in R^d](https://drive.google.com/file/d/1Aj8wEgRcTA_TWo_iRXx6wZfLzkuq--ms/view?usp=sharing)\n\nUseful Secondary Sources\n1. [Berestycki 2014 - Topics on Branching Brownian Motion](https://drive.google.com/file/d/1FTAuCZ5s6xqHbaYxuj8iovCbILwcz93g/view?usp=sharing)\n2. [Shi 2015 - Branching Brownian Motion and the Spinal Decomposition](https://drive.google.com/file/d/1Hg-rsgblfdJcFoW0TfLK0aKYO8r_GjlW/view?usp=sharing)\n\nDate: [[2022-03-25]]\n\nI want to talk about [[Extremal Points of Brownian Motion]]\n\n1. Basic Definitions\n\t1. \n\nDate: [[2022-03-13]]\n\n1. Basic Definitions\n\t1. [[Probability Space]], [[Random Variable]]\n\t2. [[Radon-Nikodym Derivative]]s, [[Conditional Expectation]]\n\t3. [[Stochastic Process]]\n\t4. [[Martingale]], [[Submartingale]], [[Supermartingale]], [[Difference Sequence]]s\n2. Typical Examples of Martingales\n\t1. Increasing [[Sigma Algebra]]s and [[Smoothing]]\n\t2. Sums of [[Independence|Independent centered random variables]]\n\t3. [[Simple Branching Process]]\n\t4. ...more?\n3. Decomposition and Convergence of Martingales\n\t1. [[Doob Decomposition]] of [[Submartingale]]s\n\t2. [[Convergence of Positive Supermartingales]]\n\t3. [[Krickeburg Decomposition]]\n\t4. [[Doob Submartingale Convergence]]\n4. Typical Examples revisited\n5. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching":{"title":"Untitled Page","content":"# Definition\nLet $G = (V, E)$ be an [[Undirected Graph]]. A [[Matching]] is a [[Set]] $M \\subset E$ so that $\\forall u \\in V$, $|\\{e \\in M : u \\cap e \\neq \\emptyset\\}| \\leq 1$.\n\n## Examples\n1. For any [[Undirected Graph]] $G$, $\\emptyset$ is a [[Matching]]. Indeed for all $u \\in V(G)$, $|\\{e \\in \\emptyset : u \\cap e \\neq \\emptyset\\}| = 0 \\leq 1$.\n3. [[TODO]]\n\n## Terminology\n* [[Matching Alternating Path]]\n* [[Matching Augmenting Path]]\n\n# Other Outlinks\n* [[Set Cardinality]]\n* [[Set Intersection]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching-Alternating-Path":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is an [[Undirected Graph]] and $M \\subset E(G)$ is a [[Matching]] on $G$. A [[Graph Path]] $P$ on $G$ from [[Graph Vertex|vertex]] $u \\in V(G)$ to $v \\in V(G)$ of length $n \\in \\mathbb{N}$ is an $M$-[[Matching Alternating Path|alternating path]] if its [[Graph Edge|edges]] are alternately in $P$. That is, $P$ is such that\n1. $\\forall i \\in [n-1]$, if $e_{i} \\in M$ then $e_{i+1} \\not\\in M$,\n2. $\\forall i \\in [n-1]$, if $e_{i} \\not\\in M$ then $e_{i+1} \\in M$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching-Alternating-Tree":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is an [[Undirected Graph]] and $M$ is a [[Matching]] on $G$. Suppose $(T, r)$ is a [[Rooted Tree]] [[Subgraph]] of $G$. Then $(T, r)$ is an $M$-[[Matching Alternating Tree]] if\n1. $r$ is [[Matching Exposed|exposed]] by $M$,\n2. $\\forall v \\in T$, the [[Graph Path|Path]] from $v$ to $r$ on $T$ is an $M$-[[Matching Alternating Path|alternating path]].\n\n## Remarks\n1. We have two special [[Set]]s $A(T)$ and $B(T)$ associated with an $M$-[[Matching Alternating Tree|alternating tree]] defined as follows\n\t1. $A(T) = \\{v \\in T : d(r, v) \\text{ is odd}\\}$\n\t2. $B(T) = \\{v \\in T : d(r, v) \\text{ is even}\\}$\n2. $|B(T)| = |A(T)| + 1$ since every $v \\in T \\setminus \\{r\\}$ is $M$-[[Matching Covered]], so we can pair up all [[Graph Vertex|vertices]] in $B(T)$ except $r$ with one in $A(T)$.\n\n# Sources\n* [[Cook - Combinatorial Optimization]] - Ch 5, Section 5.2, pg 135\n\n# Other Outlinks\n* [[Odd Integer]]\n* [[Even Integer]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching-Augmenting-Path":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is an [[Undirected Graph]] and $M \\subset E(G)$ is a [[Matching]] on $G$. Suppose $P$ is an $M$-[[Matching Alternating Path|Alternating Path]] on $G$ of length $n \\in \\mathbb{N}$ from $u \\in V(G)$ to $v \\in V(G)$. Then $P$ is an $M$-[[Matching Augmenting Path|augmenting path]] if $u$ and $v$ are both $M$-[[Matching Exposed|exposed]].\n\n## Remarks\n1. [[Augmenting Network Flow Paths for Bipartite Matching Flow Formulation are Matching Augmenting Paths]]\n2. If $P = (e_{1}, \\dots, e_{n})$ is an $M$-[[Matching Augmenting Path|augmenting path]], then $e_{1}, e_{n} \\not\\in M$. If $e_{1}$ was, then $u$ would be [[Matching Covered|covered]] by $M$. If $e_{n}$ was, then $v$ would be [[Matching Covered|covered]] by $M$. Thus $n$ is [[Odd Integer|odd]] and only $e_{2i} \\in M$ for $i \\in [\\frac{n-1}{2}]$.\n\n# Other Outlinks\n* [[Natural Numbers]]\n* [[Graph Edge]]\n* [[Graph Vertex]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching-Covered":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $M \\subset E(G)$ be a [[Matching]] on $G$. we say that $v \\in V(G)$ is [[Matching Covered|covered]] by $M$ if $$|\\{e \\in M : u \\cap e \\neq \\emptyset\\}| = 1.$$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Matching-Exposed":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $M \\subset E(G)$ be a [[Matching]] on $G$. we say that $v \\in V(G)$ is [[Matching Exposed|exposed]] by $M$ if $v$ is not [[Matching Covered|covered]] by $M$. That is, $$|\\{e \\in M : v \\cap e \\neq \\emptyset\\}| = 0.$$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Maximum-Entropy-Gaussian-Approximations-for-the-Number-of-Integer-Points-and-Volumes-of-Polytopes":{"title":"Untitled Page","content":"![hello](attachments/barvinok.combinatorics.maximum-entropy-gaussian-approximations-for-the-number-of-integer-points-and-volumes-of-polytopes.pdf)\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Maximum-Matching":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $\\mathcal{M} = \\{M \\subset E(G) : M \\text{ is a Matching}\\}$. Then a [[Maximum Matching]] is a [[Matching]] $M \\in \\mathcal{M}$  so that $|M| = \\max \\{|M| \\leq |E(G)| : M \\in \\mathcal{M}\\}$. \n\n## Remarks\n1. Note this definition is [[Well-Defined]] even for [[Infinite Set]]s because [[Cardinals form a Well-Ordering]].\n2. The [[Set Cardinality]] of a [[Maximum Matching]] on $G$ is denoted $\\nu(G)$.\n3. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Maximum-Matching-Problem":{"title":"Untitled Page","content":"# Definition\nThe [[Maximum Matching Problem]] is an [[Optimization Problem]] that asks, given an [[Undirected Graph]] $G$ to find a [[Maximum Matching]] on $G$. ","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Maximums-are-Non-Decreasing-Set-Functions":{"title":"Untitled Page","content":"# Statement\nLet $(T, \\leq)$ be a [[Total Ordering]] and suppose $A \\subset B \\subset T$ such that $\\max A$ and $\\max B$ both exist. Then $\\max A \\leq \\max B$.\n\n# Proof\nLet $m = \\max B$. Then $m \\geq b$ for all $b \\in B$. Since $A \\subset B$, we have that $m \\geq a$ for all $a \\in A$. By definition [[Maximum]], $\\max A \\in A$, so $$\\max B = m \\geq \\max A$$ $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Measureable-Function":{"title":"Untitled Page","content":"# Definition\nLet $(X, \\mathcal{M})$ and $(Y, \\mathcal{N})$ be [[Measure Space]]s. Let $f: X \\to Y$ be a [[Function]]. Then $f$ is a [[Measureable Function]] if\n$$\\sigma(f) \\subset \\mathcal{M}$$\n\n# Other Outlinks\n* [[Sigma Algebra induced by Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Metric-Topology":{"title":"Untitled Page","content":"# Definition\nLet $(M, d)$ be a [[Metric Space]]. The [[Metric Topology]] on $M$ is the [[Topological Space]] generated by the [[Topological Basis]]\n\n$$\\mathcal{B} = \\{B_{\\epsilon}(x) \\subset M: x \\in M, \\epsilon \u003e 0\\}$$\nwhere $B_{\\epsilon}(x) = \\{y \\in X : d(y, x) \u003c \\epsilon\\}$ ([[Open Ball]]s on $M$).\n\n## Remarks\n1. We should verify that $\\mathcal{B}$ is indeed a [[Topological Basis]]:\n\t\n\t**Proof**: First observe that $\\forall x \\in M$, $x \\in B_{1}(x) \\in \\mathcal{B}$. Next suppose $B_{\\epsilon}(x), B_{\\delta}(y) \\in \\mathcal{B}$ for $\\epsilon, \\delta \u003e 0$ and $x, y \\in M$. Suppose $z \\in B_{\\epsilon}(x) \\cap B_{\\delta}(y)$. Because [[Finite Intersections of Open Balls contain Open Balls about each point]], we see there exists $\\zeta \u003e 0$ so that \n\t$B_{\\zeta}(z) \\subset B_{\\epsilon}(x) \\cap B_{\\delta}(y)$. Therefore $\\mathcal{B}$ is a valid [[Topological Basis]]. $\\blacksquare$\n1. [[Every Open Set in the Metric Topology contains a Ball around each Point]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Minimal-Face":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$ and let $P \\subset V$ be a [[Convex Polytope]]. A [[Minimal Face]] of $P$ is a [[Convex Polytope Face|face]] of $P$ that does not contain any other [[Proper Face]] of $P$.\n\n## Properties\n1. [[Characterization of Minimal Faces]]\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Minimizing-Binary-Cross-Entropy":{"title":"Untitled Page","content":"Dates: [[2022-02-17]], [[2022-02-20]], [[2022-02-21]]\n\n\nConsider the following setup:\n\n1. [[Probability Space]] $(\\Omega, \\mathcal{F}, \\mathbb{P})$ \n2. [[Random Vector]] $U: \\Omega \\to \\mathbb{F}_{2}^{n}$ for $n \\in \\mathbb{N}$. This represents the input sequence. For shorthand $U_{i}=\\pi_{i}\\circ U$, where $\\pi_i$ is the $i$th projection map for $i \\in [n]$.\n3. Measureable Functions \n\t1. Encoder $f: \\mathbb{F}^{n}_{2} \\to M$ where $(M, d)$ is a metric space.\n\t2. Noise Function $g: M \\times \\Omega \\to M$ \n\t3. Decoder $h: M \\to \\mathbb{F}^{n}_{2}$\n4. Denote\n\t1. $F = f \\circ U$\n\t2. $G(\\omega) = g(F(\\omega), \\omega)$\n\t3.  $H = h \\circ G$\n\n**Definition 1**: The [[Bit Error Rate]] of $(f,g,h)$ is\n$$\n\\begin{align*}\n\\mathbb{B}(f,g,h) \u0026= \\mathbb{E}\\left[\\frac{1}{n} \\sum\\limits_{i=1}^{n}1[U_{i}\\neq H_{i}]\\right]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[1[U_{i}\\neq H_{i}]]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{P}(U_{i}\\neq H_{i})\n\\end{align*}\n$$\n**Theorem 2**: The decoder function\n$$h_{i}(y)= \\begin{cases}\n1 \u0026 \\mathbb{P}(U_{i}=1|G=y)\u003e \\mathbb{P}(U_{i}=0|G=y) \\\\\n0 \u0026 \\text{otherwise}\n\\end{cases}$$\nis the unique (up to modifications on $G_{*}(\\mathbb{P})$-null sets and measureable subsets of $E = \\{y: \\mathbb{P}(U_{i}=1|G=y) = \\mathbb{P}(U_{i}=0|G=y)\\}$) decoder function that minimizes bit error rate. It is known as the [[Bayes Classifier]]. $G_{*}(\\mathbb{P})$ is the (pushforward) probability measure on $M$ induced by $G$.\n\n**Proof**: Let $h$ be some decoder function. We decompose the expression for bit error rate:\n$$\\begin{align*}\n\\mathbb{B}(f,g,h) \u0026= \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{P}(U_{i}\\neq H_{i})\\\\\n\u0026= \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{\\Omega}\\mathbb{P}(U_{i}\\neq h_{i}(G)|G)d \\mathbb{P}(\\omega)\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{\\Omega}\\mathbb{E}(1[U_{i}=1]1[h_{i}(G)=0] + 1[U_{i}=0]1[h_{i}(G)=1]|G)d \\mathbb{P}(\\omega)\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{\\Omega}1[h_{i}(G)=0]\\mathbb{E}(1[U_{i}=1]|G) + 1[h_{i}(G)=1]\\mathbb{E}(1[U_{i}=0]|G)d \\mathbb{P}(\\omega)\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{h_i(G)=0}\\mathbb{P}(U_{i}=1|G)d \\mathbb{P}(\\omega) + \\int_{h_{i}(G)=1}\\mathbb{P}(U_{i}=0|G)d \\mathbb{P}(\\omega)\n\\end{align*}$$\n\nObserve that when we let $h_{i}$ be defined as in the statement of the theorem, we get \n$$\\begin{align*}\n\\mathbb{B}(f,g,h)\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{\\mathbb{P}(U_{i}=1|G)\\leq \\mathbb{P}(U_{i}=0|G)}\\mathbb{P}(U_{i}=1|G)d \\mathbb{P}(\\omega) + \\int_{\\mathbb{P}(U_{i}=1|G) \u003e \\mathbb{P}(U_{i}=0|G)}\\mathbb{P}(U_{i}=0|G)d \\mathbb{P}(\\omega)\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\int_{\\Omega}\\text{min}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))d \\mathbb{P}(\\omega)\\\\\n\u0026= \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[\\text{min}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))]\n\\end{align*}$$\nNow suppose we have another decoder $h'$ s.t. $h_i \\neq h_i'$ on measureable $S \\subset M$ for some $i \\in [n]$. Observe that on $E$, every decoder performs with the same BER (i.e. $\\frac{1}{2}\\mathbb{P}(E)$) and modification of $h_i$ on any $G_{*}(\\mathbb{P})$ null set does not change the above expectation. Thus, for the sake of uniqueness, we only consider $h'$ s.t. $S \\backslash E$ is not $G_{*}(\\mathbb{P})$ null, so WLOG redefine $S := S \\backslash E$. Then\n$$\\begin{align*}\n\u0026\\int_{\\Omega}\\mathbb{P}(U_{i}\\neq h'_{i}(G)|G)d \\mathbb{P}(\\omega)\\\\\n\u0026= \\int_{S^{C}}\\text{min}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))d \\mathbb{P}(\\omega) + \\int_{S}\\text{max}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))d \\mathbb{P}(\\omega)\n\\end{align*}$$\nBy definition of $S$ we know that \n$$\\text{max}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G)) \u003e \\text{min}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))$$\nSince $S$ is not null and because probabilities are non-negative, we must have\n$$\\int_{S}\\text{max}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))d \\mathbb{P}(\\omega) \u003e \\int_{S}\\text{min}(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_{i}=0|G))d \\mathbb{P}(\\omega)$$\nand therefore\n$$\\int_{\\Omega}\\mathbb{P}(U_{i}\\neq h'_{i}(G)|G)d \\mathbb{P}(\\omega) \u003e \\int_{\\Omega}\\mathbb{P}(U_{i}\\neq h_{i}(G)|G)d \\mathbb{P}(\\omega)$$\nSince this holds for some $i \\in [n]$ and nonstrict inequalities hold for the others, we have that\n$$\\mathbb{B}(f,g,h) \u003c \\mathbb{B}(f,g,h')$$\n$\\blacksquare$\n\nLet $q: M \\to [0, 1]$ be a measureable function and denote it our soft-decoding function. Define $Q = q \\circ G$.\n\n**Definition 3**: The [[Binary Cross-Entropy]] of $(f,g,q)$ is:\n$$\\begin{align*}\n\\mathbb{C}(f,g,q) \u0026= \\mathbb{E}\\left[\\frac{1}{n}\\sum\\limits_{i=1}^{n}-U_{i}\\text{lg}Q_{i} - (1 - U_{i})\\text{lg}(1-Q_{i})\\right]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[-U_{i}\\text{lg}Q_{i} - (1 - U_{i})\\text{lg}(1-Q_{i})]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[\\mathbb{E}[-U_{i}\\text{lg}(q_{i}(G)) - (1 - U_{i})\\text{lg}(1-q_{i}(G))|G]]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[-\\mathbb{E}[1[U_{i}=1]\\text{lg}(q_{i}(G))|G] - \\mathbb{E}[1[U_{i}=0]\\text{lg}(1-q_{i}(G))|G]]\n\\end{align*}$$\nSince $\\text{lg}(q_i(G))$ and $\\text{lg}(1 - q_i(G))$ are both $\\sigma(G)$-measureable, we have that\n$$\\begin{align*}\n\\mathbb{C}(f,g,q) \n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[-\\text{lg}(q_{i}(G))\\mathbb{E}[1[U_{i}=1]|G] - \\text{lg}(1-q_{i}(G))\\mathbb{E}[1[U_{i}=0]|G]]\u0026\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[-\\text{lg}(q_{i}(G))\\mathbb{P}[U_{i}=1|G] - \\text{lg}(1-q_{i}(G))\\mathbb{P}[U_{i}=0|G]]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\Big[-\\text{lg}(\\frac{q_{i}(G)}{\\mathbb{P}[U_{i}=1|G]}\\mathbb{P}[U_{i}=1|G])\\mathbb{P}[U_{i}=1|G] - \\text{lg}(\\frac{1-q_{i}(G)}{\\mathbb{P}[U_{i}=0|G]}\\mathbb{P}[U_{i}=0|G])\\mathbb{P}[U_{i}=0|G]\\Big]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\Big[\\Big( -\\text{lg}\\left( \\frac{q_{i}(G)}{\\mathbb{P}[U_{i}=1|G]} \\right) - \\text{lg}(\\mathbb{P}[U_{i}=1|G]) \\Big)\\mathbb{P}[U_{i}=1|G] \\\\\n\u0026+ \\Big( -\\text{lg}\\left( \\frac{1-q_{i}(G)}{\\mathbb{P}[U_{i}=0|G]} \\right) - \\text{lg}(\\mathbb{P}[U_{i}=0|G]) \\Big)\\mathbb{P}[U_{i}=0|G]\\Big]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\Big[-\\text{lg}\\left( \\frac{q_{i}(G)}{\\mathbb{P}[U_{i}=1|G]} \\right)\\mathbb{P}[U_{i}=1|G] -\\text{lg}\\left( \\frac{1-q_{i}(G)}{\\mathbb{P}[U_{i}=0|G]} \\right)\\mathbb{P}[U_{i}=0|G]\\\\\n\u0026-\\text{lg}(\\mathbb{P}[U_{i}=1|G])\\mathbb{P}[U_{i}=1|G]-\\text{lg}(\\mathbb{P}[U_{i}=0|G])\\mathbb{P}[U_{i}=0|G]\\Big]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[D_{KL}(\\mathbb{P}[U_{i}|G] || q_{i}(G)) + \\mathbb{H}_{2}(\\mathbb{P}[U_{i}|G])]\\\\\n\u0026=\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[D_{KL}(\\mathbb{P}[U_{i}|G] || q_{i}(G))] + \\mathbb{H}(U_{i}|G)\n\\end{align*}$$\n\nwhere $D_{KL}$ is the [[Kullback–Leibler Divergence]], $\\mathbb{H}_2$ is the [[Binary Entropy]] function and $\\mathbb{H}(X|Y)$ is the [[Conditional Entropy]] of random element $X$ given $Y$. Thus, our simplified definition of [[Binary Cross-Entropy]] is\n$$\n\\mathbb{C}(f,g,q) = \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}[D_{KL}(\\mathbb{P}[U_{i}|G] || q_{i}(G))] + \\mathbb{H}(U_{i}|G)\n$$\n**Theorem 4**: For a given choice of $f,g$, $\\mathbb{C}_{f,g}(q)$ has the unique minimizer ($G_{*}(\\mathbb{P})$-a.e.) $q$ defined component-wise as\n$$\nq_i= \\mathbb{P}[U_{i}=1|G]\n$$\n**Proof**: This follows from [[Gibb's Inequality]], which states for random elements $A$, $B$, \n\n$$D_{KL}(A || B) \\geq 0$$\nwith equality iff $A = B$ almost everywhere. Thus $\\forall i \\in [n]$\n$$\\mathbb{E}[D_{KL}(\\mathbb{P}[U_{i}|G] || q_{i}(G))] = 0$$\niff $q_{i}(G) = \\mathbb{P}[U_{i}=1|G]$ $G_{*}(\\mathbb{P})$-a.e.. Since $\\mathbb{H}(U_i|G)$ is not a function of $q(G)$, we know $q_{i}(G) = \\mathbb{P}[U_{i}=1|G]$ $G_{*}(\\mathbb{P})$-a.e. $\\forall i \\in [n]$ is the unique minimizer of $\\mathbb{C}_{f,g}(q)$. $\\blacksquare$\n\nThis theorem tells us two things\n\n1. For some channel, given an encoder, the optimal decoder (for crossentropy) is the one that exactly computes the posterior probabilities for each bit. This naturally gives us the Bayes' minimizer and thus the unique BER minimizer. On the other hand, it is possible that some soft decoders may give us the Bayes' minimizer but do not minimize cross-entropy. So minimizing cross-entropy is stronger than simply minimizing BER (from the decoder's perspective).\n2. Since $f,g$ determine the minimizing $h$, understanding the BCE minimizing code reduces to understanding the encoder that minimizes $\\sum\\limits_{i=1}^{n} \\mathbb{H}(U_{i}|G)$\n\n**Remark 5**: Recall that the [[Mutual Information]] between $U_{i}$ and $G$ $\\forall i \\in [n]$ can be written as\n$$I(U_{i};G)= \\mathbb{H}(U_{i}) - \\mathbb{H}(U_{i}|G)$$\nSince $\\mathbb{H}(U_{i})$ is constant wrt $f,g,h$, minimizing BCE is equivalent to maximizing\n$$\\sum\\limits_{i=1}^{n}I(U_{i};G)$$\n[[Mutual Information]] is a measure of shared information between $U_{i}$ and $G$ (aka their mutual dependence). So minimizing BCE is, in effect, maximizing the dependence between $G$ and $U_{i}$. Since we are maximizing a sum, it is unclear whether there are any tradeoffs between the components.\n\nFrom here on out, we denote $\\mathbb{B}(f,g)$ to be the bit error rate when using the [[Bayes Classifier]] decoder and $\\mathbb{C}(f,g)$ to be the cross entropy when using the posterior on $U_i$ given $G$ as the soft-decoder for each $i \\in [n]$. When we subscript either with $i \\in [n]$, we are referring to that quantity only for bit $U_{i}$.\n\nNext we show some inequalities relating [[Conditional Entropy]] with [[Bit Error Rate]].\n\n**Theorem 6**: The following inequalities hold for all choices of $f, g$:\n\n$$2\\mathbb{B}_{i}(f,g) \\leq \\mathbb{C}_{i}(f,g) \\leq \\mathbb{H}_{2}(\\mathbb{B}_{i}(f,g))$$\nIn particular\n$$2\\mathbb{B}(f,g) \\leq \\mathbb{C}(f,g) \\leq \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{H}_{2}(\\mathbb{B}_{i}(f,g))$$\n\n**Proof**: Let $i \\in [n]$ and fix $f,g$. Let $t = \\min(\\mathbb{P}(U_{i}=1|G), \\mathbb{P}(U_i=0|G))$. Then \n$$\\begin{align*}\n\u0026\\mathbb{B}_{i}(f,g) = \\mathbb{E}[t]\\\\\n\u0026\\mathbb{C}_{i}(f,g) = \\mathbb{H}(U_{i}|G) = \\mathbb{E}[\\mathbb{H}_2(t)]\n\\end{align*}$$\nRecall that [[Shannon Entropy is a Concave Function]], so all line segments connecting two points of $\\mathbb{H}_2$ in $\\left[0,\\frac{1}{2}\\right]$ lie in the undergraph of $\\mathbb{H}_2$. Since $\\mathbb{H}_{2}(0)= 0$ and $\\mathbb{H}_{2}\\left(\\frac{1}{2}\\right) = 1$, we know that $\\mathbb{H}_{2}(s) \\geq 2s$ $\\forall s \\in \\left[0, \\frac{1}{2}\\right]$. Thus, since [[Expectation is Non-Decreasing]], we have that\n\n$$\\mathbb{C}_{i}(f,g) = \\mathbb{E}[\\mathbb{H}_{2}(t)]\\geq 2\\mathbb{E}[t] = 2\\mathbb{B}_{i}(f,g)$$\nproving the LHS inequality.\n\nFor the second, note again that [[Shannon Entropy is a Concave Function]], so we can apply [[Jensen's Inequality]]. Then\n\n$$\\mathbb{C}_{i}(f,g) = \\mathbb{E}[\\mathbb{H}_{2}(t)]\\leq \\mathbb{H}_{2}(\\mathbb{E}[t]) = \\mathbb{H}_{2}(\\mathbb{B}_{i}(f,g))$$\ngiving us the RHS inequality.\n\nThe second inequality follows by averaging together the inequalities for each component. $\\blacksquare$\n\n**Remarks 7**:\n\n1. The application of [[Jensen's Inequality]] in this proof gives us a special case of [[Fano's Inequality]] when the symbol space is just 2 elements ($U_{i}$ is a binary random variable). \n2. Though this inequality bounds the [[Binary Cross-Entropy]] and relates it to the [[Bit Error Rate]], it does **not** prove that a choice of $f$ that minimizes the [[Binary Cross-Entropy]] also minimizes the [[Bit Error Rate]]. I haven't been able to find a counter-example that would show the minimizers can be different, and I think there might be a tighter relationship to be found between the two. It might be helpful to play around with specific choices of $g$ and $f$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Minimizing-Conditional-Entropy":{"title":"Untitled Page","content":"Dates: [[2022-03-03]]\n\nConsider the following setup:\n\n1. [[Probability Space]] $(\\Omega, \\mathcal{F}, \\mathbb{P})$ \n2. [[Random Vector]] $U: \\Omega \\to \\mathbb{F}_{2}^{n}$ for $n \\in \\mathbb{N}$. This represents the input sequence. For shorthand $U_{i}=\\pi_{i}\\circ U$, where $\\pi_i$ is the $i$th [[Projection Map]] for $i \\in [n]$.\n3. [[Measureable Function]]s \n\t1. Encoder $f: \\mathbb{F}^{n}_{2} \\to M$ where $(M, d)$ is a [[Metric Space]].\n\t2. Noise Function $g: M \\times \\Omega \\to M$ \n\t3. Decoder $h: M \\to \\mathbb{F}^{n}_{2}$\n4. Denote\n\t1. $F = f \\circ U$\n\t2. $G(\\omega) = g(F(\\omega), \\omega)$\n\t3.  $H = h \\circ G$\n\nWe are interested in understanding what it means to minimize the quantity $$R_{g}(f) = \\frac{1}{n}\\sum\\limits_{i =1}^{n}  \\mathbb{H}(U_{i} | G)$$\nover choices of $f$ ($g$, our noise function is fixed). First recall that we can write\n$$\\begin{align*}\n\\mathbb{H}(U_{i} | G) \u0026= \\mathbb{H}(U_{i}, G) - \\mathbb{H}(G)\\\\\n\u0026=\\mathbb{H}(U_{i}) + \\mathbb{H}(G|U_{i}) - \\mathbb{H}(G)\n\\end{align*}$$\nThis is [[Bayes Theorem for Entropy]]. One quantity, $\\mathbb{H}(U_{i})$ is just a constant with respect to $f$. In the case where $U$ is just a [[Uniform Distribution|uniform random variable]] over $\\mathbb{F}_{2}^{n}$, we get that\n$$\\begin{align*}\n\\mathbb{H}(U_{i}) \u0026= -\\mathbb{P}(U_{i}=1) \\lg(\\mathbb{P}(U_{i}=1))-\\mathbb{P}(U_{i}=0) \\lg(\\mathbb{P}(U_{i}=0))\\\\\n\u0026=-\\frac{1}{2}*-1 - \\frac{1}{2} * -1\\\\\n\u0026=1\n\\end{align*}$$\nSo minimizing $R_{g}(f)$ is equivalent to minimizing\n$$\\begin{align*}\nL_{g}(f) \u0026=\\frac{1}{n}\\sum\\limits_{i =1}^{n} \\Big[ \\mathbb{H}(G|U_{i}) - \\mathbb{H}(G) \\Big]\\\\\n\u0026=-\\mathbb{H}(G) + \\frac{1}{n}\\sum\\limits_{i =1}^{n} \\mathbb{H}(G|U_{i})\n\\end{align*}$$\nLet's first simply things:\n1. $M = \\mathbb{R}^{k}$ with [[Euclidean Distance]] for some $k \\in \\mathbb{N}$. So now $f$ maps input sequences to [[Real Numbers|real]] vectors and $g$ introduces noise to those vectors.\n2. $U$ is a uniform random variable over $\\mathbb{F}_{2}^{n}$. So $\\mathbb{P}(U_{i}=b) = \\frac{1}{2}$ for $b \\in \\{0,1\\}$.\n3. Let $p^{(i)}_{b, G}$ denote the [[Multivariate Density Function]] of $G$ over $\\mathbb{R}^{k}$ conditioned on the event that $U_{i} = b$.\n\nNow writing out the expression for [[Conditional Entropy]]\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) \u0026= -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}} \\lg(p^{(i)}_{1, G}(x))p^{(i)}_{1, G}(y)dy -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\lg(p^{(i)}_{0, G}(x))p^{(i)}_{0, G}(y)dy\n\\end{align*}$$\n\nTo make this expression more meaningful, we want to write this in terms of the values encoded by $f$. Since $f$ is an [[Injection]], the particular choice of codeword is fully determined by the input sequence. So if we marginalize on input sequences with a particular choice for bit $i$, we can write our PDF in terms of codewords. We have $2^{n}$ codewords, so let $x^{(j)}$ denote the $j$th codeword. Let $A \\subset \\mathbb{R}^{k}$ be a [[Measureable Set|Measureable]] subset. Then\n\n$$\\begin{align*}\n\\mathbb{P}(G \\in A | U_{i} = b) \u0026= \\sum\\limits_{f^{-1}(x^{(j)})_{i} = b}\\mathbb{P}(G \\in A | F = x^{(j)}) \\mathbb{P}(F = x^{(j)} | U_{i} = b)\\\\\n\u0026=\\sum\\limits_{f^{-1}(x^{(j)})_{i} = b}\\mathbb{P}(G \\in A | F = x^{(j)})\\\\\n\u0026=\\sum\\limits_{f^{-1}(x^{(j)})_{i} = b}\\mathbb{P}(G \\in A | F = x^{(j)})\\\\\n\u0026=\\sum\\limits_{f^{-1}(x^{(j)})_{i} = b} \\int\\limits_{A} p^{(F=x^{(j)})}_{G}(y)dy\\\\\n\u0026=\\int\\limits_{A} \\sum\\limits_{f^{-1}(x^{(j)})_{i} = b} p^{(F=x^{(j)})}_{G}(y)dy\n\\end{align*}$$\nso $$\\sum\\limits_{f^{-1}(x^{(j)})_{i} = b} p^{(F=x^{(j)})}_{G}(y)$$ is a valid [[Multivariate Density Function]] for $G$ conditioned on $U_{i} = b$ and\n$$p^{(i)}_{b, G}(y) = \\sum\\limits_{f^{-1}(x^{(j)})_{i} = b} p^{(F=x^{(j)})}_{G}(y)$$\n\nIn this form, this expression is not particularly useful. However if we rewrite it in terms of the log-densities, we get something more interesting. Let\n\n$$l^{(F=x^{(j)})}_{G}(y) = \\lg p^{(F=x^{(j)})}_{G}(y)$$\n\nand let $\\text{ep}(x) = 2^{x}$. Then\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) \u0026= -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}} \\lg(\\sum\\limits_{f^{-1}(x^{(j)})_{i} = 1} \\Big( \\text{ep}(l^{(F=x^{(j)})}_{G}(y)) \\Big) p^{(i)}_{1, G}(y)dy -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\lg(\\sum\\limits_{f^{-1}(x^{(j)})_{i} = 0} \\Big( \\text{ep}(l^{(F=x^{(j)})}_{G}(y)) \\Big) p^{(i)}_{0, G}(y)dy\n\\end{align*}$$\n\nNow using the approximation\n\n$$\\max_{i}\\{x_{i}\\} = \\lg \\sum\\limits_{i} \\text{ep}(x_{i})$$\n\nwe get\n\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) \u0026= -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}} \\max_{f^{-1}(x^{(j)})_{i} = 1} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 1} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg)dy -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\max_{f^{-1}(x^{(j)})_{i} = 0} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 0} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg)dy\n\\end{align*}$$\n\nIf we assume we are using the [[Additive White Gaussian Noise Channel]], then \n$$\\begin{align*}\nl^{(F=x^{(j)})}_{G}(y) \u0026= C - \\frac{||y - x^{(j)}||_{2}^{2}}{B} \n\\end{align*}$$\n\nSo the max of this log density is the term that minimizes the [[Euclidean Distance]] between our noisy sequence and our codeword. Recalling that [[Max of Negative is Negative of Min]]\n\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) \u0026= -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}} \\min_{f^{-1}(x^{(j)})_{i} = 1} \\Big(  \\frac{||y - x^{(j)}||_{2}^{2}}{B} - C\\Big) \\text{ep}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 1} \\Big(  \\frac{||y - x^{(j)}||_{2}^{2}}{B} - C\\Big) \\Bigg)dy - \\min_{f^{-1}(x^{(j)})_{i} = 0} \\Big(  \\frac{||y - x^{(j)}||_{2}^{2}}{B} - C\\Big) \\text{ep}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 0} \\Big(  \\frac{||y - x^{(j)}||_{2}^{2}}{B} - C\\Big) \\Bigg)dy\n\\end{align*}$$\n\nLet's similarly formulate $p_{G}(G=y)$ using the $\\max$ approximation to also approximate $\\mathbb{H}(G)$ where $p_{G}(G=y)$ is the [[Multivariate Density Function]] of $G$. We can write\n\n$$p_{G}(y) = \\sum\\limits_{x^{(j)}} p^{(F=x^{(j)})}_{G}(y)$$\nThen, approximating the log density of $G$\n$$\\begin{align*}\nl_{G}(y) \u0026= \\lg \\sum\\limits_{x^{(j)}} \\text{ep} ( l^{(F=x^{(j)})}_{G}(y) )\\\\\n\u0026\\approx \\max(l^{(F=x^{(j)})}_{G}(y))\n\\end{align*}$$\nWe get for $\\mathbb{H}(G)$\n\n$$\\begin{align*}\n\\mathbb{H}(G) \u0026= -\\int\\limits_{\\mathbb{R}^{k}} \\text{ep}\\Big( \\max(l^{(F=x^{(j)})}_{G}(y)) \\Big) \\max(l^{(F=x^{(j)})}_{G}(y)) dy\\\\\n\u0026= -\\int\\limits_{\\mathbb{R}^{k}} \\text{ep}\\Big( \\max(l^{(F=x^{(j)})}_{G}(y)) \\Big) \\max(l^{(F=x^{(j)})}_{G}(y)) dy\\\\\n\u0026\\approx -\\int\\limits_{\\mathbb{R}^{k}} \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 1} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg) \\max(l^{(F=x^{(j)})}_{G}(y)) + \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 0} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg) \\max(l^{(F=x^{(j)})}_{G}(y))dy\n\\end{align*}$$\n\nCombining with $H(G|U_{i})$\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) - \\mathbb{H}(G) \u0026\\approx -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 1} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) - 2\\max(l^{(F=x^{(j)})}_{G}(y)) \\Bigg) \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 1} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg)dy -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 0} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) -  2\\max(l^{(F=x^{(j)})}_{G}(y) \\Bigg) \\text{ep}\\Bigg( \\max_{f^{-1}(x^{(j)})_{i} = 0} \\Big( l^{(F=x^{(j)})}_{G}(y) \\Big) \\Bigg)dy\n\\end{align*}$$\n\nFor the AWGN case\n$$\\begin{align*}\n\\mathbb{H}(G|U_{i}) - \\mathbb{H}(G) \u0026\\approx -\\frac{2^{C}}{2B}\\int\\limits_{\\mathbb{R}^{k}}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 1} \\Big( ||y - x^{(j)}||_{2}^{2} \\Big) - 2\\min_{x^{(j)}}(||y - x^{(j)}||_{2}^{2}) \\Bigg) \\text{ep}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 1} \\Big( \\frac{||y - x^{(j)}||_{2}^{2}}{B} \\Big) \\Bigg)dy -\\frac{1}{2}\\int\\limits_{\\mathbb{R}^{k}}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 0} \\Big( ||y - x^{(j)}||_{2}^{2} \\Big) -  2\\min_{x^{(j)}}(||y - x^{(j)}||_{2}^{2}) \\Bigg) \\text{ep}\\Bigg( \\min_{f^{-1}(x^{(j)})_{i} = 0} \\Big( \\frac{||y - x^{(j)}||_{2}^{2}}{B} \\Big) \\Bigg)dy\n\\end{align*}$$\n\n\n\n\n\n# Other Outlinks\n* [[Joint Entropy]]\n* [[Entropy]]\n* [[Binary Field]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Minimum":{"title":"Untitled Page","content":"# Definition\nLet $(P, \\leq)$ be a [[Partial Ordering]]. Then a [[Minimum]] of $A \\subset P$ is any $x \\in A$ s.t. $\\forall y \\in A$, $x \\leq y$.\n\n# Remarks\n1. The minimum may not exist. A simple example is the set $\\mathbb{R}$. For every $x \\in \\mathbb{R}$, $x \\not\\leq x-1$. Thus there is no [[Minimum]] in $\\mathbb{R}$.\n2. If it exists, [[A Minimum in a Total Ordering is Unique]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Minimum-Vertex-Cover":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $\\mathcal{C} = \\{C \\subset V(G) : C \\text{ is a Vertex Cover}\\}$. Then a [[Minimum Vertex Cover]] is a [[Vertex Cover]] $C \\in \\mathcal{C}$  so that $|C| = \\min \\{|C| \\leq |V(G)| : C \\in \\mathcal{C}\\}$. \n\n## Remarks\n1. Note this definition is [[Well-Defined]] even for [[Infinite Set]]s because [[Cardinals form a Well-Ordering]].\n2. The [[Set Cardinality]] of a [[Minimum Vertex Cover]] on $G$ is denoted $\\tau(G)$.\n\n# Other Outlinks","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Monotone-Function":{"title":"Untitled Page","content":"# Definition\nA [[Function]] that is either [[Non-Decreasing]] or [[Non-Increasing]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Monotone-Sequence":{"title":"Untitled Page","content":"# Definition\nA [[Sequence]] that is a [[Monotone Function]] (from $\\mathbb{N}$ to its [[Codomain]]).","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Monotone-Sequences-converge-to-their-Extremum":{"title":"Untitled Page","content":"# Statement\n$(x_{n}) \\subset \\mathbb{R}$ be a [[Monotone Sequence]]. Let $t \\in \\bar{\\mathbb{R}}$ be it's [[Extremum]] corresponding to the direction of the monotone sequence ( if [[Non-Increasing]] then [[Infimum]]; if [[Non-Decreasing]] then [[Supremum]]). Then $x_{n} \\to t$.\n\n# Proof\nFirst suppose that $(x_{n})$ is [[Non-Decreasing]]. Observe that $t = \\sup\\{x_{n}\\}$ exists in $\\bar{\\mathbb{R}}$ since either\n1. $(x_{n})$ is bounded from above in $\\mathbb{R}$: By [[Completeness of the Real Numbers]], we know $\\sup\\{x_{n}\\} \\in \\mathbb{R}$.\n2. $(x_{n})$ is not bounded from above in $\\mathbb{R}$: $\\infty$ is always an [[Upper Bound]]  (by definition), so $\\sup\\{x_{n}\\}$ exists. Since no other element of $\\bar{\\mathbb{R}}$ is an [[Upper Bound]] (we're not bounded in $\\mathbb{R}$ and $- \\infty$ is less than every element in $\\mathbb{R}$), $\\infty$ is the only upper bound and thus the [[Supremum]].\n\nFirst we prove the statement for when $t \\in \\mathbb{R}$. Now let $\\epsilon \u003e 0$. Then because [[Finite Extremums get arbitrarily close]], we know there exists $N \\in \\mathbb{N}$ so that $x_{N} \u003e t - \\epsilon$. Since $(x_{n})$ is non-decreasing and $t = \\sup\\{x_{n}\\}$, we know for all $n \\geq N$:\n$$t \\geq x_{n} \\geq x_{N} \u003e t - \\epsilon$$\nThus, for all $n \\geq N$:\n$$|t - x_{n}| = t - x_{n} \u003c \\epsilon$$\nWe've satisfied the definition of [[Sequence Convergence]], so we have that $(x_{n}) \\to t$.\n\nNow suppose $t = \\infty$. Then we know $(x_{n})$ has no [[Upper Bound]] in $\\mathbb{R}$. In other words, $\\forall M \\in \\mathbb{R}$, we know there exists $N \\in \\mathbb{N}$ so that $x_{N} \u003e M$. Since $(x_{n})$ is [[Non-Decreasing]], we have that for all $n \\geq N$\n$$x_{n} \\geq x_{N} \u003e M$$\nThis satisfies the definition of [[Sequence Convergence to Infinity]], so $(x_{n}) \\to \\infty$ . \n\nNow consider when $(x_{n})$ is [[Non-Increasing]]. Then $\\forall n \u003e m \\in \\mathbb{N}$, we have that\n$$x_{n} \\leq x_{m}$$\nThus,\n$$-x_{n} \\geq -x_{m}$$\nand $(-x_{n})$ is a [[Non-Decreasing]] sequence. By our proof above, we know\n\n$$(-x_{n}) \\to \\sup\\{-x_{n}\\} = -\\inf\\{x_{n}\\}$$\nwhere the last equality follows because [[Supremum of Negative is Negative of Infimum]]. Since $x \\mapsto -x$ is [[Continuous]], we know $(x_{n}) \\to \\inf\\{x_{n}\\}$. $\\blacksquare$\n\n# Other Outlinks\n - [[Real Numbers]]\n - [[Extended Real Numbers]]\n - [[Sequence Convergence]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Multiplicative-Identity":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Musings":{"title":"Untitled Page","content":"* [[Information Space]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/N-tuples-over-a-Field-form-a-Vector-Space":{"title":"Untitled Page","content":"# Statement\nLet $F$ be a [[Field]]. Then $S = F^{n}$ is a [[Vector Space]].\n\n## Proof\nThis follows by recalling that [[Cartesian Product]]s are defined as [[Function]]s, then applying [[Functions to a Field form a Vector Space]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Natural-Numbers-are-Well-Ordered":{"title":"Untitled Page","content":"# Statement\n[[TODO]] \n\n# Other Outlinks\n* [[Well-Ordering]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Neighborhood-Basis":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Then $\\mathcal{B}_{x}$ is a [[Neighborhood Basis]] for $x \\in X$ if\n1. $B$ is [[Open]] for all $B \\in \\mathcal{B}_{x}$\n2. $x \\in B$ for all $B \\in \\mathcal{B}_{x}$\n3. If $U$ is [[Open]] and $x \\in U$, there exists $B \\in \\mathcal{B}_{x}$ so that $B \\subset U$.\n\n## Properties\n1. [[A Union of all Neighborhood Bases is a Topological Basis]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Nested-Intervals-Theorem":{"title":"Untitled Page","content":"# Statement\nLet $[a_{n}, b_{n}]$ be a [[Sequence]] of [[Closed Interval]]s in $\\mathbb{R}$ s.t. $b_{1} \\geq b_{2} \\geq ...$ and $a_{1} \\leq a_{2} \\leq \\cdots$ and $b_{n} - a_{n} \\to 0$ as $n \\to \\infty$. Then\n$$\\bigcap\\limits_{n \\in \\mathbb{N}} [a_{n}, b_{n}] = \\{c\\}$$\nfor some $c \\in \\mathbb{R}$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Neural-Network-Differentiation-Visualization":{"title":"Untitled Page","content":"Dates: [[2022-04-10]]\n\nhttps://www.math3d.org/MEty7vNn5","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Non-Decreasing":{"title":"Untitled Page","content":"# Definition\nLet $f: X \\to Y$, where $X$ and $Y$ are [[Total Ordering|totally ordered]]. Then $f$ is **Non-decreasing** if $\\forall x, y \\in X$:\n\n$$x \\leq y \\Rightarrow f(x) \\leq f(y)$$\nNote that if $x \u003e y$, we know that $y \\leq x $ so $f(x) \\geq f(y)$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Non-Increasing":{"title":"Untitled Page","content":"$x \\leq y \\Rightarrow f(x) \\geq f(y)$\n\nSee [[Non-Decreasing]] for more details.\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Nondeterministic-Polynomial-Time-Solvable":{"title":"Untitled Page","content":"[[TODO]] this is known as the [[Complexity Class]] $\\mathcal{NP}$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Nonempty":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Open":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]]. Then $U \\subset X$ is [[Open]] if $U \\in \\tau$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Open-Halfspace":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$. An [[Open Halfspace]] is a [[Set]] of the form\n$$H = \\{x : \\langle x, a \\rangle \u003e b\\}$$\nfor any $a \\in V$ and $b \\in \\mathbb{R}$.\n\n## Remarks\n1. An [[Open Halfspace]] is [[Open]]. [[TODO]] - prove this.\n2. If $H$ is an [[Open Halfspace]], then $\\text{cl} H$ is a [[Closed Halfspace]] - [[TODO]] prove this.\n3. The [[Boundary]] of a [[Closed Halfspace]] is a [[Hyperplane]] - [[TODO]] prove this.\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Closure]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Operation":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Optimal-Solution-Set-of-Linear-Program-is-a-Face-of-Constraint-Polytope":{"title":"Untitled Page","content":"# Statement\nSuppose for $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, we have the [[Linear Program]] \n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\geq 0,\n\\end{align*}$$\nThen the [[Set]] of [[Optimal Solution]]s is a [[Convex Polytope Face]] of the [[The Feasible Set of a Linear Program is a Convex Polytope|feasible set polytope]].\n## Proof\n[[TODO]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Optimal-Solution-of-Bounded-Linear-Program-is-at-Vertex":{"title":"Untitled Page","content":"# Statement\nSuppose $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$. Consider the [[Linear Program]]\n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\geq 0\n\\end{align*}$$\nSuppose there exists an optimum $\\mathbf{x}^{*} \\in \\mathbb{R}^{n}$. Then there is a [[Polytope Vertex]] $\\mathbf{v}^{*} \\in P$ of the [[Polytope]] $P$ defined by $A \\mathbf{x} \\leq \\mathbf{b}$ s.t. $\\max \\mathbf{c}^{T} \\mathbf{v}^{*} = \\max \\mathbf{c}^{T} \\mathbf{x}^{*}$. That is, $\\mathbf{v}^{*}$ is an optimal solution.\n## Proof\n[[TODO]] - this requires [[Convex Set|convexity]] of the [[Polytope]] and [[Open Halfspace]]s. Alternatively, could use [[Optimal Solution Set of Linear Program is a Face of Constraint Polytope]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Oracle-Government":{"title":"Untitled Page","content":"Dates: [[2022-04-17]]\n\nAn [[Oracle Government]] is a [[Governmental System]] such that whenever the citizens face a major problem, they go to an oracle device to get an answer for what they should do. It is a reference to how ancient Romans used to use oracles such as animals to decide what to do. They looked at the actions of these oracles as messages from the Gods. We could do something more sophisticated, but in the same vein.\n\nFor example, consider a [[Crowdsourcing Oracle]]. The [[Crowdsourcing Oracle]] will present the problem along with relevant details to [[Uniform Distribution|uniformly random]] individuals in the population (or outside the population). These individuals vote on decisions and the result comes back to the citizens, telling them what to do. We could even make the [[Crowdsourcing Oracle]] [[Algorithm]] more sophisticated by allowing the chosen crowdmembers to further crowdsource their own questions about the issue.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Order-Relation":{"title":"Untitled Page","content":"# Definition\nLet $P$ be a [[Set]] and let $R \\subset P \\times P$ be a [[Relation]] on $P$. Then $R$ is an [[Order Relation]] if $R$ is a [[Preorder Relation]] and the following holds:\n* **Antisymmetry**: Let $x, y \\in P$. If $xRy$ and $yRx$ then $x=y$\n\n# Properties\n* [[Every Preorder Relation induces an Order Relation]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Partial-Ordering":{"title":"Untitled Page","content":"# Definition\nLet $P$ be a [[Set]] and let $\\leq$ be an [[Order Relation]] on $P$. Then $(P, \\leq)$ is a [[Partial Ordering]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Partition":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]]. A [[Partition]] $\\mathcal{S} \\subset \\mathcal{P}(X)$ of $X$ is a collection of [[Mutually Disjoint]] [[Nonempty]] subsets of $X$ that [[Set Cover|cover]]  $X$. That is:\n1. $\\emptyset \\not\\in \\mathcal{S}$.\n1. $\\forall A, B \\in \\mathcal{S}$, either $A \\cap B = \\emptyset$ or $A = B$.\n2. $\\bigsqcup\\limits_{A \\in \\mathcal{S}} A = X$.\n\n# Remarks\n* For condition (2), observe that the two possibilities are [[Mutually Exclusive]]. Since $\\emptyset \\not\\in \\mathcal{S}$, if $A, B \\in \\mathcal{S}$ and $A = B$, then $\\exists x \\in A \\cap B$.\n# Other Outlinks\n* [[Power Set]]\n* [[Empty Set]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Path-Length":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is a [[Directed Graph]] ([[Undirected Graph]]) and $P$ is a [[Graph Path]] on $G$. Then the [[Path Length]] of $P$ is the [[Tuple Order]] of $P$. We denote it $|P|$.\n\n## Remarks\n1. This is related, but should not be confused with [[Set Cardinality]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Perfect-Matching":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]] and let $M \\subset E(G)$ be a [[Matching]] on $G$. Then $M$ is a [[Perfect Matching]] if $\\{v \\in V(G) : v \\text{ is covered by }M\\} = V(G)$. That is, all [[Graph Vertex|vertices]] are [[Matching Covered|covered]] by $M$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Perfect-Matching-Problem":{"title":"Untitled Page","content":"# Definition\nThe [[Perfect Matching Problem]] is a [[Decision Problem]] that asks whether a given [[Undirected Graph]] $G$ has a [[Perfect Matching]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Pointwise-Convergence":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]] and let $(Y, \\tau)$ be a [[Topological Space]]. Suppose $(f_{n})$ is a [[Sequence]] of [[Function]]s from $X$ to $Y$. Suppose $f: X \\to Y$ so that $\\lim\\limits_{n \\in \\mathbb{N}} f_{n}(x) = f(x)$ $\\forall x \\in X$. Then we say that $(f_{n})$ [[Pointwise Convergence|converges pointwise]] to $f$.\n\n## Remarks\n1. We can also talk about [[Pointwise Convergence]] on subset $S \\subset X$. This is just [[Pointwise Convergence]] of the [[Function Restriction]] on $S$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Positive-Semidefinite-Matrices-form-a-Convex-Cone":{"title":"Untitled Page","content":"# Statement\nSuppose $n \\in \\mathbb{N}$. Then $\\mathbb{S}_{n}^{+}$ is a [[Convex Cone]] over $\\mathbb{R}$.\n\n## Proof\nSuppose $X, Y \\in \\mathbb{S}_{n}^{+}$ and $a, b \\geq 0$. Consider $Z = aX + bY$. For any $x \\in \\mathbb{R}^{n}$, we have that\n\n$$\\begin{align*}\nx^{T}Z x \u0026= x^{T}(aX + bY) x\\\\\n\u0026=a x^{T}X x + b x^{T} Y x\\\\\n\u0026\\geq 0\n\\end{align*}$$\n\nSince $X, Y \\in \\mathbb{S}_{n}^{+}$ and $a, b \\geq 0$. $\\blacksquare$\n\n# Other Outlinks\n* [[Positive Semidefinite Matrix]]\n* [[Real Numbers]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Positive-Semidefinite-Matrix":{"title":"Untitled Page","content":"# Definition\nSuppose $A \\in \\mathbb{R}^{n \\times n}$ for some $n \\in \\mathbb{N}$ and that $\\forall x \\in \\mathbb{R}^{n}$, $$x^{T} A x \\geq 0$$. Then $A$ is a [[Positive Semidefinite Matrix]]. We denote the set of $n \\times n$ [[Positive Semidefinite Matrix]]s $\\mathbb{S}_{n}^{+}$\n\n## Propertes\n1. [[A Square Matrix is Positive Semidefinite iff its Eigenvalues are Nonnegative]]\n2. [[A Square Matrix is Positive Semidefinite iff all of its Principal Minors are Nonnegative]]\n3. [[A Square Matrix is Positive Semidefinite iff it is the Gram Matrix of n vectors]]\n4. [[A Square Matrix is Positive Semidefinite iff it is the product of a Matrix with its Transpose]]\n5. [[Positive Semidefinite Matrices form a Convex Cone]]\n6. [[Positive Semidefnite Matrices are Symmetric]] - Thus justifies the notation.\n7. [[A Symmetric Matrix is Positive Semidefinite iff its Frobenius Inner Product with any Positive Semidefinite Matrix is Nonnegative]]\n\n# Other Outlinks\n* [[Transpose Matrix]]\n* [[Real Numbers]]\n* [[Natural Numbers]]\n\n# Useful Sources\n* [CMU Class Lecture 12](https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/notes/lecture12.pdf)\n* [[Boyd - Convex Optimization]]\n* [Two Basic Probabilistic Proofs](https://djalil.chafai.net/blog/2011/08/23/two-basic-probabilistic-proofs/)","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Preorder":{"title":"Untitled Page","content":"# Definition\nLet $P$ be a [[Set]] and let $R$ be a [[Preorder Relation]] on $P$. Then $(P, R)$ is a [[Preorder]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Preorder-Relation":{"title":"Untitled Page","content":"# Definition\nLet $P$ be a [[Set]] and let $R \\subset P \\times P$ be a [[Relation]] on $P$. Then $R$ is a [[Preorder Relation]] if\n1. **Reflexitivity**: $xRx$ for all $x \\in P$\n2. **Transitivity**: Let $x,y,z \\in P$. If $xRy$ and $yRz$ then $xRz$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Probability-Distribution-Function":{"title":"Untitled Page","content":"# Definition\nSuppose $F$ is a [[Distribution Function]]. Then we call it a **Probability Distribution Function** if\n$$\\begin{align*}\n\u0026F(\\infty) = 1\\\\\n\u0026F(-\\infty) = 0\\\\\n\\end{align*}$$\nWe may also call $F$ a\n* [[Proper Distribution Function]]\n* [[Non-defective Distribution Function]]\n\n# Encounters\n* [[2022-02-24]] - [[Resnick - A Probability Path]] - pg 247","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Probability-Space":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Proof-by-Contradiction":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Proper-Face":{"title":"Untitled Page","content":"# Definition\nLet $V$ be an [[Inner Product Space]] over $\\mathbb{R}$ and let $P \\subset V$ be a [[Convex Polytope]]. A [[Proper Face]] of $P$ is a [[Convex Polytope Face|face]] of $P$ that is not $P$ or $\\emptyset$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Pugh-Real-Mathematical-Analysis":{"title":"Untitled Page","content":"[Google Drive Library](https://drive.google.com/file/d/1iwUMsEFDIbKqby3c9Nb0SfGJXkmoFxVB/view?usp=sharing)","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Quick-Latex-Custom-Command":{"title":"Untitled Page","content":"ua:\\uparrow, da:\\downarrow, lar: \\leftarrow, Lar:\\Leftarrow, ra:\\rightarrow, Ra:\\Rightarrow, pcw:\\begin{cases} \\end{cases}, Dun:\\bigsqcup\\limits_{}, dun:\\sqcup, Intr:\\bigcap\\limits_{}, intr:\\cap, Un:\\bigcup\\limits_{}, un:\\cup, min:\\min, setm:\\setminus, int:\\int\\limits, cal:\\mathcal{}, sequence:(x_n) \\subset X, seqset:\\{x_n\\}_{n=1}^{\\infty} \\subset X,qed:\\blacksquare, check:\\checkmark, empty:\\emptyset, ne:\\neq, restr:{\\big|}_{}, sups:\\supset, ext:\\text{ext}, sup:\\sup, inf:\\inf, max:\\max, min:\\min, id:\\text{id}_{}, lim:\\lim\\limits, inft:\\infty, ge:\\geq, le:\\leq, to:\\to , sq:\\sqrt{}, bb:\\mathbb{}, bf:\\mathbf{}, te:\\text{}, cd:\\cdot, qu:\\quad, ti:\\times, al:\\alpha, be:\\beta, ga:\\gamma, Ga:\\Gamma, de:\\delta, De:\\Delta, ep:\\epsilon, ze:\\zeta, et:\\eta, th:\\theta, Th:\\Theta, io:\\iota, ka:\\kappa, la:\\lambda, La:\\Lambda, mu:\\mu, nu:\\nu, xi:\\xi, Xi:\\Xi, pi:\\pi, Pi:\\Pi, rh:\\rho, si:\\sigma, Si:\\Sigma, ta:\\tau, up:\\upsilon, Up:\\Upsilon, ph:\\phi, Ph:\\Phi, ch:\\chi, ps:\\psi, Ps:\\Psi, om:\\omega, Om:\\Omega, sub:\\subset , subn:\\subsetneq , in:\\in , cont:\\ni, nin:\\not\\in, fa:\\forall , ex:\\exists , Rea:\\mathbb{R} , Nat:\\mathbb{N} , Rat:\\mathbb{Q} , Int:\\mathbb{Z}, Com:\\mathbb{C}, bar:\\bar{}, hat:\\hat{}, cl:\\text{cl}, inte:\\text{int}, log:\\log, exp:\\exp, cos:\\cos, sin:\\sin, co:\\circ, deriv:\\frac{du}{dt}, pderiv:\\frac{\\partial u}{\\partial t}, psd:\\mathbb{S}^{+}_{n}, tr:\\text{tr}, conj:\\overline{}, pow:\\mathcal{P}(), dots:\\dots, cdots:\\cdots","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/README":{"title":"Untitled Page","content":"# math-wiki\nPersonal Wiki for math and research\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Random-Variable":{"title":"Untitled Page","content":"# Definition\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]]. A [[Random Variable]] is a [[Borel Measureable Function]] $X: \\Omega \\to \\mathbb{R}$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Rayleigh-Quotient":{"title":"Untitled Page","content":"# Definition\nSuppose $n \\in \\mathbb{N}$, $M \\in \\mathbb{C}^{n \\times n}$ is a [[Hermitian Matrix]], and $\\mathbf{x} \\in \\mathbb{C}^{n}$ so that $\\mathbf{x} \\neq \\mathbf{0}$. Then the [[Rayleigh Quotient]] is defined as $$R(M, x) = \\frac{x^{*}Mx}{x^{*}x}$$\n\n## Properties\n1. [[Maximum of Rayleigh Quotient is maximum Eigenvalue]]\n2. [[Minimum of Rayleigh Quotient is minimum Eigenvalue]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Real-Numbers":{"title":"Untitled Page","content":"# Definition\nThe unique [[Complete Ordering|Complete]] [[Total Ordering|totally ordered]] [[Field]]. It is usually denoted $\\mathbb{R}$ and is used to represent the continuum. \n\n# Remarks\n1. One approach is to [[Axiom|axiomatically]] assume the existence of the [[Real Numbers]]\n2. An alternative approach is [[Construction of the Real Numbers|to construct the reals]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Relation":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]]. A [[Relation]] $R$ on $X$ is a subset of $X \\times X$. If $(x, y) \\in R$, then we write $xRy$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Research-Paper-Project-Notes-on-Introduction":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Resnick-A-Probability-Path":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Right-Continuous":{"title":"Untitled Page","content":"# Definition\n\nLet $f: \\mathbb{R} \\to M$ where $(M, d)$ is a [[Metric Space]]. Then $f$ is [[Right-Continuous]] at $x \\in \\mathbb{R}$ if\n\n$$\\lim_{y \\downarrow x} f(y) = f(x)$$\n$f$ is [[Right-Continuous]] if it is [[Right-Continuous]] at all $x \\in \\mathbb{R}$.\n\n# Other Outlinks\n* [[Right Function Limit]]\n# Encounters\n* [[2022-02-24]] - [[Resnick - A Probability Path]] - pg 247","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Right-Continuous-Inverse":{"title":"Untitled Page","content":"# Definition\nLet $F: \\mathbb{R} \\to \\mathbb{R}$ be a [[Distribution Function]]. The [[Right Continuous Inverse]] of $F$ is defined\n\n$$F^{\\rightarrow}(x) = \\inf \\{s \\in \\mathbb{R} : F(s) \u003e x\\}$$\n## Properties\n1. $F^{\\rightarrow}$ is [[Non-Decreasing]].\n\n\t**Proof**: Let $A_{x} = \\{s \\in \\mathbb{R} : F(s) \u003e x\\}$. Suppose $y \\leq x$. Then for all $s \\in A_{x}$, we have that $s \u003e x \\geq y$ so $s \\in A_{y}$. Thus, $A_{x} \\subset A_{y}$. Since [[Infimums are Non-Increasing Set Functions]], we have that $F^{\\rightarrow}(y) \\leq F^{\\rightarrow}(x)$. $\\blacksquare$\n2. $F^{\\rightarrow}$ is [[Right-Continuous]].\n\n\t**Proof**: [[TODO]] \n3. $\\lim\\limits_{t \\uparrow x} F^{\\rightarrow}(t) = F^{\\leftarrow}(x)$\n\n\t**Proof**: [[TODO]] \n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Left Function Limit]]\n* [[Set]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Right-Function-Limit":{"title":"Untitled Page","content":"# Definition\nLet $f: \\mathbb{R} \\to M$ be a [[Function]] where $(M, d)$ is a [[Metric Space]] and let $x \\in \\mathbb{R}$. Suppose there exists $y \\in M$ s.t. \n$$\\lim\\limits_{u \\to x} f{\\big|}_{[x, \\infty)}(u) = y$$ where $f {\\big|}_{[x, \\infty)}$ is the [[Function Restriction]] of $f$ to $[x, \\infty)$. Then we call $y$ the [[Right Function Limit]] of $f$ at $x$. We denote it $$\\lim_{u \\downarrow x} f(u)$$\n# Remarks\n1. Applying the [[Heine Criterion]] to $[x, \\infty)$ gives us the connection between [[Sequence]]s in $[x, \\infty)$ converging to $x$ and the [[Right Function Limit]] at $x$.\n# Other Outlinks\n* [[Real Numbers]]\n* [[Function Limit]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Risk":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. Then the [[Risk]] on $(X, Y, n)$ of [[Classifier]] $f$ is\n$$R(f) = \\mathbb{E}[1_{Y \\neq f(X)}] = \\mathbb{P}(Y \\neq f(X))$$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Rooted-Tree":{"title":"Untitled Page","content":"# Definition\nLet $T = (V, E)$ be a [[Tree]] and let $r \\in V$. Then we say $(T, r)$ is a [[Rooted Tree]], rooted at $r$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Scalar-product-with-1-is-additive-inverse-in-vector-spaces":{"title":"Untitled Page","content":"# Statement\nSuppose $v \\in V$. Then $-1 v = -v$.\n## Proof\n$$\\begin{align*}\nv + (-1) v \u0026= 1 v + (-1) v\\\\\n\u0026=(1 - 1)v\\\\\n\u0026= 0 v\\\\\n\u0026 \\mathbf{0},\n\\end{align*}$$\n\nwhere the last line follows from the property above. Because $V$ is an [[Abelian Group]], we know [[Left and Right Inverses in an Abelian Group are the Unique Inverse]], so $-1 v$ is the unique [[Additive Inverse]] of $v$. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Scheffes-Lemma":{"title":"Untitled Page","content":"# Statement\nLet $F_{1}, F_{2}$ be two [[Probability Distribution Function]]s and suppose they have [[Probability Density Function|density]] [[function]]s $f_{1}, f_{2}$ with respect to [[Measure]] $\\mu$ on $\\mathcal{B}( \\mathbb{R})$. Then  \n\n$$\\sup_{B \\in \\mathcal{B}(\\mathbb{R})} |F_{1}(B) - F_{2}(B)| = \\frac{1}{2} \\int |f_{1} - f_{2}| d \\mu$$\nIn particular, if $F_{n}$ is a sequence of [[Probability Distribution Function]]s, $f_{n}$ the corresponding [[Probability Density Function]]s, and we have target $F$ with density $f$, then if $f_{n} \\to f$ $\\mu$-a.e. we have that $F_{n} \\to F$ in [[Total Variation]].\n\n# Proof\n[[TODO]]\n\n# Other outlinks\n* [[Borel Sigma Algebra]]\n* [[Real Numbers]]\n\n# Encounter\n* [[2022-02-25]] - [[Resnick - A Probability Path]] - pg 253","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Semidefinite-Program":{"title":"Untitled Page","content":"# Definition\n[[TODO]]\n\n%%Suppose $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$. Then the [[Constrained Optimization Program]] \n\n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\geq 0\n\\end{align*}$$%%","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sequence":{"title":"Untitled Page","content":"# Definition\nA [[Function]] $f: \\mathbb{N} \\to X$ for some [[Set]] $X$. It is usually denoted $\\{x_n\\}_{n=1}^{\\infty} \\subset X$ or $(x_{n}) \\subset X$, and if $n \\in \\mathbb{N}$ is known, we write $x_{n}$ as shorthand for $f(n)$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sequence-Convergence":{"title":"Untitled Page","content":"# Definition 1\nLet $(X, \\tau)$ be a [[Topological Space]] and let $(x_n) \\subset X$. We say $x_{n}$ converges to $x \\in X$ if $\\forall V \\in \\tau$ so that $x \\in V$, there exists $N \\in \\mathbb{N}$ so that $x_{n} \\in V$ $\\forall n \\geq N$. If we have convergence, we write\n\n$$\\lim\\limits_{n \\to \\infty} x_{n} = x$$\nor \n$$x_{n} \\to x \\text{ (as } n \\to \\infty)$$\n\n# Definition 2\nLet $(M, d)$ be a [[Metric Space]] and let $(x_n) \\subset M$. We say $x_{n}$ converges to $x \\in X$ if $\\forall \\epsilon \u003e 0$ there exists $N \u003e 0$ s.t. $d(x_{n}, x) \u003c \\epsilon$ for all $n \\geq N$.\n\n# Properties\n1. Definition 1 is true [[If and Only If]] Definition 2 is as well on the [[Metric Topology]] of $M$:\n\t\n\t**Proof** \n\t$(\\Rightarrow)$: Suppose Definition 1 holds and $x_{n} \\to x \\in M$. Let $\\epsilon \u003e 0$. Since $B_\\epsilon(x)$ is [[Open]] in $M$, there exists $N \\in \\mathbb{N}$ so that for all $n \\geq N$ we have that $x_{n} \\in B_{\\epsilon}(x)$. By definition of $B_{\\epsilon}(x)$, this means $d(x_{n}, x) \u003c \\epsilon$ $\\checkmark$.\n\t\n\t($\\Leftarrow$): Suppose Definition 2 holds and $x_{n} \\to x \\in M$. Let $V \\in \\tau$ contain $x$. Since [[Every Open Set in the Metric Topology contains a Ball around each Point]], we know there exists $\\epsilon \u003e 0$ so that $B_{\\epsilon}(x) \\subset V$. By Definition 2, there exists $N \\in \\mathbb{N}$ so that for all $n \\geq N$, $d(x_{n}, x) \u003c \\epsilon$. In other words, $x_{n} \\in B_{\\epsilon}(x)$ and thus $x_{n} \\in V$. $\\checkmark$ $\\blacksquare$\n\n\t[[TODO]] This could probably reduced to single line about the relationship between sequence convergence and a [[Topological Basis]].\n\n\n# Other Outlinks\n* [[Open Ball]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sequence-Limits-are-unique-in-Hausdorff-Spaces":{"title":"Untitled Page","content":"# Statement\nSuppose $(X, \\tau)$ is a [[Hausdorff Space]]. Let $(x_n) \\subset X$. Then if $x_{n} \\to x \\in X$ and $x_{n} \\to y \\in Y$, $x=y$.\n\n## Proof\nSuppose not. Then there exists a sequence $(x_n) \\subset X$ so that $x_{n} \\to x$ and $x_{n} \\to y$ and $x \\neq y$. Since $X$ is a [[Hausdorff Space]], there exists $U, V \\subset X$ [[Open]] so that $x \\in U$, $y \\in V$, and $U \\cap V = \\emptyset$. Since $x_{n} \\to x$, there exists $N \\in \\mathbb{N}$ so that $\\forall n \\geq N$, we have that $x_{n} \\in U$. Likewise, since $x_{n} \\to y$, there exists $M \\in \\mathbb{N}$ so that $\\forall m \\geq M$, we have that $x_{m} \\in V$. Then $$U \\cap V \\supset \\{x_n\\}_{n=\\max(M, N)}^{\\infty}$$\n\n[[Proof by Contradiction|Contradicting]] the [[Mutually Disjoint]]ness of $U$ and $V$. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sequential-Limits-are-Limit-Points-of-the-Sequence":{"title":"Untitled Page","content":"# Definition\nSuppose $(X, \\tau)$ is a [[Topological Space]] and let $(x_n) \\subset X$. Suppose $x_{n} \\to x \\in X$. Then $x$ is a [[Limit Point]] of $\\{x_{n} : n  \\in \\mathbb{N}\\}$.\n\n## Proof\nSuppose $U \\subset X$ is [[Open]]. Then, by definition of [[Sequence Convergence]], there exists $N \\in \\mathbb{N}$ so that $\\forall n \\geq N$, $x_{n} \\in U$. Thus, $U \\cap \\{x_{n} : n \\in \\mathbb{N}\\} \\supset \\{x_{n} : n \\geq N\\}$ and $x$ is a [[Limit Point]] of our [[Sequence]].\n\n# Other Outlinks","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Shannon-Entropy-is-a-Concave-Function":{"title":"Untitled Page","content":"[[Shannon Entropy]] is a [[Concave Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sigma-Algebra":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Nonempty]] [[Set]]. Then $\\mathcal{M} \\subset \\mathcal{P}(X)$ is a [[Sigma Algebra]] on $X$ if\n1. $X \\in \\mathcal{M}$\n2. If $A \\in \\mathcal{M}$ then $A^{C} \\in \\mathcal{M}$\n3. Suppose $(A_n)_{n \\in \\mathbb{N}} \\subset \\mathcal{M}$. Then $\\bigcup\\limits_{n \\in \\mathbb{N}} A_{n} \\in \\mathcal{M}$\n\n## Properties\n1. $\\mathcal{M}$ is closed under [[Countable]] [[Set Intersection]]\n\n\t**Proof**: This follows from [[De Morgan's Law]]. Suppose  $(A_n)_{n \\in \\mathbb{N}} \\subset \\mathcal{M}$. Then\n\t$$\\bigcap\\limits_{n \\in \\mathbb{N}} A_{n} = \\Big( \\bigcup\\limits_{n \\in \\mathbb{N}} A_{n}^{C} \\Big)^{C} \\in \\mathcal{M}$$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sigma-Algebra-induced-by-Function":{"title":"Untitled Page","content":"# Definition\nLet $X$ and $(Y, \\mathcal{N})$ be [[Measure Space]]s. Let $f: X \\to Y$ be a [[Function]]. Then the [[Sigma Algebra induced by Function|sigma algebra induced by]] $f$ is\n$$\\sigma(f) := \\{f^{-1}(E) : E \\in \\mathcal{N}\\}$$\n## Remarks\n1. We should check that $\\sigma(f)$ is indeed a [[Sigma Algebra]]\n\t\n\t**Proof**: We check the criteria for being a [[Sigma Algebra]]\n\t1. $Y \\in \\mathcal{N}$ and $f^{-1}(Y) = X \\in \\sigma(f)$.\n\t2. Suppose $A \\in \\sigma(f)$. Then there exists $E \\in \\mathcal{N}$ so $f^{-1}(E) = A$. Since $\\mathcal{N}$ is [[Sigma Algebra]] we know $E^{C} \\in \\mathcal{N}$. Thus $$A^{C} = f^{-1}(E)^{C} = f^{-1}(E^{C}) \\in \\sigma(f)$$\n\t3. Suppose $(A_n)_{n=1}^{\\infty} \\subset \\sigma(f)$. Then there exists $(E_n)_{n=1}^{\\infty} \\subset \\mathcal{N}$ such that $f^{-1}(E_{n}) = A_{n}$. Then $\\bigcup\\limits_{n \\in \\mathbb{N}} E_{n} \\in \\mathcal{N}$. $$\\bigcup\\limits_{n \\in \\mathbb{N}} f^{-1}(E_{n}) = f^{-1}\\Big(\\bigcup\\limits_{n \\in \\mathbb{N}} E_{n} \\Big) \\in \\sigma(f)$$ $\\blacksquare$\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sigma-Algebra-induced-by-Random-Variable":{"title":"Untitled Page","content":"[[Sigma Algebra induced by Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sigma-Algebra-induced-by-an-Indicator-Function":{"title":"Untitled Page","content":"# Statement\nLet $X$ be a [[Set]]. Let $A \\in \\mathcal{P}(X)$. Then $$\\sigma(1_{A}) = \\{\\emptyset, A, A^{C}, X\\}$$\n\n## Proof\nWe must check that \n$$\\sigma(1_{A}) = \\{1_{A}^{-1}(E) : E \\in \\mathcal{B}(\\mathbb{R}) \\} = \\{\\emptyset, A, A^{C}, X\\}$$\n\nSuppose $E \\in \\mathcal{B}(\\mathbb{R})$. Then there are 4 possiblities\n1. $E \\cap \\{0, 1\\} = \\emptyset$: Then $1_{A}^{-1}(E) = \\emptyset$.\n2. $E \\cap \\{0, 1\\} = \\{0\\}$: Then $1_{A}^{-1}(E) = A^{C}$.\n3. $E \\cap \\{0, 1\\} = \\{1\\}$: Then $1_{A}^{-1}(E) = A$.\n4. $E \\cap \\{0, 1\\} = \\{0, 1\\}$: Then $1_{A}^{-1}(E) = X$.\n\nSince these are the only 4 possibilities, we have that \n$$\\sigma(1_{A}) = \\{1_{A}^{-1}(E) : E \\in \\mathcal{B}(\\mathbb{R}) \\} = \\{\\emptyset, A, A^{C}, X\\}$$\n$\\blacksquare$\n\n# Other Outlinks\n* [[Sigma Algebra induced by Function]]\n* [[Borel Sigma Algebra]]\n* [[Real Numbers]]\n* [[Power Set]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Simple-Function":{"title":"Untitled Page","content":"# Definition\nLet $\\phi : X \\to \\mathbb{C}^{n}$ be a [[Function]] for some [[Set]] $X$ and $n \\in \\mathbb{N}$. Then $\\phi$ is a [[Simple Function]] if $\\phi(X)$ is a [[Finite Set]]. \n\nA useful property is that [[All Simple Functions can be Written in Standard Form]].\n","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Simple-Path":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is a [[Directed Graph]] ([[Undirected Graph]]) and suppose $P$ is a [[Graph Path|path]] on $G$ with [[Path Length]] $|P| = n$. Then $P$ is a [[Simple Path]] if $\\not \\exists i \u003c j \\in [n+1]$ so that $V(P)_{i} = V(P)_{j}$. That is, no [[Graph Vertex]] is repeated on $P$.\n\n## Examples\n1. The [[Empty Tuple]] is a [[Simple Path]] from any [[Graph Vertex]] to itself.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Smoothing":{"title":"Untitled Page","content":"# Statement\nLet $(\\Omega, \\mathcal{B}, \\mathbb{P})$ be a [[Probability Space]] and let $X$ be a [[Random Variable]] on $\\Omega$. Suppose $\\mathcal{G}_{1} \\subset \\mathcal{G}_{2} \\subset \\mathcal{B}$ are sub-[[Sigma Algebra]]s on $\\Omega$. Then \n\n$$\\mathbb{E}\\big[ \\mathbb{E}[X|\\mathcal{G}_{2}] | \\mathcal{G}_{1} \\big] = \\mathbb{E}[X| \\mathcal{G}_{1}]$$\n\nRecalling that [[Conditional Expectation over the Trivial Sigma Algebra is Expectation]], we see that if $\\mathcal{G}_{1}$ is the [[Trivial Sigma Algebra]] on $\\Omega$ then\n\n$$\\mathbb{E}\\big[ \\mathbb{E}[X|\\mathcal{G}_{2}] | \\mathcal{G}_{1} \\big] = \\mathbb{E}[X| \\mathcal{G}_{1}] = \\mathbb{E}[X]$$\n\n## Proof\nWe show that $\\mathbb{E}[X| \\mathcal{G}_{1}]$ satisfies the properties for [[Conditional Expectation]] of $\\mathbb{E}[X|\\mathcal{G}_{2}]$ with respect to $\\mathcal{G}_{1}$. By definition of [[Conditional Expectation]], we know $\\mathbb{E}[X|\\mathcal{G}_{1}]$ is $\\mathcal{G}_{1}$-[[Measureable Function|measureable]]. Let $A \\in \\mathcal{G}_{1}$. Then\n\n$$\\int\\limits_{A} \\mathbb{E}[X|\\mathcal{G}_{1}] = \\int\\limits_{A} X = \\int\\limits_{A} \\mathbb{E}[X|\\mathcal{G}_{2}]$$\n\nwhere the first equality follows from the definition of [[Conditional Expectation]] and the second because $A \\in \\mathcal{G}_{2}$ as well. Thus,\n\n$$\\mathbb{E}\\big[ \\mathbb{E}[X|\\mathcal{G}_{2}] | \\mathcal{G}_{1} \\big] = \\mathbb{E}[X| \\mathcal{G}_{1}]$$\n\n$\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Soft-Classifier":{"title":"Untitled Page","content":"# Definition\nLet $(X, Y, n)$ be a [[Classification Problem]]. Then $g: \\mathcal{D} \\to [0,1]^{n}$ is a [[Soft Classifier]] if $\\forall x \\in \\mathcal{D}$, $g(x)$ is a valid [[Probability Measure|Distribution]] on $[n]$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Solution-set-of-Linear-Equations-is-an-Affine-Set":{"title":"Untitled Page","content":"# Statement\nLet $V, W$ be a [[Vector Space]]s over $\\mathbb{R}$, let $T \\in \\mathcal{L}(V, W)$, and let $b \\in W$. Let $S = \\{x \\in V : T(x) = b\\}$. Then $S$ is an [[Affine Set]].\n\n## Proof\nLet $\\lambda \\in \\mathbb{R}$. Suppose $x_{1}, x_{2} \\in S$. Then\n\n$$\\begin{align*}\nT(\\lambda x_{1} + (1 - \\lambda) x_{2}) \u0026= \\lambda T(x_{1}) + (1 - \\lambda)T(x_{2})\\\\\n\u0026=\\lambda b + (1 - \\lambda) b\\\\\n\u0026=b\n\\end{align*}$$\nso $\\lambda x_{1} + (1 - \\lambda) x_{2} \\in S$ and $S$ is an [[Affine Set]]. $\\blacksquare$\n\n[[TODO]] - Converse is true for $\\mathbb{R}^{n}$ according to [[Boyd - Convex Optimization]] (Sect 2.1 pg 22, Example 2.1). Is this true in general?\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Linear Map]]\n* [[Linear Equation]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Space-of-Polynomials-over-a-Field-form-a-Vector-Subspace-of-Function-Vector-Space":{"title":"Untitled Page","content":"\n# Statement\nLet $F$ be a [[Field]] and denote $$P[F] = \\{x \\mapsto \\sum\\limits_{k=0}^{n} c_{k}x^{k} : n \\in \\mathbb{Z}_{\\geq 0}; c_{0}, \\dots, c_{n}  \\in F \\}$$ the [[Set]] of all [[Polynomial]]s over $F$. Then $P[F]$ forms a [[Vector Subspace]] of the space $F^{F}$.\n\n## Proof\nWe know from [[Functions to a Field form a Vector Space]] that $F^{F}$ forms a [[Vector Space]]. By [[A Subset of a Vector Space is a Subspace iff it is closed under scaling and addition]], it suffices to check we are closed under scaling and addition.\n\nSuppose $c \\in F$ and $p, r \\in P[F]$. Then there exist $n, m \\in \\mathbb{Z}_{\\geq 0}$ so that $$\\begin{align*}\n\u0026p = c_{0} + c_{1} x + \\cdots + c_{n} x^{n}\\\\\n\u0026r = d_{0} + d_{1} x + \\cdots + d_{m} x^{m}\\\\\n\\end{align*}$$\n[[Without Loss of Generality]] we can take $n \\geq m$ and then rewrite $$r = d_{0} + d_{1} x + \\cdots + d_{m} x^{m} + d_{m+1} x^{m+1} + \\cdots d_{n} x^{n},$$ taking $d_{m+1}, \\dots, d_{n} = 0$. Then we get that $$\\begin{align*}\ncp + r \u0026= \\sum\\limits_{k=0}^{n}c c_{k}x^{k} + \\sum\\limits_{k=0}^{n} d_{k}x^{k}\\\\\n\u0026=\\sum\\limits_{k=0}^{n}(c c_{k} + d_{k})x^{k} \\in P[F]\n\\end{align*}$$\n\n$\\blacksquare$\n\n## Remarks\n1. This also establishes that [[Polynomial]]s over a [[Field]] form a [[Vector Space]].","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Standard-Gaussian-Random-Variable":{"title":"Untitled Page","content":"# Definition\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]]. If $X \\sim \\mathcal{N}(0, 1)$, then $X$ is a [[Standard Gaussian Random Variable]].\n\n# Other Outlinks\n1. [[Gaussian Random Variable]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Stock":{"title":"Untitled Page","content":"# Definition\nA [[Stock]] is a [[Security]] that represents fractional ownership of a [[Corporation]]. Units of [[Stock]] are called [[Share]]s.\n\n## Ownership\nA [[Share]] of a [[Stock]] represents fractional ownership of a [[Corporation]], determined by the number of [[Outstanding Share]]s. For example, if a [[Corporation]] has 5000 outstanding shares, and I hold 100 of those, then I own 2% of the [[Corporation]]. The percentage does not depend on the number of [[Reserve Share]]s.\n\nAlthough a [[Stock]] *represents* fractional ownership of a [[Corporation]], it does not mean ownership of the [[Corporation]] itself. Legally, a [[Corporation]] is considered a person and thus files its own taxes, owns assets, and can be sued. To own a [[Corporation]] would be akin to slavery. Rather, [[Shareholder]]s are *entitled* to a portion of the [[Corporation]]s assets and (depending on the type of stock) may have voting rights at shareholder meetings. Shareholders also receive [[Dividend]]s (if they are distributed), and can sell their shares to someone else.\n\n## Trading\n[[Stock]]s are usually bought and sold at [[Stock Exchange]]s. [[TODO]]\n\n## Types of Stock\n* [[Common Stock]]\n* [[Preferred Stock]]\n\n\n\n# Sources\n1. [Investopedia - Stocks](https://www.investopedia.com/terms/s/stock.asp)\n2. [American Bar Association - Corporations and People](https://www.americanbar.org/groups/crsj/publications/human_rights_magazine_home/we-the-people/we-the-people-corporations/)","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Strictly-Convex-Function":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$, let $S \\subset V$ be a [[Convex Set]], and suppose $f: S \\to \\mathbb{R}$. Then $f$ is a [[Strictly Convex Function]] if for all $\\lambda \\in [0, 1]$ and all $x,y \\in S$\n$$f(\\lambda x + (1-\\lambda) y) \u003c \\lambda f(x) + (1- \\lambda) f(y)$$\n## Properties\n1. A [[Strictly Convex Function]] is a [[Convex Function]].\n2. What about [[Equivalent Conditions for Convexity]]? Do they have analogies for [[Strictly Convex Function]]? [[TODO]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Subgraph":{"title":"Untitled Page","content":"# Definition\nLet $G$ be a [[Directed Graph]] ([[Undirected Graph]]). Then $G'$ is a [[Subgraph]] of $G$ if $V(G') \\subset V(G)$ and $E(G') \\subset E(G)$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Subsequence":{"title":"Untitled Page","content":"# Definition\nLet $f: \\mathbb{N} \\to X$ be a [[Sequence]] on $X$. Let $g: \\mathbb{N} \\to \\mathbb{N}$ be an [[Increasing]] [[Function]]. Then $g \\circ f$ is a [[Subsequence]] of $f$.\n\nOftentimes if $(a_{n}) \\subset X$ is the sequence and $(n_{k}) \\subset \\mathbb{N}$ is the increasing function (i.e. $g$), then $(a_{n_{k}})$ is our subsequence.\n\n# Encounters\n1. [[Pugh - Real Mathematical Analysis]] - Ch 2, pg unknown","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Subspace-associated-with-an-Affine-Set":{"title":"Untitled Page","content":"# Statement\nLet $V$ be a [[Vector Space]] over $\\mathbb{R}$ and let $S \\subset V$ be an [[Affine Set]]. Suppose $u \\in S$. Then $$W = S - u = \\{v - u : v \\in S\\}$$ is a [[Vector Subspace]] of $V$. Furthermore, $W$ does not depend on the choice of $u$.\n\n## Proof\nFirst we verify that $W$ is [[A Subset of a Vector Space is a Subspace iff it is closed under scaling and addition|closed under scaling and addition]]. Let $x, y \\in W$ and let $a \\in \\mathbb{R}$. Then there exist $v, w \\in S$ so that $v = x + u$ and $w = y + u$. Then $$\\begin{align*}\nax + y + u \u0026= a(x + u) + (y + u) - au \\in S\\\\\n\\end{align*}$$ since $a + 1 - a = 1$ and $x + u, y + u, u \\in S$. Thus, by definition of $W$, $ax + y \\in W$ and $W$ is a [[Vector Subspace]]. $\\checkmark$\n\nNow suppose $s \\in S$ as well and that $U = S - s$. Suppose $p \\in U$. Then $(p + s) - u \\in W$. But $s - u \\in W$ as well, thus $W \\ni (p + s - u) - (s - u) = p$ and $U \\subset W$. Repeating the argument with $U$ and $W$ swapped gets us that $U = W$. $\\checkmark$ $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Sum-of-Independent-Random-Variables-is-in-L1-if-and-only-if-Random-Variables-are-in-L1":{"title":"Untitled Page","content":"# Statement\nSuppose $(\\Omega, \\mathcal{M}, \\mathbb{P})$ is a [[Probability Space]] and $X, Y$ are [[Independence|independent]] [[Random Variable]]s on $\\Omega$. Then $X+Y \\in L^{1}(\\Omega)$ [[If and Only If]] $X, Y \\in L^{1}(\\Omega)$.\n\n## Proof\n($\\Leftarrow$): This is the easy direction. Suppose $X \\in L^{1}$ and $Y \\in L^{1}$. Since [[Lp Spaces are Vector Spaces]], we have that $X + Y \\in L^{1}$ $\\checkmark$\n\n($\\Rightarrow$): Suppose $X+Y \\in L^{1}$. Recall from the definition of [[L1 Integrable Functions]], this means\n$$\\int\\limits_{\\Omega} |X(\\omega)+Y(\\omega)| d \\mathbb{P}(\\omega) \u003c \\infty$$\nSince $(X, Y)$  [[Independent Random Elements induce Product Measure|are independent, they induce a product measure]] $F_{X,Y}$ on $\\mathbb{R}^{2}$ where for $A,B \\in \\mathbb{R}$, $F_{X,Y}(A \\times B) = F_{X}(A)F_{Y}(B)$ ($F_{X}$ and $F_{Y}$ are the [[Induced Probability Measure]] of $X$ and $Y$ respectively). Furthermore\n\n$$\\begin{align*}\n\\int\\limits_{\\Omega} |X(\\omega)+Y(\\omega)| d \\mathbb{P}(\\omega) \u0026= \\int\\limits_{\\mathbb{R}^{2}} |x+y| F_{X,Y}(d(x,y))\\\\\n\u0026=\\int\\limits_{\\mathbb{R}} \\left[\\int\\limits_{\\mathbb{R}} |x+y| F_{X}(dx)\\right] F_{Y}(dy)\n\\end{align*}$$\nand $y \\mapsto \\int\\limits_{\\mathbb{R}} |x+y| F_{X}(dx) \\in L^{1}(\\Omega)$ by [[Fubini's Theorem]]. Thus, for $y \\in \\mathbb{R}$ [[Almost Everywhere]] we have that $\\int\\limits_{\\mathbb{R}} |x+y| F_{X}(dx) \u003c \\infty$. Pick some such $y \\in \\mathbb{R}$. Then\n\n$$\\begin{align*}\n\\int\\limits_{\\Omega} |X(\\omega)| d \\mathbb{P}(\\omega) \u0026= \\int\\limits_\\mathbb{R} |x| F_{X}(dx)\\\\\n\u0026=\\int\\limits_\\mathbb{R} |x+y-y| F_{X}(dx)\\\\\n\u0026\\leq \\int\\limits_\\mathbb{R} |x+y| F_{X}(dx) + \\int\\limits_\\mathbb{R} |y| F_{X}(dx)\\\\\n\u0026=\\int\\limits_\\mathbb{R} |x+y| F_{X}(dx) + |y|\\\\\n\u0026\u003c \\infty\n\\end{align*}$$\nand $X \\in L^{1}(\\Omega)$. Swap $X$ and $Y$ to get the same result for $Y$. $\\checkmark$ $\\blacksquare$\n\n\n# Other Outlinks\n* [[Expectation]]\n* [[Fubini's Theorem]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Supremum":{"title":"Untitled Page","content":"# Definition\nLet $(T \\leq)$ be a [[Total Ordering]]. Let $A \\subset T$. Set $U = \\{x \\in T : x \\geq a \\text{ } \\forall a \\in A\\}$. That is, $U$ is the [[Set]] of [[Upper Bound]]s of $A$. Then $\\sup A$ (if it exists) is the [[Minimum]] of $U$.\n\n# Remarks\n1. If $A \\subset T$ is s.t. $A = \\emptyset$, then examining the definition of $U$, we see that the condition holds vacuously and $U = T$. Thus $\\sup \\emptyset = \\min T$ if the minmum exists.\n3. Let $A \\subset \\mathbb{R}$. For some $u \\in \\mathbb{R}$, write $u \\geq A$ if $\\forall a \\in A$, $u \\geq a$. One of the equivalent conditions for [[Completeness of the Real Numbers]] is that every $A \\subset \\mathbb{R}$ s.t. $\\exists u \\in \\mathbb{R}$ with $u \\geq A$ has a [[Supremum]] in $\\mathbb{R}$.\n\n# Other Outlinks\n* [[Real Numbers]]\n* [[Empty Set]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Supremum-of-Negative-is-Negative-of-Infimum":{"title":"Untitled Page","content":"# Statement\nLet $A \\subset \\bar{\\mathbb{R}}$. Denote $-A = \\{-a : a \\in A\\}$. Then \n$$\\sup(-A) = -\\inf A$$\n# Proof\n[[TODO]] \n\n# Other Outlinks\n* [[Extended Real Numbers]]\n* [[Set]]\n* [[Supremum]]\n* [[Infimum]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Symmetric-Matrix":{"title":"Untitled Page","content":"# Definition\nSuppose $n \\in \\mathbb{N}$ and $A \\in F^{n \\times n}$ for some [[Field]] $F$. Then we say $A$ is a [[Symmetric Matrix]] if $A^{T} = A$\n\n# Other Outlinks\n* [[Transpose Matrix]]\n* [[Natural Numbers]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/TODO":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Taylors-Theorem":{"title":"Untitled Page","content":"[[TODO]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/The-Feasible-Set-of-a-Linear-Program-is-a-Convex-Polytope":{"title":"Untitled Page","content":"# Statement\nSuppose $n, m \\in \\mathbb{N}$, $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$ define the [[Linear Program]]\n$$\\begin{align*}\n\u0026\u0026\\max \\mathbf{c}^{T} \\mathbf{x}\\\\\n\u0026\\text{s.t.}\u0026A \\mathbf{x} \\leq \\mathbf{b}\\\\\n\u0026\u0026\\mathbf{x} \\geq 0\n\\end{align*}$$\n\nThen the [[Feasible Set]] of this [[Linear Program]] is a [[Convex Polytope]].\n## Proof\n$A \\mathbf{x} \\leq \\mathbf{b}$ is a system of [[Closed Halfspace]]s $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/The-Induced-Sigma-Algebra-of-a-Borel-Function-is-Finite-iff-it-is-Simple":{"title":"Untitled Page","content":"# Statement\nLet $X$ be a [[Set]]. Suppose $f: X \\to \\mathbb{R}$. Then $\\sigma(f)$ with respect to $\\mathcal{B}(\\mathbb{R})$ is a [[Finite Set]] [[If and Only If]] $f$ is a [[Simple Function]].\n\n## Proof\n$(\\Rightarrow)$: Suppose $f$ is not [[Simple Function|simple]]. Then $f(X)$ is an [[Infinite Set]]. Since [[Singletons are Closed in the Real Numbers]], they are in $\\mathcal{B}(\\mathbb{R})$. In other words, $\\{y\\} \\in \\mathcal{B}(\\mathbb{R})$ for all $y \\in f(X)$. Since $y \\in f(X)$, we have that $f^{-1}(\\{y\\}) \\neq \\emptyset$. On the other hand for $y, z \\in f(X)$ so that $y \\neq z$, we have $f^{-1}(\\{y\\}) \\cap f^{-1}(\\{z\\}) = f^{-1}(\\{y\\} \\cap \\{z\\}) = f^{-1}(\\emptyset) = \\emptyset$, so each of our [[Function Preimage|preimages]] are [[Nonempty]] [[Disjoint Sets]] (and are thus distinct).  Therefore, we have infinitely many distinct sets in $\\sigma(f)$. $\\checkmark$\n\n($\\Leftarrow$): [[A Simple Function Induces a Finite Sigma Algebra]]. $\\checkmark$ $\\blacksquare$\n\n# Other Outlinks\n* [[Borel Sigma Algebra]]\n* [[Real Numbers]]\n* [[Set Intersection]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/The-following-are-Equivalent":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/The-inverse-in-a-Group-is-unique":{"title":"Untitled Page","content":"# Statement\nSuppose $G$ is a [[Group]] and $u, v, w \\in G$ so that $v$, $w$ are both [[Additive Inverse]]s of $u$. Then $v = w$.\n\n## Proof\nDenote $e \\in G$ the [[Additive Identity]]. Then $$w = we = w(uv) = (wu)v = ev = v$$ by [[Associativity]]. $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Thinking-about-the-information-theory-of-NNs":{"title":"Untitled Page","content":"Dates: [[2022-04-07]]\n\nSuppose we have a [[Neural Network]] parameterized by weight matrices $W_{1}, \\dots, W_{L}$ with dimensions $W_{l} \\in \\mathbb{R}^{n_{l} \\times n_{l-1}}$, where $n_{0}$ is the input dimension, and [[Activation Function]] $\\phi$. Denote our [[Neural Network]] as a function $$h(x) = W_{L} \\phi (W_{L-1}\\phi(\\cdots )) \\text{ with } x \\in R ^{n_{0}}$$ Furthermore, denote the intermediate [[Activation]] by $h_{l}$ for the output at [[Layer]] $l$. That is, $h_{L} = h$.\n\nSuppose we have some [[Classification Problem]] on $(X, Y)$ with $k$ [[Label]]s and [[Data]] over $\\mathbb{R}^{n_{0}}$. Denote $f^{*}$ the [[Bayes Classifier]] for this problem. Let $\\mathcal{X}^{m}, \\mathcal{Y}^{m}$ be our [[Dataset]] sampled [[Independently and Identically Distributed|iid]].\n\nSuppose for sake of argument that each $W_{l}$ are chosen with maximum [[Corank]] $n_{l-1}$. That is, each linear transform is [[Injection|injective]]. Then we have that $h_{l}$ is an [[Injection]] for each $l$ and thus there exists $f^{*}_{l} = f^{*} \\circ h^{-1}$ so that for $x \\in \\mathbb{R}$:\n$$f_{l}^{*}(h(x)) = f(x)$$\nThen, we should have that $I(h(X), X) = \\mathbb{H}(X)$. That is, no information has been lost. However, this is certainly not what happens from the [[Neural Network]]'s perspective. If we choose a [[Random Gaussian Matrix]] with $n_{l} \u003e\u003e n_{l-1}$ (informally), we should have injective $W_{l}$, but the remainder of the [[Neural Network]] would have a very hard time finding an approximation for $f^{*}_{l}$ unless it was very expressive (deep or wide). It seems like we might have made the task \"harder\" for the neural network; that is, it will now take \"more\" width or depth to accomplish the task compared to just receiving the original data. This transformation has made the data \"more random\".\n\nThis intuition might come from the fact that, denoting $Z_{l} = h_{l}(X)$, if we allow $Z_{l}$ to also depend on the randomness of $W_{l}$, then $Z_{l}$ will likely not share much [[Mutual Information]] with $X$.\n* more rigorously formulating this would involve some result about the linear combination of independent random variables\n\nPerhaps here is one way to think of the $Z_{l}^{t}$, with $t \\in \\mathbb{R}$ denoting the evolution of the weights during training:\n1. [[Probability Space]] $(\\Omega, \\mathcal{M}, \\mathbb{P})$\n2. $X: \\Omega \\to \\mathbb{R}^{n_{0}}$ is a [[Random Vector]] denoting our [[Data]]\n3. $Y: \\Omega \\to [k]$ is a [[Random Variable]] denoting our [[Label]]\n4. [[Random Element|Random]] [[Dataset]] $\\mathcal{X}^{m}, \\mathcal{Y}^{m}$ sampled iid\n5. $W_{l}(t): \\Omega \\to \\mathbb{R}^{n_{l} \\times n_{l-1}}$ is a [[Random Matrix Process]] with $t \\in \\mathbb{R}_{\\geq 0}$. Here $W_l(0)$ is the initialization of the [[Neural Network]].\n6. $Z_{l}(t) = h_{l}^{t}(X) = W_{l}(t) \\phi (W_{l-1}(t)\\phi(\\cdots ))$ is a [[Random Vector Process]] with $t \\in \\mathbb{R}_{\\geq 0}$.\n7. [[Cost Function]] $c: \\mathbb{R}^{n_{L}} \\to \\mathbb{R}$ that is [[Differentiation|Differentiable]].\n8. [[Learning Rate]] $\\alpha \\in \\mathbb{R}$.\n\nThen, we would want to ask \"what is $I(Z_{l}(t), X)$ and $I(Z_{l}(t), Y)$?\" When training a [[Neural Network]], given the [[Dataset]] and initialization, our weights would follow the [[Differential Equation]]\n$$\\frac{dW_{l}(t)}{dt} = - \\alpha \\frac{\\partial c(h(\\mathcal{X^{m}}))}{\\partial W_{l}(t)}$$\nIf we remove the conditioning, this becomes a [[Stochastic Differential Equation]].\n\nAside: What about the note about making things \"harder\" for the neural network. [[TODO]]. Maybe this is a question of how \"close\" am I to a \"good\" approximation function in terms of my activations or my weights.\n# Other Outlinks\n* [[Mutual Information]]\n* [[Entropy]]\n* [[Function]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Topological-Basis":{"title":"Untitled Page","content":"# Definition 1\nLet $X$ be a [[Set]]. A [[Topological Basis]] $\\mathcal{B} \\subset \\mathcal{P}(X)$ for $X$ is a collection of subsets of $X$ such that:\n1. $\\forall x \\in X$ there exists $B \\in \\mathcal{B}$ so that $x \\in B$.\n2. Let $B_{1}, B_{2} \\in \\mathcal{B}$. $\\forall x \\in B_{1} \\cap B_{2}$ there exists $B_{3}$ s.t.\n\t1. $x \\in B_{3}$\n\t2. $B_{3} \\subset B_{1} \\cap B_{2}$\n\nWe refer to $\\tau(\\mathcal{B}) = \\bigcap \\{ \\tau : \\tau \\supset \\mathcal{B}, \\tau \\text{ is a topology on } X\\} = \\{\\bigcup\\limits_{B \\in \\mathcal{C}} B : \\mathcal{C} \\subset \\mathcal{B}  \\}$ is the [[Topology Generated by a Basis|topology generated by]] $\\mathcal{B}$.\n\n# Definition 2\nLet $X, \\tau$ be a [[Topological Space]]. Then $\\mathcal{B} \\subset \\tau$ is a [[Topological Basis]] for $X$ if \n$$\\tau = \\{\\bigcup\\limits_{B \\in \\mathcal{C}} B : \\mathcal{C} \\subset \\mathcal{B}\\}$$\n\n## Remarks\n1. $\\tau(\\mathcal{B})$ is a [[Topological Space]] since [[Intersection of Structures is still a Structure]].\n2. We should verify the last equality holds:\n\t\n\t**Proof**: First we show $\\text{RHS} \\supset \\text{LHS}$. We can show this by establishing that $\\text{RHS}$ is a [[Topological Space]]. It contains $\\mathcal{B}$ because for each $B \\in \\mathcal{B}$ we can take $\\mathcal{C} = \\{B\\}$. Checking the definition of a [[Topological Space]]:\n\t1. Since $\\forall x \\in X$ there exists $B \\in \\mathcal{B}$ so that $x \\in B$, we see $\\bigcup\\limits_{B \\in \\mathcal{B}} B \\supset X$. Since $\\mathcal{B} \\subset \\mathcal{P}(X)$, we see that $\\bigcup\\limits_{B \\in \\mathcal{B}} B \\subset X$. Thus $\\bigcup\\limits_{B \\in \\mathcal{B}} B = X$. Since $\\mathcal{B} \\subset \\mathcal{B}$, we have $$X = \\bigcup\\limits_{B \\in \\mathcal{B}} B \\in \\text{RHS}$$\n\t2. By taking $\\mathcal{C} = \\emptyset$, we get that $\\emptyset \\in \\text{RHS}$.\n\t3. Let $\\mathcal{D} \\subset \\text{RHS}$ and consider $\\bigcup\\limits_{D \\in \\mathcal{D}} D$. Indexing $\\mathcal{D} = \\{D_{\\alpha}\\}_{\\alpha \\in I}$ with [[Index Set]] $I$, note that each $D_{\\alpha}= \\bigcup\\limits_{B \\in \\mathcal{C}_{\\alpha}} B$  for some $\\mathcal{C}_{\\alpha} \\subset \\mathcal{B}$. Then letting $\\mathcal{C} = \\bigcup\\limits_{\\alpha \\in I} \\mathcal{C}_{\\alpha}$, $$\\bigcup\\limits_{\\alpha \\in I} D_{\\alpha}= \\bigcup\\limits_{\\alpha \\in I} \\bigcup\\limits_{B \\in \\mathcal{C}_{\\alpha}} B = \\bigcup\\limits_{B \\in \\mathcal{C}} B \\in \\text{RHS}$$\n\t4. Let $C, D \\in \\text{RHS}$. Consider $C \\cap D$. We can write $C = \\bigcup\\limits_{A \\in \\mathcal{C}} A$ and $D = \\bigcup\\limits_{B \\in \\mathcal{D}} B$ for some $\\mathcal{C}, \\mathcal{D} \\subset \\mathcal{B}$. Then $$\\begin{align*}\nC \\cap D \u0026= \\Big( \\bigcup\\limits_{A \\in \\mathcal{C}} A \\Big) \\cap \\Big( \\bigcup\\limits_{B \\in \\mathcal{D}} B \\Big)\\\\\n\u0026= \\bigcup\\limits_{(A, B) \\in \\mathcal{C} \\times \\mathcal{D}} A \\cap B\n\\end{align*}$$\n\t\tConsider $(A, B) \\in \\mathcal{C} \\times \\mathcal{D}$. Recall that property (2) of a [[Topological Basis]] states that for each $x \\in A \\cap B$ there exists $B_{x} \\in \\mathcal{B}$ so that $x \\in B_{x}$ and $B_{x} \\subset A \\cap B$. Then\n\t\t$$\\bigcup\\limits_{x \\in A \\cap B} B_{x} = A \\cap B$$\n\t\tThus $A \\cap B \\in \\text{RHS}$. Then by (3), we have that $C \\cap D \\in \\text{RHS}$. Therefore $\\text{RHS}$ is a topology containing $\\mathcal{B}$ and $\\text{RHS} \\supset \\text{LHS}$. \n\t\t\n\t\tTo see the other direction, recall that a [[Topological Space]] is closed under [[Set Union]]s. Thus, because $\\mathcal{B} \\subset \\text{LHS}$, we must have that $\\text{RHS} \\subset LHS$. Therefore $\\text{RHS} = \\text{LHS}$ and our definition is justified. $\\blacksquare$\n3. **Definition 2** is effectively giving us a \"[[Converse]]-like\" definition. It's saying that if we have a collection so that unions of its subcollections is our topology, then it is a basis that generates our topology. We should verify this rigorously:\n\n\t**Proof**: Suppose $\\mathcal{B}$ is such that **Definition 2** holds. We need only show that $\\mathcal{B}$ satisfies the criteria for being a [[Topological Basis]] specified in **Definition 1**. If it does, then **Remark 2** tells $\\tau(\\mathcal{B}) = \\tau$. To this end observe\n\t1. Since $X \\in \\tau$, there exists $\\mathcal{C} \\subset \\mathcal{B}$ so that $\\bigcup\\limits_{B \\in \\mathcal{C}}B = X$. Thus for each $x \\in X$, there is some $B_{x} \\in \\mathcal{C}$ so that $x \\in B_{x}$. $\\checkmark$\n\t2. Suppose $B_{1}, B_{2} \\in \\mathcal{B}$. For any $B \\in \\mathcal{B}$, we have that $B = \\bigcup\\limits \\{B\\} \\in \\tau$, so $B_{1}, B_{2} \\in \\tau$. Since $(X, \\tau)$ is a [[Topological Space]], we know $B_{1} \\cap B_{2} \\in \\tau$. Thus there exists some $\\mathcal{C} \\subset \\mathcal{B}$ so that $\\bigcup\\limits_{B \\in \\mathcal{C}} B = B_{1} \\cap B_{2}$. Thus, for $x \\in B_{1} \\cap B_{2}$, there is some $B_{x} \\in \\mathcal{C}$ so that $x \\in B_{x}$ and $B_{x} \\subset B_{1} \\cap B_{2}$ by construction of $\\mathcal{C}$. $\\checkmark$ $\\blacksquare$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Topological-Space":{"title":"Untitled Page","content":"# Definition\nLet $X$ be a [[Set]] and $\\tau \\subset \\mathcal{P}(X)$. Then $(X, \\tau)$ is a [[Topological Space]] if\n1. $X, \\emptyset \\in \\tau$.\n2. Suppose $F \\subset \\tau$. Then $\\bigcup\\limits_{U \\in F} U \\in \\tau$.\n3. Suppose $G \\subset \\tau$ and $G$ is a [[Finite Set]]. Then $\\bigcap\\limits_{U \\in G} \\in \\tau$","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Topology-Generated-by-a-Basis":{"title":"Untitled Page","content":"See [[Topological Basis]]","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Total-Ordering":{"title":"Untitled Page","content":"# Definition\nA [[Partial Ordering]] $(T, \\leq)$ is a [[Total Ordering]] if $\\forall x,y \\in T$, either $x \\leq y$ or $y \\leq x$.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Tree":{"title":"Untitled Page","content":"# Definition\nSuppose $G$ is an [[Undirected Graph]]. Then $G$ is a [[Tree]] if $G$ is a [[Connected Graph]] and for each $u, v \\in V(G)$, there exists only one [[Simple Path]] from $u$ to $v$.","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Tree-Distance":{"title":"Untitled Page","content":"# Definition\nSuppose $T$ is a [[Tree]]. Then the [[Tree Distance]] between $u, v \\in V$ is the [[Path Length]] of the (unique) [[Simple Path]] from $u$ to $v$. We denote it $d(u, v)$.\n\n## Remarks\n1. [[Tree Distance]] is [[Shortest Path Distance]] on a [[Tree]], since the [[Shortest Path]] between any two [[Graph Vertex|vertices]] is the unique [[Simple Path]] between them. Thus our above notation can be used without confusion between the two.","lastmodified":"2022-04-25T04:32:04.814518443Z","tags":null},"/Trivial-Sigma-Algebra":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Tutte-Berge-Formula":{"title":"Untitled Page","content":"# Statement\nLet $G = (V, E)$ be an [[Undirected Graph]].  For any $B \\subset V$, let $\\text{oc}(G[B])$ be the number of [[Graph Connected Component]]s of $G[B]$ with [[Odd Integer|odd]] number of [[Graph Vertex|vertices]]. Then \n$$\\nu(G) = \\min \\{\\frac{1}{2} \\Big(|V| - \\text{oc}(G[V \\setminus A] + |A|\\Big) : A \\subset V\\}$$\n## Proof\n[[TODO]] - See [[Cook - Combinatorial Optimization]] Ch 5, Section 5.1, pg 131\n\n# Other Outlinks\n* [[Maximum Matching]]\n* [[Induced Subgraph]]","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Tuttes-Matching-Theorem":{"title":"Untitled Page","content":"# Statment\nLet $G = (V, E)$  be an [[Undirected Graph]]. For any $B \\subset V$, let $\\text{oc}(G[B])$ be the number of [[Graph Connected Component]]s of $G[B]$ with [[Odd Integer|odd]] number of [[Graph Vertex|vertices]]. Then $G$ has a [[Perfect Matching]] if and only if $\\forall A \\subset V$,  $\\text{oc}(G[V \\setminus A]) \\leq |A|$.\n\n## Proof\n[[TODO]] - See [[Cook - Combinatorial Optimization]] Ch 5, Section 5.1, pg 131 - 133\n\n# Other Outlinks\n* [[Induced Subgraph]]\n* [[Set Cardinality]]","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Undirected-Graph":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Set]] and suppose $E \\subset \\{\\{v, w\\} \\in \\mathcal{P}(V) : v, w \\in V, v \\neq w\\}$. Then $(V, E)$ is a [[Undirected Graph]] with [[Graph Vertex|vertices]] $v \\in V$ and [[Graph Edge|edges]] $e \\in E$.\n\n## Properties\n1. We can [[Represent an Undirected Graph as a Directed Graph]] by adding both directions for each edge.\n\n## Remarks\n1. Sometimes we say $G$ is an [[Undirected Graph]] and refer to its [[Graph Vertex|vertices]] and [[Graph Edge|edges]] as $V(G)$ and $E(G)$ respectively.","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Undirected-Hypergraph":{"title":"Untitled Page","content":"# Definition\nLet $V$ be a [[Set]] and suppose $E \\subset \\mathcal{P}(V)$. Then $(V, E)$ is an [[Undirected Hypergraph]] with [[Graph Vertex|vertices]] $v \\in V$ and [[Hypergraph Edge|edges]] $e \\in E$.","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Untitled":{"title":"Untitled Page","content":"","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Vector-Space":{"title":"Untitled Page","content":"# Definition\nSuppose $V$ is a [[Nonempty]] [[Set]], $F$ is a [[Field]], and $+: V \\times V \\to V$ and $*: F \\times V \\to V$ are [[Operation]]s. Then $(V, F, +, *)$ is a [[Vector Space]] if\n1. $(V, +)$ is an [[Abelian Group]]:\n\t1. [[Additive Identity]]: There exists $\\mathbf{0} \\in V$ so that for all $v \\in V$ we have that $\\mathbf{0} + v = v$\n\t2. [[Associativity]] of $+$: For all $x,y,z \\in V$, $(x + y) + z = x + (y + z)$\n\t3. [[Commutativity]] of $+$: For all $x, y \\in V$, $x + y = y + x$ \n\t4. [[Additive Inverse]]s: If $x \\in V$, then there exists $-x \\in V$ so that $x + (-x) = \\mathbf{0}$\n2. Compatibility with [[Scalar Multiplication]] ($*$):\n\t1. [[Multiplicative Identity]]: For all $v \\in V$ we have that $1 * v = v$ (where $1$ is the [[Multiplicative Identity]] of $F$).\n\t2. [[Associativity]]: For all $c,d \\in F$, $v \\in V$, we have that $(cd)*v = c*(d*v)$\n\t3. [[Distributivity]] of $*$ over $+$: For all $u, v \\in V$ and $c \\in F$, $c*(u + v) = c*u + c*v$\n\t4. [[Distributivity]] of $*$ over $+_{F}$: For all $u \\in V$ and $c, d \\in F$, $(c +_{F} d) * u = c * u + d * u$\n\n## Examples\n1. [[Functions to a Field form a Vector Space]]\n2. [[N-tuples over a Field form a Vector Space]]\n\n### n-tuples\n\n\n## Properties\n### Scalar multiplication by $0$ is $\\mathbf{0}$\nSuppose $v \\in V$. Then $0v = \\mathbf{0}$.\n#### Proof\n$$\\begin{align*}\n\u00260v = (0 + 0)v = 0v + 0v\\\\\n\u0026\\Rightarrow \\mathbf{0} = 0v.\n\\end{align*}$$\n$\\blacksquare$\n\n### If scalar product is $\\mathbf{0}$, then either scalar is $0$ or vector is $\\mathbf{0}$\nSuppose $v \\in V$, $c \\in F$ and that $cv = \\mathbf{0}$. Then either $c = 0$ or $v = \\mathbf{0}$.\n#### Proof\nIf $c = 0$, then we are done. Otherwise, $c$ has [[Multiplicative Inverse]] $c^{-1}$. Then\n$$\\begin{align*}\n\u0026c^{-1}(cv) = (c^{-1}c)v = 1v = v = \\mathbf{0}.\\\\\n\\end{align*}$$\n$\\blacksquare$\n\n### ![[Scalar product with -1 is additive inverse in vector spaces]]\n","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Vector-Subspace":{"title":"Untitled Page","content":"# Definition\nSuppose $V$ is a [[Vector Space]] over [[Field]] $F$. Supppose $W \\subset V$ and $W$ is a [[Vector Space]] over $F$ when endowed with the same $+$ and $*$ operations as $V$. Then we say $W$ is a [[Vector Subspace]] of $V$.\n\n## Properties\n1. [[A Subset of a Vector Space is a Subspace iff it is closed under scaling and addition]]\n\n## Examples\n1. [[Space of Polynomials over a Field form a Vector Subspace of Function Vector Space]]\nConsider \n\n","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Vertex-Cover":{"title":"Untitled Page","content":"# Definition\nLet $G$ be an [[Undirected Graph]]. A [[Vertex Cover]] $C \\subset V(G)$ is a [[Set]] s.t. $\\{e \\in E(G) : e \\cap C \\neq \\emptyset\\} = E(G)$.\n\n## Remarks\n1. If $E(G)$ is a [[Finite Set]], then the definition holds [[If and Only If]] $|\\{e \\in E(G) : e \\cap C \\neq \\emptyset\\}| = |E(G)|$.","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null},"/Vertex-Cover-and-Matching-Duality":{"title":"Untitled Page","content":"# Statement\nSuppose $G = (V, E)$ is an [[Undirected Graph]]. Then $\\tau(G) \\geq \\nu(G)$.\n## Proof\n[[TODO]] - This is a simple counting argument\n\n# Sources\n* [[Cook - Combinatorial Optimization]] - I forget what page, but early on.\n\n# Other Outlinks\n* [[Minimum Vertex Cover]]\n* [[Maximum Matching]]","lastmodified":"2022-04-25T04:32:04.818518454Z","tags":null}}