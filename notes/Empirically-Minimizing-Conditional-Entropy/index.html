<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Dates: [[2022-04-21]]
Recall from [[Minimizing Conditional Entropy]] the following setup:
 [[Probability Space]] $(\Omega, \mathcal{F}, \mathbb{P})$ [[Random Vector]] $U: \Omega \to \mathbb{F}{2}^{n}$ for $n \in \mathbb{N}$."><title>Empirically Minimizing Conditional Entropy</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://abhmul.github.io/quartz//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://abhmul.github.io/quartz/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://abhmul.github.io/quartz/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><script>const BASE_URL="https://abhmul.github.io/quartz/",fetchData=Promise.all([fetch("https://abhmul.github.io/quartz/indices/linkIndex.fdd1a591b09c16e025b3711aab3fa379.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://abhmul.github.io/quartz/indices/contentIndex.fb5c67c9f72d4d2e2e999410a1adc018.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://abhmul.github.io/quartz/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://abhmul.github.io/quartz/>ðŸª´ Quartz 3.2</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Empirically Minimizing Conditional Entropy</h1><p class=meta>Last updated April 25, 2022</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#further-inspection-of-entropy-estimate>Further inspection of entropy estimate</a></li></ol></li></ol></nav></details></aside><p>Dates: <a class="internal-link broken">2022-04-21</a></p><p>Recall from <a href=/quartz/notes/Minimizing-Conditional-Entropy rel=noopener class=internal-link data-src=/quartz/notes/Minimizing-Conditional-Entropy>Minimizing Conditional Entropy</a> the following setup:</p><ol><li><a href=/quartz/notes/Probability-Space rel=noopener class=internal-link data-src=/quartz/notes/Probability-Space>Probability Space</a> $(\Omega, \mathcal{F}, \mathbb{P})$</li><li><a class="internal-link broken">Random Vector</a> $U: \Omega \to \mathbb{F}<em>{2}^{n}$ for $n \in \mathbb{N}$. This represents the input sequence. For shorthand $U</em>{i}=\pi_{i}\circ U$, where $\pi_i$ is the $i$th <a class="internal-link broken">Projection Map</a> for $i \in [n]$.</li><li><a href=/quartz/notes/Measureable-Function rel=noopener class=internal-link data-src=/quartz/notes/Measureable-Function>Measureable Function</a>s<ol><li>Encoder $f: \mathbb{F}^{n}_{2} \to M$ where $(M, d)$ is a <a class="internal-link broken">Metric Space</a>.</li><li>Noise Function $g: M \times \Omega \to M$</li><li>Decoder $h: M \to \mathbb{F}^{n}_{2}$</li></ol></li><li>Denote<ol><li>$F = f \circ U$</li><li>$G(\omega) = g(F(\omega), \omega)$</li><li>$H = h \circ G$</li></ol></li></ol><p>We are interested in ways to empirically minimize the quantity</p><p>$$R_{g}(f) = \frac{1}{n}\sum\limits_{i =1}^{n} \mathbb{H}(U_{i} | G)$$</p><p>Where the <a href=/quartz/notes/Conditional-Entropy rel=noopener class=internal-link data-src=/quartz/notes/Conditional-Entropy>Conditional Entropy</a> $\mathbb{H}(U_{i} | G)$ is defined as</p><p>$$\begin{align*}
&\mathbb{E}[\mathbb{H}<em>{2}(\mathbb{P}(U</em>{i} | G))]\\&=\mathbb{E}[-\mathbb{P}(U_{i}=1 | G) \lg \mathbb{P}(U_{i}=1 | G) - (1 - \mathbb{P}(U_{i}=1 | G)) \lg (1 - \mathbb{P}(U_{i}=1 | G))]
\end{align*}$$</p><p>We first establish the following theorem</p><p><strong>Theorem 1</strong>: Minimizing $\mathbb{H}(U_{i}|G)$ is equivalent to minimizing</p><p>$$\mathbb{H}(G|U_{i}) - \mathbb{H}(G)$$</p><p>where $\mathbb{H}(G) = \mathbb{E}()$</p><p><strong>Proof</strong>: $A \subset \mathbb{R}$ so that $\mathbb{P}(U_{i} = 1, G \in A) > 0$. By <a href=/quartz/notes/Bayes-Theorem rel=noopener class=internal-link data-src=/quartz/notes/Bayes-Theorem>Bayes Theorem</a>, we have that</p><p>$$\begin{align*}
\mathbb{P}(U_{i}=1 | G \in A) &= \frac{\mathbb{P}(U_{i}=1, G \in A)}{\mathbb{P}(G \in A)}\\&=\frac{\mathbb{P}(G \in A | U_{i} = 1) \mathbb{P}(U_{i}=1)}{\mathbb{P}(G \in A)}
\end{align*}$$</p><p>Then &mldr; #TODO I&rsquo;ll come back to this after my <a href=/quartz/notes/Interpreting-Deep-Codes rel=noopener class=internal-link data-src=/quartz/notes/Interpreting-Deep-Codes>Interpreting Deep Codes</a> meeting. For now I just need a basic algorithm with the assumption that $G$ is <a class="internal-link broken">Additive White Gaussian Noise Channel</a>.</p><p>Let $Y$ be the received noisy sequence corrupted by the <a class="internal-link broken">Additive White Gaussian Noise Channel</a> channel with <a class="internal-link broken">Variance</a> $\sigma^2$. By <a href=/quartz/notes/Bayes-Theorem rel=noopener class=internal-link data-src=/quartz/notes/Bayes-Theorem>Bayes Theorem</a> we get that</p><p>$$\mathbb{P}(U_{i}=b|Y=y) = \frac{f_{Y|U_{i}=b}(y) \mathbb{P}(U_{i}=b)}{f_{Y}(y)}$$</p><p>Then $$\lg \mathbb{P}(U_{i}=b|Y=y) = \lg f_{Y|U_{i}=b}(y) + \lg \mathbb{P}(U_{i}=b) - \lg f_{Y}(y)$$ and we get</p><p>$$\begin{align*}
&\mathbb{E}[-\mathbb{P}(U_{i}=1 | G) \lg \mathbb{P}(U_{i}=1 | G) - (1 - \mathbb{P}(U_{i}=1 | G)) \lg (1 - \mathbb{P}(U_{i}=1 | G))]\\&=\int\limits_\mathbb{R} \frac{f_{Y}(y)}{f_{Y}(y)} \Big[-f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg f_{Y|U_{i}=1}(y) -f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg f_{Y|U_{i}=0}(y) \\&- f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg \mathbb{P}(U_{i}=1) - f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg \mathbb{P}(U_{i}=0) \\&+ f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg f_{Y}(y) + f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg f_{Y}(y) \Big] dy\\&=\Big[\int\limits_\mathbb{R} -f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg f_{Y|U_{i}=1}(y)dy + \int\limits_\mathbb{R} -f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg f_{Y|U_{i}=0}(y)dy \\&+ \int\limits_\mathbb{R} - f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg \mathbb{P}(U_{i}=1)dy + \int\limits_\mathbb{R} - f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg \mathbb{P}(U_{i}=0)dy \\&- \int\limits_\mathbb{R} - f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) \lg f_{Y}(y)dy + \int\limits_\mathbb{R} - f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \lg f_{Y}(y)dy \Big]\\&= \Bigg[ \mathbb{E}\Big(\int\limits_\mathbb{R} -f_{Y|U_{i}}(y) \lg f_{Y|U_{i}}(y)dy\Big) \\&+ \mathbb{E}\Big( \lg \mathbb{P}(U_{i})\int\limits_\mathbb{R} - f_{Y|U_{i}}(y)dy \Big) \\&- \int\limits_\mathbb{R} \Big[ - f_{Y|U_{i}=1}(y) \mathbb{P}(U_{i}=1) - f_{Y|U_{i}=0}(y) \mathbb{P}(U_{i}=0) \Big]\lg f_{Y}(y)dy \Bigg]\\&=\Bigg[ \mathbb{E}\Big(\mathbb{E}( - \lg f_{Y|U_{i}}|U_{i})\Big) \\& + \mathbb{E}\Big( \lg \mathbb{P}(U_{i})\Big)\\&-\mathbb{E} \Big( - \lg f_{Y} \Big) \Bigg]\\&=\Bigg[ \mathbb{H}(Y|U_{i}) + \mathbb{H}(U_{i}) -\mathbb{H}(Y)\Bigg]</p><p>\end{align*}$$</p><p>$\mathbb{H}(U_{i})$ is a constant, so we really just want to minimize</p><p>$$\frac{1}{n} \sum\limits_{i=1}^{n} \Big[ \mathbb{H}(Y|U_{i}) \Big] - \mathbb{H}(Y)$$</p><p>So how do we estimate these two terms. For $\mathbb{H}(Y)$, this is straightforward, we sample $m$ samples independently, call them $u^{(j)}$ for $j \in [m]$, encode them, then noise them to get $y^{(j)}$ for $j \in [m]$. These constitute $m$ independent samples of noise. Then we compute</p><p>$$\begin{align*}
&\mathbb{H}(Y) = \mathbb{E} \Big( - \lg f_{Y} \Big)\\&\approx \frac{1}{m} \sum\limits_{j=1}^{m} -\lg f_{Y}(y^{(j)})\\&=\frac{1}{m} \sum\limits_{j=1}^{m} -\lg \Big[ \mathbb{E}(f_{Y|X}(y^{(j)})) \Big]\\&\approx \frac{1}{m} \sum\limits_{j=1}^{m} -\lg \Big[ \frac{1}{m}\sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\&= \frac{1}{m} \sum\limits_{j=1}^{m}\lg m -\lg \Big[ \sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\&=\lg m - \frac{1}{m} \sum\limits_{j=1}^{m}\lg \Big[ \sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\\end{align*}$$</p><p>For the other term, for each $i \in [n]$, split our samples into groups ${j : u^{(j)}<em>{i} = b}$ for $b \in \mathbb{F}</em>{2}$. From there, we compute a similar computation as above on each of these subsets, then average them together based on the proportion in the subset.</p><p><strong>Algorithm</strong>:</p><ol><li>Sample ${u^{(j)} : j \in [m]}$ <a class="internal-link broken">Independently and Identically Distributed</a> from $\text{Unif}{0, 1}$.</li><li>Sample ${\nu^{(j)} : j \in [m]}$ <a class="internal-link broken">Independently and Identically Distributed</a> from $\mathcal{N}(0, \sigma^2)$.</li><li>Compute $x^{(j)} = C(u^{(j)})$ for $j \in [m]$.</li><li>Compute $y^{(j)} = x^{(j)} + \nu^{(j)}$ for $j \in [m]$.</li><li>Compute $f_{Y|X=x^{(k)}}(y^{(j)}) = \mathcal{N}_{\sigma^{2}}(y^{(j)} - x^{(j)})$ for $j, k \in [m]^{2}$.</li><li>Compute $\hat{\mathbb{H}} = \hat{\mathbb{H}}({y^{(j)}: j \in [m]}) = \ln m - \frac{1}{m} \sum\limits_{j=1}^{m}\ln \Big[ \sum\limits_{j=1}^{m}(f_{Y|X=x^{(j)}}(y^{(j)})) \Big]$.</li><li>For $i \in [n]$:<ol><li>Let $B_{0} = {j : u^{(j)}<em>{i} = 0}$ and $B</em>{1} = {j : u^{(j)}_{i} = 1}$</li><li>Compute $\hat{\mathbb{H}}({y^{(j)}: j \in B_{0}}) = \ln |B_{0}| - \frac{1}{|B_{0}|} \sum\limits_{j \in B_{0}} \ln \Big[ \sum\limits_{k \in B_{0}}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]$.</li><li>Compute $\hat{\mathbb{H}}({y^{(j)}: j \in B_{1}}) = \ln |B_{1}| - \frac{1}{|B_{1}|} \sum\limits_{j \in B_{1}} \ln \Big[ \sum\limits_{k \in B_{1}}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]$</li><li>Compute $\hat{\mathbb{H}}<em>{i} = \frac{|B</em>{0}|}{m} \hat{\mathbb{H}}({y^{(j)}: j \in B_{0}}) + \frac{|B_{1}|}{m} \hat{\mathbb{H}}({y^{(j)}: j \in B_{1}}$.<ul><li>Replace the fractions with $\mathbb{P}(u_{i} = 1) = \frac{1}{2}$?</li></ul></li><li>Compute $\mathbb{H}(u_{i}) = \ln 2$</li></ol></li><li>Return $\frac{1}{n} \sum\limits_{i=1}^{n} \Big[ \hat{\mathbb{H}}_{i} \Big] - \hat{\mathbb{H}} + \ln 2$</li></ol><h3 id=further-inspection-of-entropy-estimate>Further inspection of entropy estimate</h3><p>$$\begin{align*}
&\mathbb{H}(Y) = \mathbb{E} \Big( - \lg f_{Y} \Big)\\&\approx \frac{1}{m} \sum\limits_{j=1}^{m} -\lg f_{Y}(y^{(j)})\\&=\frac{1}{m} \sum\limits_{j=1}^{m} -\lg \Big[ \mathbb{E}(f_{Y|X}(y^{(j)})) \Big]\\&\approx \frac{1}{m} \sum\limits_{j=1}^{m} -\lg \Big[ \frac{1}{m}\sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\&= \frac{1}{m} \sum\limits_{j=1}^{m}\lg m -\lg \Big[ \sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\&=\lg m - \frac{1}{m} \sum\limits_{j=1}^{m}\lg \Big[ \sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \Big]\\\end{align*}$$
This will not be unbiased since in general</p><p>$$\begin{align*}
&\mathbb{E} \left[ \frac{1}{m} \sum\limits_{j=1}^{m} -\lg \left[ \frac{1}{m}\sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \right] \right] \\&=\mathbb{E} \left[-\lg \left( \frac{1}{m}\sum\limits_{k=1}^{m}(f_{Y|X=x^{(k)}}(y^{(j)})) \right) \right]\\&\neq \mathbb{E}\Bigg[ -\lg \Big[ \mathbb{E}(f_{Y|X}(Y)|Y) \Big] \Bigg] \\&= \mathbb{E} \Big( - \lg f_{Y} \Big) = \mathbb{H}(Y)
\end{align*}$$
If we use the expression of the log of the <a class="internal-link broken">Gaussian Density Function</a>:
$$\begin{align*}
\ln f_{Y|X=x^{(k)}}(y^{(j)}) &= \ln \left[ \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma}e^{-\frac{(y^{(j)}<em>{i} - x^{(k)}</em>{i})^{2}}{\sigma^{2}} }\right]\\&=\ln \left[ \frac{1}{(2 \pi \sigma^{2})^{\frac{n}{2}}} \prod_{i=1}^{n} e^{-\frac{(y^{(j)}<em>{i} - x^{(k)}</em>{i})^{2}}{\sigma^{2}} }\right]\\&=\ln \left[ \frac{1}{(2 \pi \sigma^{2})^{\frac{n}{2}}} e^{-\frac{\sum\limits_{i=1}^{n} (y^{(j)}<em>{i} - x^{(k)}</em>{i})^{2}}{\sigma^{2}} }\right]\\&=-\frac{n}{2}\ln (2 \pi \sigma^{2}) - \frac{\sum\limits_{i=1}^{n} (y^{(j)}<em>{i} - x^{(k)}</em>{i})^{2}}{\sigma^{2}}\\&=-\frac{n}{2}\ln (2 \pi \sigma^{2}) - \frac{1}{\sigma^{2}}||y^{(j)} - x^{(k)}||^{2}
\end{align*}$$
So the log conditional density matrix can be computed by computing pairwise distances between $y$ and $x$. Furthermore observe that since
$$||y -x ||^{2} = \langle y - x, y- x \rangle = ||y||^{2} - 2\langle y, x \rangle + ||x||^{2}$$
and if we write the matrices $Y$ and $X$ both of dimension $m \times n$, we could compute $Y X^{T}$ to get the inner products. Computing the norms of the rows of $Y$ and $X$ and applying the above formula with broadcasting gives us an efficient way to compute the above distances. Noting that $y = x + \nu$, we see</p><p>$$||y||^{2} = \langle x&rsquo; + \nu, x&rsquo; + \nu \rangle = ||x&rsquo;||^{2} + 2 \langle x&rsquo;, \nu \rangle + ||\nu||^{2}$$</p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://abhmul.github.io/quartz/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://abhmul.github.io/quartz/notes/Empirically-Minimizing-Conditional-Entropy","https://abhmul.github.io/quartz",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>Github</a></li></ul></footer></div><script src=https://abhmul.github.io/quartz/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://abhmul.github.io/quartz")</script></div></body></html>